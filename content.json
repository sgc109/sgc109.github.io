{"pages":[{"title":"About","text":"Career &amp; Education Bucketplace - Software Engineer | 2022.04 ~ Now Global Team NAVER Clova - Software Engineer | 2020.06 ~ 2021.05 AI Server Production Samsung Research - Software Engineer | 2019.01 ~ 2020.04 Big Data Team Korea Aerospace University - B.Eng. | 2012.03 ~ 2019.02 Software And Computer Engineering Interests Open Source Contributions Debezium - DBZ-3528 Support for MongoDB Outbox Event Router SMT Contests 21st Place @ 2018 ACM-ICPC Seoul Regional - 2018.11 17th Place @ 2017 ACM-ICPC Daejeon Regional - 2017.11 장려상 3rd Place @ 2017 shake! (경인지역 6개대학 연합 프로그래밍 경시대회) - 2017.07 우수상 2nd Prize @ 2018 마이다스 챌린지 - 2018.05 최우수상","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Kubernetes 의 API 서버 보안","text":"이 포스팅에서는 쿠버네티스에에서 API 서버의 보안과 아래의 리소스들에 대해 이야기한다. ServiceAccount Role 과 RoleBinding ClusterRole 과 ClusterRoleBinding 쿠버네티스에서 API 서버 보안은 왜 필요할까? 쿠버네티스에서는 이론적으로 파드 외부 또는 내부에서 API 서버로 적절한 요청을 하면 어떤 리소스던 생성, 삭제, 수정, 조회가 가능하다. 그런데 만약 개발자의 코딩 실수로 어떤 파드에서 아무 관련이 없는 다른 파드를 삭제해버릴 수 있다면 큰 문제가 될 것이다. 그렇기 때문에 사용자 또는 파드에게 적절하게 권한을 부여하는 기능은 보안과 안정적인 운영을 위해 필수적이다. 회사에서 여러개의 팀이 하나의 공용 클러스터를 함께 사용하는 멀티테넌트 환경에서는 이러한 권한의 분리가 더욱 중요할 것이다. 만약 어떤 팀에서 실행 중인 파드의 목록과 상세 정보를 다른 팀에서 볼 수 있고, 심지어 삭제할 수가 있다면 문제가 될 것이다. 악의적인 목적이건, 실수이건 간에 위와 같은 일은 언제라도 벌어질 수 있기 때문에 모든 유저와 파드에게 적절한 권한을 부여해야 하는 것이다. 쿠버네티스 클러스터는 보통 위와 같은 문제가 발생하지 않도록 하기 위해 API 서버가 사용자나 Pod 의 요청을 받을 때 명시적으로 설정된 권한만 허용하고, 그 이외의 모든 권한은 허용하지 않도록 동작한다. Service Account Service Account 란? 서비스 어카운트(Service Account) 는 Kubernetes 의 파드에서 API 서버에 요청을 보냈을 때 이 &quot;파드&quot;를 식별하기 위한 리소스다. (사용자를 식별하는데 사용되지는 않는다) 파드에서 API 서버에 요청을 보내면 이 파드의 정체가 무엇인지 알아야 어떤 권한을 가지고 있는지도 알 수 있고, 이를 기반으로 파드의 요청이 권한에 맞는지를 확인하여 요청을 처리해줄지 말지를 결정할 것이다. 실제로 권한을 정의하고, 설정하는 부분은 이후에 설명할 Role, ClusterRole, RoleBinding, ClusterRoleBinding 의 역할이고, ServiceAccount 는 이러한 권한을 적용할 수 있는 주체 중 한가지로서, Pod 의 신분증같은 것이라고 생각하면된다. (물론 하나의 ServiceAccount 는 여러 Pod 가 사용할 수 있기 때문에 정확히 신분증과 동일한 개념은 아닐 것이다.) ServiceAccount 의 특징 모든 파드는 무조건 하나의 ServiceAccount 와 매핑이 되어야 실행될 수가 있다. 그런데 ServiceAccount 를 만들지 않고, 파드의 매니페스트에 ServiceAccount 를 명시적으로 적어주지 않아도 파드가 잘 생성이 되고 실행 되는 것을 보고 의아할 수도 있다. 사실 이것은, 쿠버네티스의 ServiceAccount Controller 가 모든 네임스페이스에 default 라는 이름의 서비스어카운트가 있도록 자동 생성해주며, ServiceAccount Admission Controller 가 파드의 매니페스트에 명시적으로 서비스어카운트를 정의하지 않으면 default 서비스어카운트를 매핑해주기 때문에 가능한것이다. 뿐만 아니라, 서비스어카운트는 mountable secrets 에 지정한 시크릿만 파드에 마운트할 수 있도록 강제하는 기능과, Image pull secrets 기능을 통해 프라이빗 이미지 레지스트리에서 이미지를 가져올 수 있도록 하기 위한 시크릿을 이 서비스어카운트를 사용하는 파드에 자동으로 마운트시켜주는 기능도 가지고 있는데 image pull secrets 기능도 ServiceAccount Admission Controller 가 수행한다. ServiceAccount 의 동작 방식 kubectl create sa &lt;name&gt; 명령어로 서비스어카운트를 생성할 수 있다. Token Controller 는 서비스어카운트가 생성될 때마다 자동으로 kubernetes.io/service-account-token 타입의 Secret 을 생성하여 매핑시켜준다. Token Controller 에 의해 생성된 Secret 에는 아래와 같은 3가지 데이터가 base64 로 인코딩되어 들어있다. ca.crt - API 서버와 통신 시, SSL 인증을 위한 증명서 token - 서비스어카운트 이름, Secret 이름 등의 정보에 서명한 JWT. API 서버에 요청시 Bearer 토큰으로 사용됨 namespace - 네임스페이스 파드 내 애플리케이션은 위의 데이터를 사용하여 API 서버와 통신한다. API 서버는 요청의 Authorization 헤더에 있는 Bearer token 을 디코딩하여 어느 서비스어카운트를 사용해 보낸 요청인지 식별하게 된다. RBAC 란? 어떤 사람, 혹은 파드가 API 서버에 요청을 하면 API 서버는 인증(Authentication)과 인가(Authorization)를 수행한다. 인증은 접근 가능 여부를 확인하는 것이고, 인가는 접근 가능한 요청에 대해 요청된 자원에 접근할 수 있는지를 확인하는 것이다. RBAC(Role-Based Access Control) 는 API 서버가 인가를 수행하는 여러 방법 중 하나다. 쿠버네티스는 다음과 같은 인가 방식을 제공한다. Node Kubelet 에 의한 요청에 대한 인가를 위한 방식 ABAC(Attribute-Based Access Control) 리소스의 속성에 따라 인가를 하는 방식 RBAC Role 을 기반으로 인가를 하는 방식 Webhook 외부 API 를 통해 인가를 하는 방식 이 중에서 RBAC가 표준이며, 1.8.0 부터는 대부분의 클러스터에서 기본적으로 사용하는 방식이다. RBAC 는 특정 주체(subject) 가 특정 대상(url, resource 타입, 혹은 특정 resource) 에 대해 특정 행위(verb) 를 할 수 있는지를 지정하는 방식이다. 이를 어떻게 지정하는 지는 이후 Role 과 RoleBinding 에 대한 설명에서 명확해 질 것이다. Role 과 RoleBinding 앞서 RBAC 는 특정 주체가 특정 대상에 대해 특정 행위를 할 수 있는지를 지정하는 방식이라고 했다. 여기서 Role 에 대상(resource 등) 과 행위(verb) 를 지정하며, RoleBinding 에 주체(subject) 를 지정한다. 여기서 주체는 3가지(User, Group, ServiceAccount) 중 한가지가 된다. 대상은 보통 resource 의 타입을 정하는데, resourceName 으로 특정 리소스를 지정할 수도 있다. 이후에 설명할 ClusterRole 에서는 리소스가 아닌 URL 을 지정할 수도 있다. /healthz 와 같이 특정 리소스에 대한 요청이 아닌 경우도 있기 때문이다. 행위는 아래 표와같이 API 요청에 사용된 HTTP 메서드에 따라 특정 행위에 매핑되는데, HTTP method verb POST create GET, HEAD get(개별 리소스), list(전체 오브젝트 내용을 포함한 리소스 모음), watch(개별 리소스 또는 리소스 모음을 주시) PUT update PATCH patch DELETE delete(개별 리소스), deletecollection(리소스 모음) 행위에 대한 대상이 리소스라면 행위를 verb 로 적어주고, URL 이라면 HTTP 메소드로 적어준다. 이름 그대로 Role 은 역할이고, RoleBinding 은 이러한 역할을, 역할을 수행하는 주체에 연결시켜 주는 것이다. Role 과 RoleBinding 의 특징 Role 과 RoleBinding 은 특정 네임스페이스에 종속된 개념이다. 그렇기 때문에 RoleBinding 은 다른 네임스페이스의 Role 을 바인딩해줄 수는 없다. 하지만, RoleBinding 이 같은 네임스페이스의 Role 을 다른 네임스페이스의 subject 에게 바인딩해 줄 수는 있다. 그래서 RoleBinding 에 subject 들을 명시할 때는 name 과 namespace, 그리고 kind(user/group/serviceaccount) 를 함께 명시한다. 하나의 Role 은 여러개의 RoleBinding 에 의해 바인드될 수 있고, 하나의 RoleBinding 은 하나의 Role 만 참조할 수 있다. 즉, Role 과 RoleBinding 은 일대다(one-to-many) 관계다. 반면 하나의 RoleBinding 은 하나의 Role 을 여러 주체에 연결시켜 줄 수 있고, 하나의 주체는 여러개의 RoleBinding 에 의해 권한이 부여될 수 있다. 즉, RoleBinding 과 Subject(ServiceAccount 등) 는 다대다(many-to-many) 관계다. Role 과 RoleBinding 만들기 Role 과 RoleBinidng 을 만드는 방법은 크게 두가지가 있다. 하나는 매니페스트를 통해 만드는 방법이고, 나머지 하나는 kubectl create 명령어를 통해 만드는 방법이다. 다음과 같이 YAML 파일을 작성하지 않고도 직접 verb, resource, role, serviceaccount 등을 인자로 주어 Role 과 RoleBinding 을 생성할 수 있다. RoleBinding 의 속성 중 Role 은 단수형이고, Subjects 는 복수형이라는 것을 봐도 이들간의 연관관계를 짐작할 수 있다. 다음과 같이 YAML 파일을 통해 Role 과 RoleBinding 을 생성할 수도 있다. apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: namespace: foo name: service-readerrules: - apiGroups: [\"\"] verbs: [\"get\", \"list\"] resources: [\"services\"] ClusterRole 과 ClusterRoleBinding ClusterRole 과 ClusterRoleBinding 이 필요한 이유는, Role 과 RoleBinding 이 특정 네임스페이스에 종속된 개념이기 때문이다. 그렇기 때문에 크게 다음과 같은 3가지 한계가 존재한다. Role 과 RoleBinding 의 한계 첫번째로, 모든 네임스페이스에 대해 똑같은 권한을 부여하고 싶은 경우에 각 네임스페이스마다 매번 똑같은 내용의 Role 을 만들어줘야 할 것이다. 하지만 이런 방식은 번거롭고 실수하기도 쉽다. 두번째로, Role 은 같은 네임스페이스에 존재하는 리소스에 대한 권한만 허용할 수 있다. 그렇기 때문에 하나의 서비스어카운트에 모든 네임스페이스에 존재하는 특정 리소스에 대한 권한을 주고 싶은 경우에 모든 네임스이스마다 Role 과 RoleBinding 을 만들고 이 서비스어카운트에 바인딩해줘야 한다. 세번째로, 네임스페이스에 독립적인 클러스터 수준의 리소스나 리소스와 관련이 없는 요청도 있다. 예를 들어, Node 나 Namespace 와 같은 리소스은 물론이고, ClusterRole 과 ClusterRoleBinding 또한 특정 네임스페이스에 속한 리소스가 아니며, 앞서 언급한 /healthz 와 같은 URL 은 리소스와는 관련이 없는 요청이다. 위와 같은 3가지 경우 ClusterRole 과 ClusterRoleBinding 을 사용해야 한다. 첫번째의 경우는, RoleBinding 은 Role 뿐 아니라 ClusterRole 도 참조할 수 있기 때문에 ClusterRole 만 하나 만들어 두고, 각 네임스페이스에 있는 RoleBinding 이 이를 참조하여 subject 에 바인딩해주도록 하면 네임스페이스마다 Role 을 정의하는 대신 ClusterRole 이라는 글로벌한 Role 을 정의할 수 있게 된다. 두번째의 경우는, ClusterRole 에 위와 같이 RoleBinding 을 사용하는 것이 아니라, ClusterRoleBinding 을 사용하면 해결할 수 있다. ClusterRole 을 사용하더라도 RoleBinding 을 사용하면 RoleBinding 과 같은 네임스페이스에 속한 리소스에대한 권한만 얻을 수 있다. 반면, ClusterRoleBinding 을 사용하여 subject 에 ClusterRole 을 바인딩을 해주면 모든 네임스페이스에 속한 리소스에 대한 권한을 얻을 수 있다. 세번째의 경우도 두번째의 경우와 마찬가지로 ClusterRole 과 ClusterRoleBinding 을 사용하면 해결할 수 있다. ClusterRole 에는 Role 과는 달리 Resources 는 물론 Non-Resource URLs 을 정의할 수가 있다. 다음은 kubectl 을 통해 ClusterRole 을 만들고 이를 조회한 결과 화면이다. 디폴트 ClusterRole &amp; ClusterRoleBinding API 서버는 기본적으로 디폴트 ClusterRole 과 ClusterRoleBinding 을 생성한다. 보통 디폴트로 생성된 클러스터롤과 클러스터롤바인딩은 system: 접두사를 가진다. 대부분의 디폴트 ClusterRole 들은 같은 이름을 가진 ClusterRoleBinding 이 존재한다. 그 중에서도 system:discovery 와 system:public-info-viewer 의 경우 공개되어도 안전하다고 여겨지는 정보들에 대한 읽기 권한을 system:authenticated 이나 system:unauthenticated group 에 바인딩하며, system:controller: 접두사를 가지는 것들은 쿠버네티스의 여러 컨트롤러들에게 권한을 부여하는데 사용된다. 다음과같이 system: 접두사를 가지지 않은 디폴트 클러스터롤(&amp; 클러스터롤바인딩)도 존재한다. cluster-admin 모든 리소스에 대한 모든 권한 admin ResourceQuota 와 Namespace 를 제외한 모든 리소스에 대한 조회 및 수정 가능 edit admin 의 권한에서 (Cluster)Role 과 (Cluster)RoleBinding 은 조회만 가능 view (Cluster)Role, (Cluster)RoleBinding, Secret 을 제외한 모든 리소스 조회만 가능 이 중 system:masters group 에 바인딩 되어있는 cluster-admin 을 제외하고 나머지 ClusterRole 들은 기본적으로 ClusterRoleBinding 에 의해 어딘가에 바인딩 되어 있진 않다. 참조 쿠버네티스 인 액션(2020, 에이콘출판사) 서비스 어카운트 관리하기(https://kubernetes.io/ko/docs/reference/access-authn-authz/service-accounts-admin/) Authentication(https://kubernetes.io/docs/reference/access-authn-authz/authentication/) 인가 개요(https://kubernetes.io/ko/docs/reference/access-authn-authz/authorization/) Using RBAC Authorization(https://kubernetes.io/docs/reference/access-authn-authz/rbac/)","link":"/2021/01/23/api-server-security-in-k8s/"},{"title":"MSA가 좋지만은 않은 이유","text":"마이크로서비스 아키텍처(MSA)의 목표는 하나의 거대한 서비스를 일정한 기준으로 쪼갠 여러 stand-alone 서비스로 구동하여 각 애플리케이션의 복잡도와 이들 간의 결합도를 낮춤으로써 궁극적으로는 전체 시스템을 좀 더 생산적으로 개발하고 운영하는 것이라고 볼 수 있다. 시스템의 요구사항이 많아지면 애플리케이션은 점점 덩치가 커져 하나의 거대한 모놀리식 애플리케이션이 되고, 그렇게 되면 다음과같은 단점이 있기 때문에 서비스를 더 작은 부분들로 쪼개는 것이다. 모놀리식 애플리케이션의 단점 낮은 개발 생산성 시스템의 요구사항이 많아질 수록 코드의 양은 늘어날 것이다. 그러면 개발자가 모든 피쳐를 파악하기가 점점 더 어려워지기 때문에 새로운 기능을 추가하거나 코드를 변경하기도 더 어려워 질 것이다. 빌드 시간도 함께 늘어날 것이다. 예를 들어 배달 서비스를 개발한다고 할 때, 음식을 주문하는 로직에 아주 약간의 코드를 변경했다고 해보자. 그럼에도 불구하고 거의 전체 애플리케이션을 다시 빌드하고 재배포해야 할 수도 있다. 빌드 시간이 느려지면 코드를 변경하고 재배포 될 때 까지의 시간이 늘어나기 때문에 개발 생산성이 떨어질 수 있다. 그런데 만약 주문 서비스가 분리되어 있다면 주문 서비스만 빌드하고 재배포하면 될 것이다. 게다가 코드의 양이 과도하게 많으면 IDE 의 실행 속도도 느려지기 때문에 개발 환경에 따라 개발 생산성에 영향을 미칠 수도 있다. 팀 운영의 어려움 애플리케이션의 규모가 커질 수록 그에 비례하여 필요한 인력도 많아질 것이다. 한 팀이 너무 많은 인원으로 이루어진다면 다양한 의견을 수렴하거나 의사결정하는데 어려움이 있을 수 있다. 만약 팀을 효율적으로 운영하기 위해서 여러개의 팀으로 나누어 하나의 코드 베이스를 개발하고 운영하더라도 역할의 경계가 명확하지 않아 책임소재도 불분명해지는 등 다양한 어려움이 있을 수 있다. 하지만 만약 하나의 거대한 서비스가 여러개의 작은 서비스로 분리된다면 팀 별로 서비스를 나누어 독자적으로 개발 및 운영을 할 수 있게 된다. 그리고 팀마다 서로 다른 프로그래밍 언어, 또는 기술을 사용할 수 있게 되어 새로운 기술의 도입이 비교적 자유롭다는 추가적인 이점도 있다. 결함 격리의 어려움 주식 거래 서비스를 예로 들면, 모놀리식 애플리케이션에서 사용자의 주식 주문 히스토리를 반환하는 API 에 문제가 생겨 애플리케이션이 다운되었다고 해보자. 그러면 다시 복구가 될 때까지 사용자는 원활하게 주문을 하지 못할 수도 있고, 결과적으로 회사는 막대한 금전적인 손실을 입을 수도 있다. 하지만 만약 주문 히스토리 서비스와 주문 서비스가 분리되어 별도의 애플리케이션으로 실행 중이라면, 주문 히스토리 서비스가 다운되더라도 사용자가 자신의 주문 히스토리를 볼 수 없게 될 지언정, 주문을 하는데는 아무런 문제가 없기 때문에 상대적으로 금전적인 손실을 덜 입을 가능성이 높다. 확장의 어려움 예를 들어 이미지 프로세싱은 CPU 집중적인 작업이기 때문에 높은 CPU 성능이 필요할 것이고, 많은 데이터를 메모리에 유지하는 작업은 높은 메모리 용량이 필요할 것이다. 하지만 이미지 프로세싱 모듈과 많은 메모리를 사용하는 모듈이 하나의 애플리케이션으로 실행 중이라면 각 모듈을 독립적으로 확장할 수가 없기 때문에 상대적으로 리소스 사용 효율성이 떨어질 수 있다. MSA 에서 추가로 고민해야 할 점 시스템을 MSA 로 구성하면 위와 같은 모놀리식 애플리케이션의 문제를 어느정도 해결할 수 있다. 하지만 모든 기술에 trade-off 가 있듯이 MSA 에도 단점이 있다. 한 줄로 요약하자면 할 일이 많아지고 고민해야 할 부분이 많아진다는 것이다. 그렇기 때문에 개발 및 운영 생산성이 단순히 향상된다고만 볼 수는 없다. IPC 도메인 서비스 간의 상호작용 시 모놀리식에서는 하나의 프로세스 내에서의 메서드 콜이었지만 MSA 에서는 각 서비스가 별도의 프로세스로 실행된다. 그러므로 IPC(Inter-Process Communication) 에 대해 고민해야 한다. 보통 각 서비스의 프로세스는 네트워크 상에서 떨어져서 실행된다. 그렇기 때문에 이 네트워크 상에서 떨어진 서로 다른 프로세스 간의 통신을 위한 방법을 고민해야 한다. 가장 떠올리기 쉬운 방법은 역시 HTTP 나 gRPC 같은 동기 방식이다. 하지만 동기 방식으로 통신하기 위해서는 통신의 양쪽 주체가 모두 정상적으로 실행 중이어야 하기 때문에 상대적으로 가용성이 떨어진다. 비동기 메시징 방식으로 통신할 수도 있다. Akka 와 같은 별도의 메시지 브로커 없이 동작하는 액터 기반의 도구를 사용할 수도 있을 것이다. 관리 대상 컴포넌트가 추가되지 않는다는 장점이 있지만 가용성은 낮다. 카프카와 같은 메시지 브로커를 통한 비동기 메시징 방식을 사용하면 관리 대상 컴포넌트가 추가되고 SPOF 가 되지 않도록 클러스터링을 해야하기 때문에 복잡도는 증가하지만 가용성이 높고 여러 이점이 있기 때문에 event-driven microservice architecture 를 구현할 때 보통 많이 사용되는 방식이다. 또한 카프카와 같이 at-least-once 전달 방식의 메시지 브로커를 사용한다면 애플리케이션의 메시지 핸들러를 멱등하게 작성해야한다는 사실도 잊으면 안된다. 각 IPC 방식의 장단점이 있기 때문에 적절히 혼합하여 사용하는 것이 가장 좋을 것이다. 서로 다른 서비스의 엔드포인트를 알아내기 위해 서비스 디스커버리도 필요할 수 있다. Netflix 의 유레카 같은 도구를 쓰기도 하지만 K8S를 사용하면 굳이 별도의 서비스 디스커버리를 위한 도구를 고려할 필요는 없다. 아무튼 어떤 방법을 사용하던 관련 코드가 추가되고, 관리 대상 컴포넌트가 추가될 수 있기 때문에 MSA 의 경우도 모놀리식 애플리케이션과는 다른 이유에서 어느정도의 개발 및 운영 비용이 증가한다고 볼 수 있다. 분산 트랜잭션 모놀리식 애플리케이션에서는 하나의 로컬 DB 트랜잭션으로 여러 하위 도메인의 데이터를 ACID 하게 변경할 수 있다. 예를 들어 주식 거래 앱으로 사용자가 했던 주문이 체결되었을 때 하나의 로컬 트랜잭션으로 사용자가 보유한 현금은 줄이고, 주문 상태는 '체결됨' 상태로 변경하고, 보유 주식은 늘리는 것이 가능하다. 하지만 MSA 에서는 각 서비스가 DB 를 가지고, 기존에는 하나의 DB 에 모두 저장되었던 데이터는 각 서비스에 나뉘어져 관리된다. 그렇기 때문에 만약 주식 거래 서비스를 주문 서비스, 주식 잔고 서비스, 회계 서비스 등으로 구성한다면 각 서비스에서 실행되는 여러 로컬 트랜잭션을 하나의 글로벌 트랜잭션으로 묶어 ACID 하게 할 필요가 있는 것이다. ACID 트랜잭션에 대한 이해가 없다면 분산 트랜잭션 없이 그냥 매번 HTTP/gRPC 등으로 다른 서비스에 요청하여 데이터를 변경시키면 되는거 아닌가 할 수도 있는데, 이러면 데이터의 변경이 Atomic 하지 않기 때문에 중간에 서버나 인프라 오류 등 발생 시 데이터 정합성이 깨질 수 있으며, 트랜잭션 컨텍스트도 유실될 가능성이 있다. 아무튼 분산 트랜잭션을 하는 방법은 몇 가지가 있는데 우선 2PC[1] 방식으로 처리할 수 있다. 이 방식의 단점은 가용성이 낮다는 것이다. 분산 트랜잭션을 성공적으로 수행하기 위해서는 트랜잭션의 시작과 끝까지 트랜잭션에 참여하는 모든 컴포넌트가 정상적으로 실행 중이어야 하며 트랜잭션 커밋 요청을 정상적으로 수행해야 한다. 만약 한 컴포넌트라도 실행 중이지 않다면 전체 트랜잭션은 성공할 수 없을 것이다. 즉 트랜잭션에 참여하는 컴포넌트들에게 강한 의존성을 갖는다고 볼 수 있다. 또 다른 방법으로는 Saga 패턴이 있는데, 트랜잭션에 참여하는 컴포넌트들 간의 결합을 느슨하게 하여 2PC 방식보다 높은 가용성을 가질 수 있다[2]는 장점이 있어 요즘 많이 사용되는 방식이다. 사가 패턴에서는 각 로컬 트랜잭션이 비동기 메시지를 통해 순차적으로 커밋이 된다. 만약 중간에 문제가 발생하면 지금까지와 반대의 순서로 보상 트랜잭션을 실행한다. 하지만 사가 패턴은 ACID 에서 I(Isolation)가 빠진 ACD 트랜잭션이다. 즉 여러개의 트랜잭션이 동시에 실행될 경우 격리가 되지 않아 서로에게 영향을 미칠 수가 있다는 뜻이다. 이 때 발생할 수 있는 대표적인 문제는 한 트랜잭션이 변경한 데이터를 다른 트랜잭션이 덮어 쓰는 lost updates 나 한 트랜잭션이 실행되는 도중에 다른 트랜잭션이 변경한 데이터를 읽는 dirty reads 와 같은 것들이 있다. 이런 비격리로 인해 발생하는 문제를 해결하기 위한 여러 방법들이 있다. 높은 일관성을 요구하는 데이터만 사가 패턴이 아닌 2PC 방식을 혼용해서 사용할 수도 있다. 아무튼 사가 패턴을 사용할 경우 이런 부분을 꼼꼼하게 고려하지 않으면 큰 문제로 이어질 수 있기 때문에 신경써야할 부분이 더 많다. 뿐만 아니라, 사가 패턴을 사용하거나 도메인 이벤트를 발행해야 하는 경우 데이터의 변경과 메시지의 발행을 Atomic 하게 동작하도록 하기 위한 Transactional Messaging 도 고려해야 한다. DB 와 메시지 브로커는 트랜잭션 메커니즘이 다르기 때문에 서로의 operation 을 하나의 트랜잭션으로 묶을 수가 없다. 그렇기 때문에 이 둘에 따로 따로 쓰기를 수행할 경우(일명 dual-writes) 일부 데이터가 유실되어 inconsistent 하게 될 수 있기 때문에 DB 던 메시지 브로커던 둘 중 하나에만 쓰기를 수행해야 한다. 만약 메시지 브로커에만 쓰기를 수행하는 경우를 본다면, 메시지 브로커에 데이터 변경 이벤트를 쓰고 서버가 이를 구독하여 DB 업데이트를 할 수 있다. 이 경우 메시지의 발행과 구독 사이의 lag 이 존재하기 때문에 유저가 방금 변경한 데이터를 즉시 볼 수 없는 문제가 발생할 수 있다. 로컬 캐시를 통해 해결할 수도 있지만 다수의 인스턴스가 실행 중인 경우에는 문제가 복잡해진다. 그렇기 때문에 이 방식 보다는 반대로 DB 에만 쓰기를 하는 방식이 더 선호된다. DB 에 저장된 데이터의 변경과 발행하고자 하는 이벤트의 쓰기를 하나의 트랜잭션으로 묶는 Transactional Outbox Pattern 을 사용하는 것이다. Outbox 패턴을 사용한다면, 새로 삽입된 이벤트를 메시지 브로커에 발행하기 위해 CDC(Change Data Capture) 를 함께 사용한다. CDC 는 polling 방식이나 transaction log tailing 방식을 사용하여 구현할 수 있는데, polling 방식은 주기적으로 DB 쿼리를 하는 방식이기 때문에 구현이 간단한 대신 DB에 불필요한 부담을 줄 수 있으며 데이터 변경을 포착하는데 있어 약간의 delay 가 있을 수 있다는 단점이 있다. 반면, transaction log tailing 방식은 이름처럼 DB 트랜잭션 로그를 테일링하는 방식으로, 구현이 복잡하고 별도의 운영 프로세스가 늘어난다는 단점이 있지만 DB에 불필요한 부담을 주지 않고 데이터 변경 이벤트를 실시간으로 메시지 브로커에 보낼 수 있기 때문에 장기적인 관점에서는 더 좋다고 할 수 있다. Debezium 은 Kafka Connect 를 활용한 대표적인 transaction log tailing 방식의 CDC 도구이다. 시스템 요구사항에 따라 Orchestration Saga 와 Choreography Saga 중에 어떤 종류의 사가를 사용할지도 결정해야 한다. 만약 오케스트레이션 사가를 사용한다면 사가 코디네이터에 오케스트레이터 상태도 저장하고 관리해야 한다. 아웃박스 패턴이 아닌 Event Sourcing 을 사용할 수도 있다. 이벤트소싱에서는 이벤트가 데이터의 source of truth 이므로 데이터를 변경해야할 때는 Event Store 에만 이벤트를 발행하고 별도의 로컬 DB 업데이트를 할 필요가 없어지기 때문이다. Axon Framework 는 Outbox 패턴 대신 Event Sourcing 을 사용하는 대표적인 예다.[3] 쿼리 앞서 말한 것처럼 모놀리식 애플리케이션에서는 보통 모든 데이터가 하나의 DB 에 저장된다. 그렇기 때문에 여러 데이터를 조인해야하는 경우 조인 쿼리를 쉽게 할 수 있다. 하지만 MSA 의 경우 데이터가 서로 다른 DB 에 저장되므로 조인 쿼리를 할 수가 없다. 이를 해결하기 위한 방법은 크게 2가지가 있다. 첫번째는 API Composition Pattern 을 사용하는 것인데, 쉽게 말해 데이터를 인메모리 조인하는 것이다. 하지만 이 방식은 한계가 명확하다. 우선 데이터를 인메모리 조인하면 당연히 DB 조인보다 속도가 느리기 때문에 너무 많은 양의 데이터의 경우 응답 속도가 느려질 수 있다. 또 각 서비스에서 조인의 대상이 되는 데이터만 가져와야 하는데, 조건문에 해당하는 필드가 특정 서비스의 데이터에는 존재하지 않을 수가 있다. 그렇다고 모든 데이터를 가져올 수도 없다는 문제가 있다. 조인을 위한 또 다른 방법은 CQRS 를 사용하여 별도의 쿼리 용 뷰를 만드는 것이다. 데이터 조회를 위한 뷰 서비스를 만들고 도메인 이벤트를 구독하여 필요한 데이터의 복제본을 DB 에 저장하는 것이다. 이러면 조인에 필요한 데이터가 하나의 DB 에 저장되어 있으니 DB 조인이 가능하다. 테스트 모놀리식 애플리케이션에서는 도메인 서비스간의 데이터 상호작용은 단순 메서드 콜이었다. 하지만 도메인이 서로 다른 서비스로 분리되면서 IPC 를 통해 상호작용해야하게 된다. 서비스간의 통신은 REST 나 gRPC 같은 동기 방식일 수도 있고, 카프카를 통한 비동기 메시징 방식일 수도 있다. 어떤 방식이 되었건 같은 범위에 대한 테스트를 작성하더라도 모놀리식에서는 인프라가 개입되지 않았기 때문에 단순히 일부 객체의 mock 만 떠서 주입시켜 주면 되는 경우에도 MSA 에서는 네트워크 통신이 개입되기 때문에 mock server 를 띄우거나 Spring Cloud Contract 와 같은 별도의 라이브러리가 필요할 수도 있다. 만약 각 서비스를 서로 다른 팀이 개발/운영하는 경우 각 API 에 대한 테스트가 Consumer-Driven Contract Testing 과 같은 좀 더 복잡한 과정을 수반해야할 수도 있다. 운영 모놀리식 애플리케이션의 단점 중에 개발 및 운영이 어렵다는 것이 있었는데 MSA 도 다른 이유로 개발 및 운영 비용이 올라간다. 일단은 아무래도 관리의 대상이 되는 컴포넌트의 수가 많아지기 때문이다. 하나의 서비스가 여러개의 작은 서비스로 나누어지고, 위에서 말한 쿼리용 서비스를 포함하여 추가 컴포넌트가 필요할 수도 있다. 클라이언트가 여러 종류의 데이터를 필요로 하는 경우를 생각해보자. 모놀리식 애플리케이션의 경우 이를 위한 API 를 만들면 클라이언트는 한 번의 요청으로 필요한 데이터를 모두 가져올 수 있다. MSA 는 필요한 데이터가 서로 다른 서비스에 의해 관리되고 있으니 각 서비스에 직접 데이터를 요청하기 위해서는 여러번 요청해야할 것이다. 하지만 이렇게 했을 경우에는 서버 API 스펙이 클라이언트 코드에 존재하여 유지보수 측면에도 좋지 않고, 성능 면에서도 좋지 않을 것이다. 이를 해결하기 위해서는 이를 모아주는 API 가 필요하다. 기존 서비스 중 하나에 이 API 를 구현할 수도 있지만 관심사의 분리 관점에서 좋지 않으니 별도의 컴포넌트에 API 를 구현하여 이를 수행하도록 하는 것이 좋을 것이다. API Gateway 가 이 역할을 수행할 수도 있다. 로그 파일이 분산된다는 단점도 있다. 물론 로깅 라이브러리나 Elastic Stack 등을 활용하여 로그 수집 파이프라인을 구축하면 해결할 수 있다. 트러블 슈팅과 모니터링이 상대적으로 어렵다는 단점도 있다. Istio[4] 같은 서비스 메쉬를 사용하면 어느정도 커버할 수 있다. 보안 도메인 서비스 간의 상호작용 시 모놀리식에서는 하나의 프로세스 내에서의 메서드 콜이었지만 MSA 에서는 네트워크 통신이기 때문에 암호화에 더 신경을 써야한다. Istio 를 활용하여 기존의 코드를 변경하지 않고도 mTLS 를 통해 비교적 쉽게 암호화를 적용할 수 있다. 결론 위와 같은 부분을 몸소 경험해보면 MSA 의 여러 장점에도 불구하고 MSA 을 도입하는 것이 최선인지 더 고민을 하게 된다. 개인적으로는 만약 팀의 규모가 작거나 시스템의 규모가 작다면 처음부터 MSA 로 시스템을 개발하기 보다, 우선 모놀리식으로 서비스를 개발하고 나서 추후에 필요성이 느껴질 때 MSA 구조로 바꾸는 것이 좋지 않을까 하는 생각이다. 어떻게 보자면 모놀리식 애플리케이션을 개발하고 운영하는 비용이 MSA 로 개발하고 운영하는 비용보다 높아질 때가 MSA 로 전환해야하는 시점일 것이다. 비용을 정량화하기는 어렵겠지만 가능한 이 시점을 잘 파악하여 마이그레이션 하는 것이 좋을것 같다. 또한, 초기에 모놀리식으로 개발을 할 때 추후 별도의 서비스로 떼어내기 쉽도록 도메인 주도 설계(DDD)를 하고 모듈을 잘 분리하면 좋을것이다. 예를 들어 만약 여러 엔티티 클래스들이 서로를 레퍼런스 참조하도록 구현했다면 추후 각 서비스를 추출하는 것이 굉장히 어려운 작업이 될 것이다. 당연한 얘기지만 다른 JVM 에 있는 객체를 참조할 수는 없기 때문이다. 따라서 MSA 전환을 고려한다면 애그리거트 루트끼리의 참조는 PK를 통해 간접 참조하도록 설계하는 것이 좋다. 모놀리스를 여러개의 마이크로 서비스로 분해할 때 처음부터 다시 개발하거나 한 번에 마이그레이션 하는 것은 굉장히 시간이 오래 걸리며 비즈니스 상황에서는 비현실적인 작업이다. 리팩토링보다는 당장 새로운 기능을 개발하거나 기존의 기능을 수정하는 작업이 가장 중요할 것이기 때문이다. 그렇기 때문에 스트랭글러 패턴(Strangler Pattern)[5]을 사용하여 새로운 비즈니스 요구사항을 처리하면서 동시에 한 서비스씩 점진적으로 분리하는 것이 좋다. 2PC 는 2 Phase Commit 의 약자로 간단하게 말하자면 분산 트랜잭션을 관장하는 코디네이터가 분산 트랜잭션에 참여하는 컴포넌트에게 로컬 트랜잭션 커밋을 요청하는 1단계와 실제로 로컬 트랜잭션을 커밋하는 2단계로 이루어진 방식이다. ↩︎ 2PC 는 분산 트랜잭션에 참여하는 모든 컴포넌트가 동시에 구동 중이어야 트랜잭션이 실행될 수 있으므로 2PC 의 가용성은 각 컴포넌트의 가용성의 곱이 된다. 반면 사가의 경우 만약 어떤 트랜잭션 참여자가 다운이 되더라도 나중에 재구동되고 난 후에 멈춰진 부분부터 시작할 수 있으니 꼭 모든 컴포넌트가 동시에 구동 중이지 않더라도 트랜잭션이 진행될 수 있으므로 상대적으로 높은 가용성을 가질 수 있는 것이다. 보통 사가에 참여하는 컴포넌트는 메시지 브로커를 통해 통신한다. ↩︎ : https://discuss.axoniq.io/t/the-outbox-pattern/2031 ↩︎ Istio 는 각 파드에 Envoy 사이드카 프록시를 주입하여 기존의 코드를 (거의)전혀 변경하지 않고도 Circuit Breaking, Retry, Telemetry 같은 다양한 기능을 쉽게 사용할 수 있게 한다. 그리고 Jaeger, Kiali, Prometheus, Grafana 애드온을 통해 비교적 쉽게 분산 트레이싱, 트래픽 관리, 시스템 모니터링 등을 할 수 있게 하여 마이크로서비스 간의 통신을 high-level view 에서 관장하는데 큰 도움이 된다. ↩︎ 스트랭글러 덩굴은 열대 우림 지대에서 흔히 자라는 식물로, 숲 꼭대기 너머로 햇볕을 쬐기 위해 나무 주위를 칭칭 감고 자란다. 언젠가 나무 전체를 덩굴로 뒤덮어 나무가 수명이 다 되어 죽으면 나무 모양의 덩굴만 덩그러니 남게 된다. 스트랭글러 패턴은 MSA 에서 스트랭글러 애플리케이션이 기존 모놀리스의 역할을 점점 작게 만들어 결국에는 소멸시키는 모양을 이 스트랭글러 덩굴에 비유한 것이다. 모놀리스에서 한 서비스씩 분리해 나갈 때 이 분리된 서비스들을 통틀어 스트랭글러 애플리케이션이라고 부르는데, 이 스트랭글러 애플리케이션이 점점 커지다 보면 어느새 원래의 모놀리스는 점점 작아져 사라져 버리거나 하나의 마이크로 서비스가 된다. ↩︎","link":"/2021/10/22/about-msa/"},{"title":"Red Hat 오픈소스에 기여한 이야기","text":"얼마전 Debezium(디비지움)이라는 오픈소스 프로젝트에 처음으로 코드로 기여를 하였다. 이전까지는 여러 오픈소스 프로젝트의 README 나 문서에 대한 컨트리뷰션 밖에 해보지 못했는데 드디어 코드로써의 기여를 하게 되었다. 비록 특별한 내용은 없지만 나의 경험이 처음으로 오픈소스 컨트리뷰션을 하고자 하는 사람들에게 조금이나마 도움이 되었으면 하는 마음에서 글을 작성하게 되었다. Debezium(디비지움)은 Red Hat 에서 관리되는 프로젝트이며, CDC(Change Data Capture) 를 위한 도구이다. CDC는 간단히 말해서 데이터의 변화를 포착하여 적절한 처리를 할 수 있도록 하는 행위를 말하는데, 데이터 파이프라인(ETL) 이나 마이크로서비스 아키텍처(Outbox Pattern) 등 다양한 곳에서 활용된다. 동기 Debezium 에 기여하게된 동기에 대한 이야기를 하기 전에 아주 간단하게 Debezium 에 대한 설명을 하는 것이 좋을것 같다. Debezium 의 특징 Debezium 은 기본적으로 DB의 replication 메커니즘에서 사용되는 transaction log(e.g. MySQL 의 binlog 와 MongoDB 의 oplog)를 tailing 하여 실시간으로 데이터의 변경을 포착한다. 현재 지원하는 DB는 8가지(MySQL, MongoDB, PostgreSQL, Oracle, SQL Server, Db2, Cassandra, Vitess)이다. Debezium 를 사용하는 방법은 크게 3가지가 있다. 우선 Kafka Connect 의 connector 로 실행시키는 것이 가장 일반적인 사용 방법이다. 디비지움은 각 DB 별 connector 를 제공한다. 이 방식을 사용할 경우 Debezium connector 는 데이터 변경 이벤트를 Kafka 에 실시간으로 전송하게 된다. 두번째는 Debezium Engine 라는 것을 사용하는 것인데, 애플리케이션에서 API를 통해 디비지움의 CDC 기능을 사용할 수 있다. 마지막으로는 Debezium Server 라고, 디비지움에서 자체적으로 만든 별도의 서버를 띄우는 방법이다. 참고로 이 방식은 아직 인큐베이팅 상태라서 향후 기능이 변경 될 가능성이 있다. 두번째와 세번째 방식은 첫번째 방식과는 다르게 Kafka 를 거치지 않고도 다른 곳으로 데이터를 바로 보낼 수 있다. Kafka Connect 는 SMT(Single Message Transforms) 라는 기능을 제공하는데, source connector 가 카프카에 레코드를 쏘기 전에, 혹은 sink connector 가 카프카에서 레코드를 가져온 후에 Key, Value, Header 등 레코드의 정보를 변경할 수 있도록 한다. Debezium 도 자체 connector 와 함께 사용할 수 있는 SMT 를 몇 가지 제공한다. 그래서 어떤 기여를, 왜 했나 Event-driven MSA로 서버 아키텍처를 구성하던 도중 트랜잭셔널 아웃박스 패턴을 위해서 CDC 기능이 필요했다. 그래서 어떤 도구를 사용할까 살펴보던 도중 Debezium 이 눈에 들어왔다. 어느정도 훑어보니 괜찮아 보여서 한 번 사용해보고 싶었는데, SQL DB 의 경우 즉시 사용할 수 있는 SMT 가 존재했지만, MongoDB 는 직접 커스텀 SMT 를 개발해야 했다. 기존의 Outbox SMT 가 MongoDB 를 지원하지 않는 이유는 MongoDB 의 connector 가 만들어내는 Record 에서 변경된 이후의 데이터를 담고있는 after 필드의 구조가 다르기 때문이다. Debezium 에서 제공하는 다른 SMT의 경우도 SQL DB 용과 MongoDB 용이 별도로 존재하는 경우가 있다. 아무튼 그래서 이참에 그냥 내가 직접 개발해서 컨트리뷰션을 하면 어떨까 생각을 하게 되었다. 그래서 디비지움 IRC 에서 메인테이너에게 이 기능에 관심이 있다고 했고, 메인터이너는 기존에 있던 이슈의 링크를 주었다. 참고로 디비지움은 원래 Gitter 를 통해 IRC 채널을 운영했는데, 2021년 하반기부터는 Zulip을 사용하고 있다. 이슈를 살펴보니 이슈의 작성자가 설계에 대한 고민을 적어두었는데, 내가 했던 고민과 정확히 같았다. 그래서 메인테이너에게 내가 생각하는 구현 방식을 제안해봤더니 좋은것 같다고 해서 개발을 시작하게 되었다. 과정 우선 CONTRIBUTE.md 의 내용을 정독하는 것이 굉장히 중요했다. 프로젝트마다 차이는 있을 수 있겠지만 디비지움의 경우는 컨트리뷰션 가이드가 꽤 상세하게 기술되어 있다. 그래서 초반에 드는 궁금증 중에 상당 부분은 이곳에서 답을 찾을 수 있다. PR 과 커밋이 따라야하는 규칙들, 커밋 메시지의 형식, 테스트 방법, code formatting 방식 등의 대한 정보가 적혀있는데, 예를 들자면 새로운 기능을 추가하는 PR은 항상 그 기능에 대한 단위 테스트/통합 테스트, 그리고 문서도 함께 작성돼야 한다는 규칙이 존재한다. 위와 같은 규칙에 따라 문서도 함께 작성하게 되었다. 물론 기존의 SQL DB 용 SMT 와 동일한 기능을 제공하는것이 목표였기 때문에 기존의 문서와 전체적인 짜임새는 비슷하게 갈 수 있었다. 개발 이렇게 큰 프로젝트에 기여를 막상 하려고하면 처음엔 굉장히 막막하다. 그래서 우선 전체적인 동작 원리와 사용 방식에 대한 감을 잡기위해 최대한 많은 정보를 활용하려고 했다. 우선 프로젝트의 전체적인 그림과 방향성을 파악하기 위해 깃헙 레포를 Watch 하고 구글 그룹에 가입하여 실시간으로 개발자들간의 토론이나 새로운 이슈에 대한 알림을 받아보았다. 공식 문서와 블로그의 글을 많은 부분 정독했고, 가장 관련이 있는 기능들의 비즈니스 로직과 단위/통합 테스트 코드를 참고했다. 참고했던 주요한 부분은 기존의 SQL DB 용 Outbox SMT 인 Outbox Event Router 와 MongoDB connector 에서 JSON string 형태의 envelope 을 flattening 할 때 사용되는 SMT 인 MongoDB New Document State Extraction 이었다. 잘 작성된 테스트 코드는 클래스의 기능을 잘 나타내기 때문에 테스트를 하나씩 실행시키면서 테스트 코드를 분석했다. 그러면서 어떤 기능들이 필요한지 둘러보았고, 테스트는 어떤 식으로 작성해야하는지 파악했다. 전체적인 그림이 잡혀나가기 시작할 무렵, 우선 필요한 기능들에 대한 단위 테스트를 작성했고, 이것들을 모두 통과하기 위한 실제 코드를 작성하였다. 개발 중간마다 설계에 대한 고민과 이 SMT 를 유저가 어떻게 사용하게 할 것인지에 대한 여러가지 고민을 하기도 했는데, 이건 IRC 에서 메인테이너들과 이야기를 나누거나 코드 리뷰를 통해 개발 방향을 명확히 하였다. PR 을 올리고 나서 받은 다양한 피드백 중에 동의하는 부분은 수정했고, 일부 의견에 대해서는 대화가 필요하기도 했다. 나는 내가 고민한 부분과 그것에 대한 솔직한 나의 의견에 대해 가능한 자세하게 말했다. 물론 전체 프로젝트에 대해서는 메인테이너가 훨씬 잘 알것이고 실력도 나보다 뛰어나겠지만 큰 규모의 프로젝트를 관리하다보면 디테일한 부분을 모두 기억하지 못 할 수도 있고, 적어도 내가 작업한 부분에 대해서는 나도 어느정도 잘 안다고 생각했기 때문에 자신감과 오너십을 가져야한다고 생각했다. 후속 작업 애그리거트 ID 기본 타입 변경 Outbox 콜렉션에서 aggregateid 의 기본 타입을 String 으로 개발을 진행했는데, 메인테이너가 ObjectId 로 변경하는 것이 어떻겠냐고 했다. 만약 같은 생각이면 후속 PR로 이 부분을 수정하면 좋을것 같다고 했는데 나도 같은 생각이라 이 부분에 대한 작업을 하면 좋을것 같다. + 추가(21.11.21): 후속 PR을 통해 변경을 완료했다. 샘플 프로젝트 디비지움은 debezium-example이라는 레포를 통해 Debezium 을 활용한 다양한 예제를 제공한다. 기존 SQL 용 Outbox SMT 에 대한 예제도 존재한다. 이곳에 MongoDB Outbox Event Router 에 대한 예제 프로젝트도 만들어야하는데 이 부분도 개발할 생각이 있는지 제안을 받았다. 그래서 시간이 날 때 추가 기여를 하려고 생각 중이다. + 추가(21.11.27): 후속 PR을 통해 예제 추가를 완료했다. 메인테이너가 로컬에서 머지를 해서 원래 PR 은 closed 가 됐다. 하지만 main 브랜치에는 커밋이 잘 들어가 있는 것을 확인할 수 있다. Embedded Document 형태를 지원하는 옵션 MongoDB 같은 Document DB 의 경우 일반적인 관계형 DB 와 데이터 모델링을 하는 방식이 조금 다르다. 보통 높은 정규화를 하지 않고 여러 데이터를 하나의 Document 에 저장하도록 데이터를 모델링하는 경우가 많기 때문에 multi-document 트랜잭션을 사용하지 않고 Outbox Pattern 을 사용할 수 있도록 하는 기능을 제공하면 좋겠다고 생각했다. 그래서 처음에 Jira 이슈에서도 우선은 별도의 Collection 으로 Outbox 데이터를 저장하여 multi-document 트랜잭션을 사용하도록 하고, 추후 업데이트를 통해서 이런 기능을 사용할 수 있는 설정을 추가 개발하면 어떻겠냐고 제안했었다. 그 외 MySQL 과 MongoDB Connector 의 코어 로직에 대한 기여도 해보고 싶어서 소스 코드를 분석하고 있다. 또, Jira 에 올라와있는 다양한 이슈들을 보면서 현재 어떤 작업들이 필요한지 둘러보고 이 중에 관심있는 이슈는 watch 기능으로 진행 상황을 지켜보려고 하고있다. 느낀점 개발자가 새로운 팀에 합류하게 되면 기존에 개발되어 있는 방대한 양의 코드와 아키텍처를 빠르게 파악하고 기존의 규칙들을 정확하게 준수하여 코드를 기여해야 하는 경우가 많을 것이고, 기존 기능을 변경하거나 새로운 기능을 추가 하기 위해 다른 개발자와의 커뮤니케이션이 중요할 것이다. 이러한 점에서 다양한 오픈소스에 기여를 하는 경험이 실제 업무를 하는데도 큰 도움이 될 것 같다는 생각이 들었다. 또, 영어로 전세계의 다양한 개발자들과 소통을 하며 오픈소스에 기여함으로써 외국 기업에서의 협업을 간접적으로 경험할 수 있는것 같다. 그래서 만약 나중에 실제 업무에서 영어로 협업을 해야하는 상황이 온다면 이런 경험이 조금이나마 도움이 되지 않을까 생각이 든다. 추가로, 잘 관리되는 오픈소스 프로젝트의 소스코드를 분석하면 좋은 설계와 코드를 경험할 수 있어 좋은것 같다. 평소에 오픈소스를 '사용할 때'에는 직접 작성할 일이 많지 않을 수 있는 정교한 최적화나 로우 레벨 코드도 볼 수 있는 경우도 있어서 공부가 되기도 한다. 이러한 점에서 앞으로도 Debezium 뿐 아니라 다양한 오픈소스의 코드를 분석하여 기여해보려고 한다. 비록 이번에 기여한 기능은 복잡한 기능은 아니었기 때문에 상대적으로 어렵지 않았지만, 다른 오픈소스 프로젝트의 코드에도 기여할 수 있다는 자신감이 생겨서 시간이 날 때 내부 동작을 자세하게 알고 싶은 다양한 오픈소스들의 코드를 분석해 보려고 한다.","link":"/2021/11/18/contribution-to-debezium/"},{"title":"Git 의 서브모듈(Submodule)","text":"Git 의 서브모듈(Submodule) 이란 하나의 저장소 안에 있는 또 다른 별개의 저장소이다. 보통 다른 원격 저장소를 가져와(pull) 서브모듈로 사용하게 된다. 본 포스트에서는 Git 의 서브모듈에 대해 알아본다. 서브모듈 시작하기 myblog 라는 프로젝트 디렉터리에서 블로그를 개발하다가, chat-module 이라는 채팅 모듈을 원격 저장소에서 가져와 블로그 프로젝트에서 사용하고 싶다고 가정하자. 현재 myblog 로컬 저장소에는 Comment.java, Post.java 2개의 파일이 있고, chat-module 원격 저장소에는 Chat.java 파일이 있다. myblog 디렉터리에서 chat-module 저장소를 서브모듈로 사용하기 위해 다음 명령어를 사용한다. scriptgit submodule add https://github.com/sgc109/chat-module.git 그럼 아래와 같이 .gitmodules 와 chat-module/ (Chat.java 와 함께)가 생성되어 스테이징 된다. 이때 .gitmodules 파일을 열어보면 다음과 같이 서브모듈에 대한 정보(이름, 경로, 원격 저장소 url)가 적혀있다. 또, 루트 디렉터리에서 git diff --cached chat-module 를 실행하면, (참고로, --cached 와 --staged 는 같다) 다음과 같이 서브 디렉터리 내 모든 파일에 대한 정보가 자세히 나오는 것이 아니라 어떤 하나의 커밋에 대한 정보만 적혀있다. 아래와 같이 스테이징된 변경사항을 commit 해보면, 일반적인 파일처럼 100xxx 의 형태로 표시되는 .gitmodules 와는 달리 chat-module 디렉터리는 160000 의 형태로 하나의 특수한 파일로 인식한다는 것을 확인할 수 있다. 해당 커밋을 Github 에 push 하면 서브모듈은 다음과 같은 아이콘으로 표시된다. 서브모듈 포함한 프로젝트 Clone 팀 내 다른 개발자가 앞서 서브모듈을 포함하여 push 된 원격 저장소를 clone 하고 싶다고 하자. 아래와 같이 git clone 명령어를 실행한다. 보이는 것처럼 서브모듈 디렉터리 내 파일들은 하나도 가져오지 않는다. 메인 프로젝트 입장에서 서브모듈은 사실 단지 현재 가리키는 커밋과 변경 여부만 적혀있는 하나의 파일에 불과하기 때문이다. 서브모듈 디렉터리에서 변경사항을 만들고 상위 디렉터리에서 git diff 를 해보면 이 사실을 알 수 있다. 그래서 서브모듈 디렉터리에서 2가지 명령어를 추가로 실행해야 서브모듈의 파일을 모두 가져올 수 있다. scriptgit submodule initgit submodule update git submodule init 은 서브모듈 디렉터리를 git 로컬 저장소로 초기화 해주고(git init 처럼) git submodule update 는 서브모듈의 원격 저장소에서 파일을 가져온다. 최신 commit 을 가져 오는건 아니고 .git/modules/chat-module/HEAD 에 서브모듈이 어떤 커밋을 가리키는지 명시돼 있어서 해당 커밋을 가져온다. 혹은, 애초에 clone 을 할 때 --recurse-submodules 옵션을 주면 굳이 모든 서브모듈 디렉터리에서 2개의 추가 명령어를 실행하지 않아도 한방에 초기화를 할 수 있다. scriptgit clone --recurse-submodules https://github.com/sgc109/myblog.git 서브모듈 포함한 프로젝트 작업 서브모듈 업데이트하기 만약 다른 개발자가 채팅 모듈에 Message.java 를 추가하여 chat-module 의 원격 저장소에 변경 사항을 push 했다고 가정하자. 그럼 chat-module 서브모듈을 사용하는 myblog 로컬 저장소에서는 이를 반영해야 할 것이다. 기본적인 방법은 서브모듈 디렉터리에서 다음 2가지 명령어를 입력하는 것이다. scriptgit fetchgit merge origin/master 하지만 더 간단한 방법이 있다. 서브모듈 디렉터리에서 2개의 명령어를 입력하는 대신 루트 디렉터리에서 다음 명령어를 입력하면 된다. scriptgit submodule update --remote chat-module 이때 만약 로컬에서도 서브모듈에 변경이 있는 경우에는 --merge 나 --rebase 옵션을 줘야한다. 기본 브랜치 변경하기 참고로 지금까지의 명령어들은 기본적으로 서브모듈 원격 저장소의 master 브랜치를 기준으로 동작한다. 만약 기본 브랜치를 예를 들어 bugfix-chat 로 변경하고 싶다면, 다음 명령어를 사용하면 된다. scriptgit config -f .gitmodules submodule.chat-module.branch bugfix-chat 서브모듈 수정 사항 공유하기 myblog 에서 작업을 하다가 서브모듈인 chat-module 의 내용에 변경이 필요한 경우도 있을 것이다. 이럴 때는 먼저 서브모듈의 내용을 원격 저장소에 반영한 뒤, 메인 프로젝트의 내용을 Push 해야한다. 그렇지 않으면 메인 프로젝트를 Pull 한 다른 개발자들은 변경된 서브모듈의 내용을 알 수 없기 떄문이다. 하지만 서브모듈의 내용을 Push 하는 것을 깜빡할 수도 있는데, 이때 --recurse-submodules 라는 유용한 Push 옵션이 있다. scriptgit push --recurse-submodules=checkgit push --recurse-submodules=on-demand 이 옵션을 check 로 주면 서브모듈이 Push 되지 않았다면 메인 프로젝트의 Push 가 중단되며, 옵션을 on-demand 로 주면 자동으로 서브모듈을 Push 한다. 물론 단순히 서브모듈 디렉터리에서 직접 Push 를 해주고 메인 프로젝트를 Push 할 수도 있다. 서브모듈 Merge 하기 내가 서브모듈의 내용을 수정하고 있는데 다른 사람도 수정하여 Upstream 에 Push 했을 경우 Pull 을 했다고 하자. 만약 서브모듈의 커밋이 단순히 Fast-Forward 관계라면 최신 커밋을 선택한다. 하지만, 그렇지 않다면? 충돌이 발생하게 된다. 이때 메인 디렉터리에서 git diff 명령을 통해 충돌이 난 서브모듈의 두 커밋의 SHA 해시 값을 알 수 있다. 그럼 일단 Upstream 의 커밋으로 새로운 브랜치를 만들고, 서브모듈 디렉터리에서 Merge 를 한다. script$ cd chat-module$ git branch upstream-commit c771610$ git merge upstream-commit 그럼 물론 충돌이 발생하고, 이를 서브모듈 디렉터리에서 해결한 뒤 커밋을 하고, 다시 메인 디렉터리로 가서 변경 사항을 스테이징에 올린 뒤 커밋을 하면 된다. script$ git add conflicted-file$ git commit -m 'merged 2 commits in submodule'$ cd ..$ git add chat-module$ git commit -m 'all merged in main directory' 서브모듈 팁 Foreach 여러개의 서브모듈을 포함한 프로젝트에서 각 서브모듈에 공통적인 명령어를 수행하고 싶을 때가 있다. 예를 들어 작업 도중 메인 프로젝트에서 다른 브랜치로 변경하고 싶은데 메인 프로젝트와 여러 서브모듈에 스테이징은 됐지만 아직 Commit 하지 않은 변경사항이 있는 경우다. 직접 모든 서브모듈 디렉터리에서 git stash 를 하기엔 너무 번거롭다. 그럴 땐 다음과 같이 foreach 를 사용하면 유용하다. scriptgit stash; git submodule foreach 'git stash' 그리고 다음 명령어로 모든 서브모듈과 함께 새로운 브랜치로 이동할 수 있다. scriptgit checkout -b bugfix-post; git submodule foreach 'git checkout -b bugfix-post' 모든 서브모듈을 포함한 변경사항을 알고싶은 경우 다음과 같이 활용할 수도 있다. scriptgit diff; git submodule foreach 'git diff' Alias 앞서 살펴본 submodule 과 관련된 명령문은 너무 길다는 단점이 있다. 이럴 땐 다음과 같이 서브모듈과 관련된 명령어들의 Alias 로 등록하면 편하다. script$ git config alias.sdiff '!'\"git diff &amp;&amp; git submodule foreach 'git diff'\"$ git config alias.spush 'push --recurse-submodules=on-demand'$ git config alias.supdate 'submodule update --remote --merge' 참고로 alias 설정에서 앞에 '!' 와 같이 느낌표가 붙으면 git 명령어가 아닌 shell 명령어라는 의미이다. 참고: https://git-scm.com/book/ko/v2/Git-도구-서브모듈","link":"/2020/07/16/git-submodule/"},{"title":"[책 리뷰] Head First Design Patterns","text":"헤드 퍼스트 디자인 패턴(Head First Design Patterns) 이라는 책을 읽어보았다. GoF (1994) vs Head First (2004) 디자인 패턴과 관련된 가장 유명한 책을 2권 뽑는다면 한 권은 1994년에 발간된 Design Patterns: Elements of Reusable Object-Oriented Software(번역본 제목으로는 GoF의 디자인 패턴) 이고, 나머지 한 권은 2004년에 발간된 Head First Design Patterns 일 것이다. GoF 디자인 패턴은 최초로 디자인 패턴을 정리한 책이고 매우 다양한 패턴을 카탈로그 형식으로 정리했다는 점에서 유명하지만, 워낙 오래된 책이라 요즘에는 잘 쓰이지 않거나 조금 변형된 패턴들도 수록되어 있다고 알고있다. 반면, Head First 는 가장 자주 쓰이는 디자인 패턴들 위주로 집중적으로 설명하고 있다. 게다가 구어체를 사용하고 있으며, 각 패턴을 설명하기 위해 설정된 스토리가 너무 적절하여 매우 이해하기가 쉽다. 또한, 보통 디자인 패턴과 관련된 설명들은 가끔 너무 추상적이라 초보자에게는 이해가 간듯 안 간듯 할 때가 있는데, 이 책은 질의응답 코너가 있어서 사소할 수도 있는 질문들을 자문 자답하며 많은 궁금증을 해결 해주어 머릿속 개념이 선명해지는 것을 느꼈다. 디자인 패턴을 알아야 하는 이유 보통 소프트웨어를 유지 및 보수하는 데는 소프트웨어를 개발하는 만큼의, 혹은 더 많은 양의 비용과 노력이 들기 때문에 확장성, 혹은 유지 보수성은 매우 중요하다. 디자인 패턴을 잘 활용하면 확장성있는 유연한 소프트웨어를 작성하는데 도움이 된다. 또한, 디자인 패턴을 알면 다른 개발자와 소프트웨어의 설계에 대해 이야기 할 때 장황한 설명 없이 특정 패턴의 이름을 말하는 것만으로도 어떤 구조를 말하는 건지 의도를 빠르고 명확하게 전달할 수 있다는 점에서 커뮤니케이션 실수를 줄이고 생산성을 높이는데 큰 도움이 된다. 게다가 이미 존재하는 수많은 복잡한 시스템이나 프레임 워크, 혹은 API 들은 대부분 디자인 패턴을 활용하여 작성 되었다. 그렇기 때문에 디자인 패턴을 알면 소스 코드를 조금만 훑어봐도 어떤 패턴을 사용하여 구현된건지 금방 눈치 챌 수 있어 큰 그림을 이해하는데 도움이 된다. 그럼 이 책의 구성과 내용을 간략하게 알아보자. 책의 구성 및 내용 이 책은 자주 쓰이는 디자인 패턴 15가지에 대해 구체적으로 설명하는데, 이 15가지 패턴은 다음과 같다. Strategy 패턴 Observer 패턴 Decorator 패턴 Factory Method 패턴 &amp; Abstract Factory 패턴 Singleton 패턴 Command 패턴 Adapter 패턴 &amp; Facade 패턴 Template Method 패턴 Iterator 패턴 &amp; Composite 패턴 State 패턴 Proxy 패턴 Compound 패턴 부록에서는 상대적으로는 덜 쓰이지만 유명한 패턴 9가지에 대해 간략히 소개한다. 9가지 패턴은 다음과 같다. Bridge 패턴 Builder 패턴 Chain of Responsibility 패턴 Flyweight 패턴 Interpreter 패턴 Mediator 패턴 Memento 패턴 Prototype 패턴 Visitor 패턴 이 책의 좋은점 이 책은 단순히 여러 디자인 패턴들에 대한 설명만 하는 것이 아니라, 객체지향의 원칙들에 대해 설명하면서 어떤 설계가 좋고, 왜 좋은지 설명해준다. 또한 비슷한 디자인 패턴들을 비교하며 공통점과 차이점에 대해 명확히하며 자칫하면 헷갈릴 수도 있고, 의문이 드는 점들을 잘 해결해준다. 게다가 디자인 패턴이 만능이 아니며, 법칙이 아니라 도구에 불과하다는 조언도 담겨있다. 초보자가 저지르기 쉬운 실수는, 무리하게 디자인 패턴을 사용해서 불필요하게 프로그램의 복잡도를 높이는 것이다. 지금 당장 변경되거나 확장될 여지가 별로 없다면 디자인 패턴을 사용하지 않는 것이 더 좋다. 그리고 굳이 디자인 패턴을 사용하지 않고 단순히 객체 지향 원칙을 적용하는 것만으로도 해결할 수 있는 문제의 경우에도 디자인 패턴을 사용하지 않는 편이 좋다. 이런 내용까지 폭넓게 다루기 때문에 책을 모두 읽었을 때 개발자로서 조금은 성장한 듯한 느낌을 받을 수 있다고 생각한다. 결론 누구든 만약 객체지향 프로그래밍에 대해서 잘 알고 싶다면, 게다가 아직 디자인 패턴을 공부한 적이 없는 사람이라면 개인적으로 꼭 읽어봤으면 하는 책이다. 특히 대학생, 혹은 신입 개발자가 있다면 정말 강력하게 추천하고 싶은 책이다. 참고1: https://github.com/sgc109/design-pattern-study 참고2: https://sgc109.github.io/2020/07/18/compound-pattern-feat-mvc/","link":"/2020/07/19/head-first-design-patterns/"},{"title":"Compound 패턴 (feat. MVC 패턴)","text":"Compound 를 사전에서 찾아보면 복합체, 혼합물 등의 뜻이 나온다. 그렇다면 Compound 패턴은 무엇일까? Compound 패턴이란? 컴파운드(Compound) 패턴은 이름 그대로 여러 디자인 패턴이 혼합된 디자인 패턴을 말한다. 하지만 단순히 여러 패턴이 사용되었다고 해서 컴파운드 패턴인 것은 아니다. 여러 패턴이 사용되는 동시에 일반적인 문제를 해결하는데 반복적으로 사용될 수 있어야 한다. Compound Pattern 의 대표적인 예가 바로 그 유명한 MVC Pattern 이다. MVC 패턴이란? MVC 는 Model-View-Controller 의 약자로서, 역할에 따라 3개의 컴포넌트로 분리하고 여러 디자인 패턴을 적용하여 재사용성을 높인 대표적인 컴파운드 패턴의 예다. 그렇다면 MVC 패턴에서 사용된다는 여러 디자인 패턴은 대체 무엇일까? 전통적인 MVC 패턴에서는 다음 3가지 패턴이 사용된다. 옵저버(Observer) 패턴[1] Model 의 상태가 변경 되었을 때 Controller, 혹은 View 에게 이 사실을 알리는데 사용된다. 컴포지트(Composite) 패턴[2] View 를 구성하는 컴포넌트들은 계층 구조를 이룬다. (e.g. Java Swing 의 JFrame/JLabel 등, Android 의 View/ViewGroup, HTML 의 DOM) 스트래티지(Strategy) 패턴[3] Controller 의 핵심 기능을 인터페이스로 분리하여 View 가 이 인터페이스를 통해 Controller 를 구성(Composition) 한다. 그렇기 때문에 View 는 Controller 를 갈아 끼우며 기능을 변경할 수 있다. 또한, 필요에 따라 어댑터(Adapter) 패턴[4] 을 함께 사용할 수도 있다. MVC 패턴은 사용되는 곳에 따라(모바일, 웹 등) 여러 프레임워크에서 다양한 형태로 변형되어 적용되곤 한다. 하지만 이번 섹션에서는 변형되지 않은 전통적인 MVC 패턴에 대해 알아보고, 이어지는 섹션에서 웹 버전의 MVC 인 JSP Model 2 에 대해 알아본다. 그렇다면 전통적인 MVC 의 3가지 컴포넌트에 대해 알아보자. 모델(Model) Model 은 애플리케이션의 핵심 로직과 데이터를 가지고 있는 컴포넌트다. Controller 가 Model 에게 상태 변경을 요청하면 Model 은 일련의 과정을 거쳐 자신의 상태를 변경하게 되고 상태 변경이 완료되면 이를 Controller 와 View 에게 알린다. 그런데 Model 의 한가지 큰 특징은 Controller 와 View 에 대해 알지 못한다는 것이다. 이게 가능한 이유는 위에서 언급한 것처럼 Observer 패턴을 사용하기 때문인데 조금 설명을 하자면, Controller 와 View 는 Observer 라는 인터페이스를 구현하고 이 타입을 통해 Model 을 subscribe 하므로 Model 입장에서 이들은 단순히 모두 똑같은 Observer 일 뿐이지 각 Observer 가 실제로 어떤 객체인지는 알 필요가 없다. 클래스 다이어그램을 그려보면 다음과 같다 이를 통해 Controller 와 View 에 대한 Model 의 결합(Coupling)을 느슨하게 하고, 이들의 재사용성을 높이게 된다. 참고로, 전혀 다른 비즈니스 로직을 가지는 Model 에 Adapter 패턴을 활용하여 기존의 View 와 Controller 를 그대로 재사용할 수도 있다. 뷰(View) View 는 사용자와의 상호작용을 담당하며, 크게 2가지 역할을 수행한다. 첫째는 사용자에게 화면을 보여주는 것이다. GUI 환경이라면 버튼이나 체크박스 등이 될 수 있고, CLI 환경이라면 텍스트가 될 것이다. Model 로 부터 자신의 상태가 변경 되었다는 알림을 받을 때마다 Model 에게 데이터를 받아, 이를 기반으로 화면을 업데이트한다. Model 의 데이터를 기반으로 변경하는 것 뿐만 아니라, 단순히 Controller 요청에 따라 화면을 변경하기도 한다. 둘째는 사용자의 입력 이벤트를 받는 것이다. 정확히는 사용자의 이벤트를 받아 Controller 에게 전달한다. 컨트롤러(Controller) Controller 는 View 와 Model 사이의 중재자이다. Controller 는 View 로 부터 사용자의 입력 이벤트를 받으면 다시 View 에게 화면 업데이트를 요청할 수도 있고 Model 에게 상태 변경을 요청할 수도 있다. 위에서 말한 것처럼 Model 은 상태 변경이 완료되면 Observer 패턴을 통해 Controller 와 View 에게 이를 알린다. JSP Model 2 란? JSP Model 2[5] 란 MVC 패턴을 웹 애플리케이션에 맞는 형태로 적용시킨 것이다. 즉, Spring 과 같은 Web Framework 에서 사용하는 MVC 패턴은 JSP 를 사용하지 않는다고 하더라도 사실상 전통적인 MVC 패턴 보다는 이 JSP Model 2 에 해당한다고 할 수 있다. (좀 더 구체적으로 말하자면, Spring MVC 에서 사용하는 패턴은 Servlet(DispatcherServlet)에서 HTTP 요청을 처리하는 것을 제외한 Controller 로직을 분리한 구조로, Front Controller 패턴[6] 이라고 부른다.) 사실 JSP Model 1[7] 도 있는데 Model 1 에서는 View 와 Controller 의 역할이 분리되지 않고 하나의 컴포넌트에서 담당하는 모양을 하고있다. Model 2 의 등장으로 웹 애플리케이션 개발 단위는 View 에 해당하는 JSP pages 와 Controller 에 해당하는 Servlet 이 완벽하게 분리되었기 때문에 HTML 과 약간의 JSP 에 대한 지식을 가진 웹 퍼블리셔와 전문적인 소프트웨어 지식을 가진 개발자의 역할을 분리하여 생산성을 높이는데 기여했다. 그럼 Model 2 에서는 MVC 의 각각의 컴포넌트가 전통적인 MVC 와 어떻게 다른지 알아보자 Java Bean (Model) Model 2 에서의 Model 이 전통적인 MVC 에서의 Model 과 가장 큰 차이점은 Model 2 에서는 View 로 부터 사용자의 입력 이벤트가 들어올 때, 네트워크(HTTP)를 통해 들어온다는 것이다. 그렇기 때문에 Model 의 상태 변경이 완료되었을 때 View 에게 이를 알리지 않는다. 단지 Controller 의 요청이 있을 때만 Model 이 Java Bean 으로서 Controller 를 통해 View 에게 전달될 뿐이다. JSP (View) Controller 를 통해 Model 로 부터 전달받은 Java Bean 내 데이터를 기반으로 화면을 구성한다. Servlet (Controller) HTTP 요청에 따라 Model 에 상태 변경을 요청하고 상태가 변경된 Model 을 View 에 전달한다. HTTPServlet 클래스를 상속받아 doGet(), doPost(), doPut(), doDelete() 등의 메소드를 Override 하여 HTTP 요청을 받으며, 메소드 인자로 전달된 HttpServletRequest 객체를 사용하여 요청과 함께 전달된 데이터를 읽어들인다. 참고 https://github.com/sgc109/design-pattern-study Head First Design Patterns JavaServer Pages Spec https://github.com/sgc109/design-pattern-study/tree/master/02-Observer-Pattern ↩︎ https://github.com/sgc109/design-pattern-study/tree/master/09-Iterator-and-Composite-Patterns ↩︎ https://github.com/sgc109/design-pattern-study/tree/master/01-Strategy-Pattern ↩︎ https://github.com/sgc109/design-pattern-study/tree/master/07-Adapter-and-Facade-Patterns ↩︎ https://en.wikipedia.org/wiki/JSP_model_2_architecture ↩︎ https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-servlet ↩︎ https://en.wikipedia.org/wiki/JSP_model_1_architecture ↩︎","link":"/2020/07/18/compound-pattern-feat-mvc/"},{"title":"Kubernetes 의 Downward API","text":"쿠버네티스의 파드 내에서 파드의 매니페스트나 속성에 대한 정보를 얻기위한 방법인 Downward API 에 대해 알아보자. Downward API 란? 애플리케이션이 실행되기 전에 이미 알고있는 속성이나 설정 값들은 ConfigMap 이나 Secret 으로 파드에 전달할 수 있지만, 파드의 이름, 파드의 IP, 파드가 실행되는 노드의 이름 등 실제로 파드가 생성 및 실행이 되기전에는 알 수 없는 속성들도 존재한다. 물론 파드의 레이블이나 어노테이션과 같은 일부 속성들은 파드 생성 이전에도 알 수 있지만, 파드 내에서 정보를 사용하고 싶다는 이유로 이미 설정되어 있는 속성을 ConfigMap 등을 통해 중복하여 정의하고 싶지는 않을 것이다. 이런 속성들을 컨테이너에서 실행 중인 애플리케이션에서 알아내려면 어떻게 해야할까? 이 때 사용되는 것이 Downward API 이다. Downward API 는 단순히 환경변수, 또는 파일(downwardAPI 볼륨을 통해) 로 위와 같은 속성들을 컨테이너에서 손쉽게 사용할 수 있도록 하는 기능일 뿐이다. Downward API 를 통해 전달할 수 있는 정보는 다음과 같다. 파드의 이름 파드의 IP 주소 파드가 속한 네임스페이스 파드가 실행중인 노드의 이름 파드가 실행 중인 서비스 어카운트 이름 각 컨테이너의 CPU와 메모리 request 각 컨테이너의 CPU와 메모리 limit 파드의 label 파드의 annotation 참고로, 네임스페이스 정보를 얻기 위해서는 굳이 Downward API 를 사용할 필요도 없다. k8s 에서는 파드가 API server 와 통신할 수 있도록 하기 위해 각 파드마다 기본적으로 Default token 시크릿 볼륨을 만들어 파드 내 컨테이너의 /var/run/secrets/kubernetes.io/serviceaccount/에 마운트해 주는데, 이 곳에 namespace 라는 파일에 네임스페이스가 적혀있기 때문이다. 환경 변수로 전달하기 vs 볼륨으로 전달하기 Downward API 를 통해 데이터를 전달하기 위한 방법으로는 환경 변수를 통한 방법과 볼륨을 통한 방법, 이렇게 크게 두가지가 있다. 대부분의 경우 환경변수를 통한 방법과 볼륨을 통한 방법 중 어떤 방법을 사용해도 크게 문제가 없지만 약간의 차이점이 있다. 우선 일부 정보들은 둘 중 한가지 방법으로만 얻을 수 있다. 예를 들어 Pod 의 label 과 annotation 은 downwardAPI 볼륨을 통해서만 전달할 수 있다. 그 이유는, Pod 의 label 과 annotation 은 Pod 가 실행되는 동안 수정될 수가 있는데, 이 때 Pod 가 변경된 데이터를 볼 수 있도록 해야 한다. 하지만 환경변수는 컨테이너가 생성된 이후에 외부에서 변경할 수 있는 방법이 없기 때문이다. 반면, 파드가 실행중인 노드의 이름과 IP 는 환경 변수를 통한 방법으로만 얻을 수 있다. 각 방법을 통해 얻을 수 있는 정보들의 전체 목록은 공식 문서를 통해 확인할 수 있다. 다음으로, 각 컨테이너가 가지는 속성인 컨테이너의 리소스 request 와 limit 의 경우 환경변수를 통한 방식으로는 다른 컨테이너의 리소스 정보를 사용할 수가 없지만, 볼륨을 통한 방식으로는 다른 컨테이너의 리소스 정보도 사용할 수가 있다. 왜냐하면 애초에 컨테이너 마다 정의해야 하는 환경변수(env 속성)와는 달리 볼륨은 파드 단위로 정의하기 때문이다(spec.volumes 속성을 통해) 그렇기 때문에 볼륨에서 item 을 정의할 때는 container 를 명시하게 되는데 이 때문에 다른 컨테이너의 리소스 정보도 사용할 수 있는 것이다. 환경 변수로 전달하기 다음 매니페스트 파일로 파드를 생성해보자. apiVersion: v1kind: Podmetadata: name: downward-envspec: containers: - name: main image: busybox command: [\"sleep\", \"99999\"] resources: requests: cpu: 15m memory: 100Ki limits: cpu: 100m memory: 20Mi env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: CONTAINER_CPU_REQUEST_MILLICORES valueFrom: resourceFieldRef: resource: requests.cpu divisor: 1m - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Ki 위와 같이 환경변수로 파드의 매니페스트 파일에 정의한 값들이 설정되어있는 것을 확인할 수 있다. 볼륨으로 전달하기 다음 매니페스트 파일로 파드를 생성해보자. apiVersion: v1kind: Podmetadata: name: downward-volume labels: foo: bar annotations: key1: value1 key2: | multi line valuespec: containers: - name: main image: busybox command: [\"sleep\", \"9999999\"] resources: requests: cpu: 15m memory: 100Ki limits: cpu: 100m memory: 40Mi volumeMounts: - name: downward mountPath: /etc/downward volumes: - name: downward downwardAPI: items: - path: \"podName\" fieldRef: fieldPath: metadata.name - path: \"podNamespace\" fieldRef: fieldPath: metadata.namespace - path: \"labels\" fieldRef: fieldPath: metadata.labels - path: \"annotations\" fieldRef: fieldPath: metadata.annotations - path: \"containerCpuRequestMilliCores\" resourceFieldRef: containerName: main resource: requests.cpu divisor: 1m - path: \"containerMemoryLimitBytes\" resourceFieldRef: containerName: main resource: limits.memory divisor: 1 위와 같이 /etc/downward에 볼륨이 잘 마운트되어 매니페스트 파일에서 정의한 필드들이 파일로 세팅되어있는 것을 확인할 수 있다. Downward API 의 한계 Downward API 는 특정 파드에서 자신의 메타데이터 및 속성 정보를 얻을 수 있게 해주는 기능이다. 그런데 만약 다른 파드, 혹은 파드가 아닌 다른 리소스(예를 들면 job)의 정보가 필요한 경우에는 Downward API 가 도움이 되지 않는다. 이런 정보는 서비스 관련 환경변수나 DNS 로 얻거나, API Server 와 직접 통신해야 한다. API 서버와 통신하기 API 서버와 통신하기 위해서는 우선 API 서버의 IP 와 포트를 알아야 한다. 쿠버네티스에서는 default 네임스페이스의 경우 kubernetes 라는 이름의 서비스가 자동으로 노출되고 API 서버를 가리키도록 되어있다. 고로, kubectl get svc kubernetes 명령어를 실행하면 API 서버의 IP 와 포트를 알 수 있다. 또한, 파드에는 각 서비스에 대한 정보가 환경변수로 세팅되어 있으므로, 파드 내에서 env | grep KUBERNETES_SERVICE 명령어를 실행해도 kubernetes 서비스, 즉 API Server 의 IP 와 PORT 를 알 수 있다. 이제 IP 와 포트를 알았으니, 앞서 잠깐 언급했던 default token 시크릿 볼륨에 있는 token 과 ca.crt 를 사용하여 API 서버에 정보를 요청할 수 있다. 다음 명령어는 API 서버에 현재 네임스페이스 내에 있는 모든 파드의 목록을 요청하는 명령어다. $ TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)$ CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt$ NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)$ curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes/api/v1/namespaces/$NS/pods 아마 대부분의 경우 위의 명령어를 수행했을 때 액세스 권한이 없다는 응답이 올 것이다. 이는 RBAC(Role-Based Access Control) 라는 것이 활성화되어 있기 때문이다. 테스트 목적이라면 다음의 명령어를 수행하여 cluster-admin ClusterRole 를 통해 모든 파드에 API 서버에 대한 모든 권한을 부여할 수 있다. RBAC(Role-Based Access Control) 와 ServiceAccount, Role, ClusterRole, ClusterRoleBinding 에 대해서는 추후 다른 포스트에서 다룰 예정이다. $ kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --group=system:serviceaccounts GET 요청 대신 PUT 이나 PATCH 를 통해 리소스를 업데이트 하는 등 CRUD 작업을 모두 수행할 수 있다. 앰배서더 컨테이너를 두어, 애플리케이션에서는 아무런 헤더 없이 단순히 http 요청을 하면 인증서와 토큰 세팅과같은 번거로운 과정을 앰배서더 컨테이너가 대신하도록 할 수도 있다. 참고 쿠버네티스 공식 문서(https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#capabilities-of-the-downward-api) 쿠버네티스 인 액션(http://www.yes24.com/Product/Goods/89607047)","link":"/2021/01/17/k8s-downward-api/"},{"title":"히스토그램에서 가장 큰 직사각형","text":"히스토그램에서 가장 큰 직사각형(Largest Rectangle in Histogram)이라는 문제를 푸는 다양한 방법에 대해 알아보자. 이 문제는 임의의 높이를 가진 $N$개의 막대가 주어질 때, 막대 안에 포함되는 직사각형 중 가장 넓이가 큰 직사각형의 넓이를 구하는 유명한 문제인데, 푸는 방법이 다양하다는 점에서 재미있다. 유명한 문제이기 때문에 다양한 온라인 저지 사이트에서 이 문제가 올라와 있는 것을 볼 수 있다. 문제마다 입력으로 주어지는 값들의 조건은 조금씩 다르기도 하다. 여기서는 LeetCode 84. Largest Rectangle in Histogram 를 기준으로 설명하겠다. 본 글에서는 이 문제를 해결하는 5가지 풀이에 대해 알아볼 것이다. 풀이 브루트포스 시간복잡도 : $O(N^3)$ 우선 알고리즘의 효율성에 대해 생각하지 말고 가장 무식한 방법을 떠올려보자. 만들 수 있는 모든 직사각형을 만들어 직사각형의 넓이를 구하면 답을 계산할 수 있을 것이다. 모든 직사각형은 자신의 가장 왼쪽에 위치한 막대와 가장 오른쪽에 위치한 막대가 있을 것이다. 그 직사각형의 높이는 위치상 직사각형에 포함되는 막대들의 높이 중 가장 낮은 막대의 높이가 될 것이다. 그렇기 때문에 모든 막대 쌍 l, r 에 대해 l 부터 r 까지의 막대 중 가장 높이가 낮은 막대의 높이를 계산한 뒤 직사각형의 넓이를 계산해 답을 갱신하면 문제를 해결할 수 있다. 모든 막대 쌍을 순회하는 데 $O(N^2)$ 의 시간이 소요되고, 각 막대 쌍에 대해 최대 높이를 구하는데 $O(N)$ 의 시간이 소요되므로, 총 $O(N^3)$ 의 시간이 소요된다. class Solution { const int MAX_H = 10000;public: int largestRectangleArea(vector&lt;int&gt;&amp; heights) { int N = heights.size(); int ans = 0; for(int i = 0; i &lt; N; i++) { for(int j = i; j &lt; N; j++) { int minH = MAX_H + 1; for(int k = i; k &lt;= j; k++) { minH = min(minH, heights[k]); } ans = max(ans, (j - i + 1) * minH); } } return ans; }}; 최적화된 브루트포스 시간복잡도 : $O(N^2)$ 앞서 설명한 알고리즘을 다시 한 번 보자. 첫번째 반복문에서 왼쪽 막대를 정하고, 두번째 반복문에서 오른쪽 막대를 정하게 된다. 그런데 직사각형의 높이를 찾기위한 세번째 반복문을 잘 보면, 오른쪽 막대를 오른쪽으로 한 칸씩 이동할 때마다 매번 맨 왼쪽 막대부터 처음부터 확인하는 것을 알 수 있다. 굳이 이미 봤던 막대를 다시 볼 필요가 없기 때문에 두번째와 세번째 반복문을 하나의 반복문으로 합칠 수가 있다. 그리하여 시간복잡도는 $O(N^2)$ 이 된다. class Solution {public: int largestRectangleArea(vector&lt;int&gt;&amp; heights) { int N = heights.size(); int ans = -1; for(int i = 0; i &lt; N; i++) { int minH = heights[i]; for(int j = i; j &lt; N; j++) { minH = min(minH, heights[j]); ans = max(ans, (j - i + 1) * minH); } } return ans; }}; 세그먼트 트리 시간복잡도 : $O(NlogH)$ 이번에는 문제를 다르게 한 번 정의해 보자. 앞에서는 양쪽 막대의 가능한 쌍을 모두 확인했기 때문에 그것만으로도 $O(N^2)$ 의 시간복잡도가 소요되었다. 하지만 잘 생각해보면 히스토그램 안에 있는 직사각형들은 높이를 가지고, 직사각형 안에는 항상 직사각형과 같은 높이를 가지는 막대가 있을 것이다. 그렇기 때문에 이번에는 높이가 되는 막대를 정하고 그 높이를 가지는 직사각형 중 가장 큰 직사각형을 찾아보자. 즉, 정해진 높이를 가질 수 있는 가장 왼쪽 막대와 가장 오른쪽 막대를 찾아보자. 문제를 단순화하기 위해 왼쪽과 오른쪽 중에 일단 왼쪽부터 보자. 즉, 각 막대에 대해 해당 막대의 높이로 직사각형을 만들 수 있는 가장 왼쪽 막대의 인덱스를 찾아보자. 살짝 다르게 말해서, (1) 나보다 왼쪽에 있는 막대 중, (2) 나보다 낮은 막대 중에 (3) 가장 오른쪽에 있는 막대를 찾아보자. 그럼 그 막대의 오른쪽 막대가 현재 보는 기둥의 높이로 만들 수 있는 직사각형의 가장 왼쪽 막대가 된다. 그럼 maxIdx 라는 배열을 만들어 maxIdx[i] 를 i의 높이를 가지는 막대들의 index 중 최대값이라고 정의해보자. 막대는 왼쪽부터 오른쪽으로 봐 나갈 것이고, 막대를 볼 때마다 해당 막대의 높이 정보로 maxIdx 를 업데이트한다고 하면, 결국 1번 조건은 자동으로 만족되고, 2번과 3번 조건은 RMQ(Range Maximum Query) 를 통해 로그 시간으로 계산할 수 있다. 쿼리를 수행하는 범위(range)는 [0, 직사각형 높이 - 1] 이 된다. 여기서 주의해야할 점은, 문제에서 주어지는 막대의 최대 높이가 너무 커서 세그먼트 트리를 구축할 수 없는 경우다. 하지만, 막대의 개수는 충분히 작을 것이기 때문에(대부분의 경우는 $10^5$ 이하) 좌표 압축을 수행하면 된다. 여기서는 막대의 최대 높이가 $10^4$ 라고 가정하겠다. 아무튼 이렇게 하여 막대의 최대 높이가 $H$ 일 때, $O(NlogH)$ 의 시간복잡도로 각 막대를 높이로 삼는 직사각형의 왼쪽 끝 막대를 찾을 수 있었다. 이번엔 오른쪽 끝 막대를 찾아볼 텐데, 사실 잘 생각해보면 막대의 순서를 뒤집어서 같은 일을 하면 된다. 여기서 오른쪽 끝 막대의 경우 계산하고 난 뒤에 배열과 인덱스들을 다시 뒤집어 주는 것을 잊으면 안된다. 결론적으로, 세그먼트 트리를 구축하는데 $O(H)$ 의 시간이 걸리고 각 높이 별 왼쪽 끝 막대와 오른쪽 끝 막대의 인덱스를 찾는데 각각 $O(NlogH)$ 의 시간 복잡도가 걸려서, 전체 시간 복잡도는 $O(H + NlogH)$ 다. 물론, 여기서 $H$ 가 커진다면 $O(NlogN)$ 으로 좌표압축을 해야하지만, 지금은 $H &lt; N$ 이기 때문에, 무시해도 될 것이다. 즉, 최종 시간복잡도는 $O(NlogH)$. class Solution { int MAX_H = 10000; int maxRange = 1; vector&lt;int&gt; tree; vector&lt;int&gt; lBorders; vector&lt;int&gt; rBorders;public: Solution() { while(maxRange &lt; MAX_H) maxRange *= 2; tree = vector&lt;int&gt;(maxRange * 2, -1); } void update(int pos, int val) { pos += maxRange; tree[pos] = max(tree[pos], val); for(; pos &gt; 1; pos /= 2) { tree[pos / 2] = max(tree[pos], tree[pos ^ 1]); } } int query(int l, int r) { l += maxRange, r += maxRange; int ret = -1; for(; l &lt;= r; l /= 2, r /= 2) { if(l &amp; 1) ret = max(ret, tree[l++]); if(~r &amp; 1) ret = max(ret, tree[r--]); } return ret; } void solve(vector&lt;int&gt;&amp; heights, vector&lt;int&gt;&amp; borders) { for(int i = 0; i &lt; heights.size(); i++) { borders[i] = query(0, heights[i] - 1); update(heights[i], i); } } int largestRectangleArea(vector&lt;int&gt;&amp; heights) { int N = heights.size(); lBorders = vector&lt;int&gt;(N, 0); rBorders = vector&lt;int&gt;(N, 0); int ans = -1; solve(heights, lBorders); vector&lt;int&gt; reversed = heights; reverse(begin(reversed), end(reversed)); tree = vector&lt;int&gt;(2 * maxRange, -1); solve(reversed, rBorders); reverse(begin(rBorders), end(rBorders)); for(int i = 0; i &lt; N; i++) rBorders[i] = N - rBorders[i] - 1; for(int i = 0; i &lt; N; i++) { ans = max(ans, (rBorders[i] - lBorders[i] - 1) * heights[i]); } return ans; }}; 분할정복 + 그리디 시간복잡도 : $O(NlogN)$ 재귀적으로 문제를 해결해보자. 일단 $N$ 개의 막대로 이루어진 임의의 히스토그램이 주어졌을 때 막대들을 절반으로 나누어보자. 히스토그램 내 직사각형은 3가지 경우 중 하나일 것이다. (1) 왼쪽 절반에 있거나, (2) 오른쪽 절반에 있거나, (3) 사이에 걸쳐있거나. 그럼 (1)번과 (2)번의 경우는 결국 일부 막대에 대해 완전히 동일한 문제를 푸는 것이 되기 때문에 재귀 호출로 간단히 해결할 수 있다. 그럼 (3)번의 경우만 계산해주면 문제를 해결할 수 있다. 풀이2 에서 썼던 무식한 방법으로도 계산할 수 있지만, 이러면 애초에 풀이2 로 푸는것보다도 비효율적이다. 여기서는 잘 생각해보면 다음과같이 그리디하게 $O(N)$ 으로 계산할 수가 있다. 왼쪽 끝 막대와 오른쪽 끝 막대를 2개의 포인터로 가리키게 해보자. 이 두 포인터를 가장 가운데 있는 두 막대를 가리키도록 초기화 하고 둘을 바깥쪽으로 하나씩 이동시키면서 답을 계산할 것이다. 매 단계마다 두 포인터 중에서 다음 막대의 높이가 더 높은 포인터를 이동시킨다. 그리고 새로 가리킨 막대의 높이로 직사각형의 높이를 갱신한다. 이렇게 했을 때 답을 구할 수 있다는 것을 증명해보자. 위의 알고리즘의 경우 매 반복마다 두 포인터 중 하나를 한 칸 이동시키기 때문에 직사각형의 너비는 항상 1만큼 증가한다. 중요한 것은 높이의 변화인데, 다음 4가지 상황으로 나눌 수 있다. (1) 만약 왼쪽 포인터와 오른쪽 포인터 모두에 대해 다음 막대가 현재 직사각형의 높이보다 높거나 같은 높이인 경우 어느 포인터를 이동시키든 직사각형의 높이는 그대로 유지되기 때문에 어느쪽을 이동시키든 상관없다. (2) 왼쪽 포인터와 오른쪽 포인터 모두, 다음 막대가 현재 직사각형의 높이보다 낮은 경우 이 경우에는 높이가 최소한으로 줄어들 수 있는 선택을 해야한다. 직사각형의 높이는 직사각형을 이루는 모든 막대들의 높이 중 최소값이므로 이 경우 직사각형의 높이는 새로 가리키게 되는 막대의 높이로 변한다. 그렇기 때문에 다음 막대의 높이가 더 큰 포인터를 이동시킨다. (3) 한쪽은 직사각형의 높이보다 높거나 같고, 한쪽은 낮은 경우 높거나 같은 쪽으로 움직여야 직사각형의 높이가 줄어들지 않고 그대로 유지되므로 높이가 높은 쪽 포인터를 움직인다. 위의 방법은 직사각형의 높이가 가능한한 천천히 감소하도록 너비를 1씩 증가시켰기 때문에, 가능한 모든 너비에서 가장 높은 높이로 답을 계산하게 된다. 다르게 생각해보면, 직사각형의 높이는 절대 커지지 않는다. 만약 변한다면 작아지기만 한다. 그렇기 때문에 위의 알고리즘은 높이가 유지되는 선에서 최대한 양쪽 포인터를 바깥쪽으로 늘리게 되고, 작아져야만 하는 경우에는 최소한으로만 감소시키기 때문에 결국엔 모든 막대들을 높이로 내림차순 정렬한 배열에서, 초기 높이에서부터 하나씩 작아지는 방향으로 직사각형의 높이를 변화시키게 된다. 이 때 재귀함수 호출의 깊이는 최대 $logN$ 번이고, 각 함수에서는 전체 막대기를 한 번씩 보게 된다. 그렇기 때문에 각 재귀 단계마다 전체 막대를 한 번씩 확인하는 셈이며, 그래서 전체 시간 복잡도는 $O(NlogN)$ 이다. class Solution { const int MAX_H = 10000; vector&lt;int&gt; heights; int solve(int left, int right) { if(left == right) return heights[left]; int mid = left + (right - left) / 2; int ans = max(solve(left, mid), solve(mid + 1, right)); int lpos = mid, rpos = mid + 1; int minH = MAX_H + 1; while(left &lt;= lpos &amp;&amp; rpos &lt;= right) { minH = min({minH, heights[lpos], heights[rpos]}); ans = max(ans, (rpos - lpos + 1) * minH); if(left == lpos) ++rpos; else if(right == rpos) --lpos; else if(heights[lpos - 1] &lt;= heights[rpos + 1]) ++rpos; else --lpos; } return ans; }public: int largestRectangleArea(vector&lt;int&gt;&amp; heights) { this-&gt;heights = heights; return solve(0, heights.size() - 1); }}; 스택 시간복잡도 : $O(N)$ 이번에도 마찬가지로 각 막대의 높이를 기둥으로 하는 직사각형의 가장 왼쪽 막대와 가장 오른쪽 막대를 찾아볼 것이다. 여기서는 스택을 사용하여 왼쪽 막대부터 보면서 하나씩 스택에 넣어줄 건데, 이 막대를 높이로하는 직사각형의 넓이의 계산은 나중으로 미루게 된다. 언제 계산하냐면, 이 막대보다 작은 높이의 막대를 발견하는 즉시 계산해준다. 왜냐하면 발견한 막대가 바로 앞서 스택에 넣었던 막대보다 오른쪽에 있으면서 높이가 작은 가장 왼쪽 막대이기 때문이다. 직사각형 넓이의 계산과 동시에 해당 막대는 스택에서 빼준다. 해당 막대를 높이로 하는 직사각형을 이미 계산했기 때문이다. 이런 방식으로 스택을 관리해주면 자연스레 스택에 있는 막대들은 높이가 오름차순으로 정렬된 상태가 되며, 스택 내에서 특정 막대의 바로 이전 막대는 그 막대보다 왼쪽에 있으면서 높이가 작은 가장 오른쪽 막대가 된다. 즉, 직사각형의 넓이를 계산해야 될 때, 왼쪽 막대(스택에서 최상위 막대 바로 이전에 저장된 막대), 높이(스택의 최상위 막대), 오른쪽 막대(현재 보는 막대)를 모두 상수 시간에 알 수 있다는 뜻이다. 즉, 모든 막대를 한 번 씩 보면서 각 막대가 딱 한 번 씩 스택에 들어갔다 나오며, 중간 중간 직사각형의 넓이를 상수시간에 계산하기 때문에 전체 시간 복잡도는 $O(N)$ 이 된다. class Solution {public: int largestRectangleArea(vector&lt;int&gt;&amp; heights) { heights.push_back(-1); stack&lt;int&gt; stk; stk.push(-1); int ans = 0; for(int i = 0; i &lt; heights.size(); i++) { while(stk.size() &gt; 1 &amp;&amp; heights[i] &lt; heights[stk.top()]) { int mid = stk.top(); stk.pop(); int left = left = stk.top(); ans = max(ans, (i - left - 1) * heights[mid]); } stk.push(i); } return ans; }}; 사실 그림으로 설명하면 훨씬 이해가 쉬울텐데 그리기가 귀찮기 때문에 나중에 그림이 그리고 싶어졌을 때 첨부할 예정이다. 문제 링크 리트코드(LeetCode) - https://leetcode.com/problems/largest-rectangle-in-histogram/ 백준 온라인저지(BOJ) - https://www.acmicpc.net/problem/6549","link":"/2021/03/18/largest-rectangle-in-histogram/"},{"title":"DDD 의 Aggregate","text":"본 글에서는 도메인 주도 설계(Domain Driven Design) 에서 굉장히 중요한 개념인 애그리거트(Aggregate)에 대해 알아본다. 간략한 설명은 DDD 시작하기 에서 다룬다. 본 포스팅에 없는 내용도 있으니 함께 읽는것을 추천한다. Aggregate 란? 애그리거트(Aggregate)는 한마디로 서로 관련이 있는 도메인 모델들의 집합이다. 많은 수의 도메인 모델 간의 복잡한 관계를 파악하기란 쉬운 일이 아니다. 그렇기 때문에 서로 관련이 있는 도메인 모델들 끼리 묶어 각 도메인 모델의 상세 구현보다는 더 큰 그림으로 도메인 모델간의 관계를 파악하는것이 좋다. 자세한 사항이 궁금하면 그 때 애그리거트 내부를 살펴보면 된다. 대부분의 경우 하나의 애그리거트는 하나의 엔티티와 여러개의 밸류로 구성된다. 드물게 하나의 애그리거트에 두개의 엔티티가 존재하기도 한다. 각 애그리거트에는 애그리거트 루트라는 도메인 엔티티가 하나씩 있다. 애그리거트 루트는 애그리거트 내에 속한 객체의 변경을 책임지며, 도메인 규칙에 따라 언제나 애그리거트 내 모든 도메인 모델들의 일관성을 유지할 책임이 있다. 애그리거트는 DB 에 도메인을 저장하거나 읽어들이는 단위이며, 애그리거트를 읽을 때는 애그리거트 루트의 id 를 이용한다. 일반적으로 하나의 애그리거트에는 하나의 도메인 엔티티(애그리거트 루트)가 존재하며, 0개 이상의 밸류 타입이 존재한다. 드물게 한 애그리거트에 2개 이상의 엔티티가 존재하지만, 사실은 애그리거트 루트를 제외한 도메인 모델이 엔티티가 아니라 밸류 타입이거나 다른 애그리거트에 속해야 하는 경우가 많으므로 잘 확인해야한다. 서로 다른 도메인 모델이 변경의 주체와 생성 및 변경의 시점이 같다면 같은 애그리거트에 속할 가능성이 높다. Aggregate 와 트랜잭션 하나의 트랜잭션에서 둘 이상의 애그리거트를 수정하는 것은 성능상 좋지 않아 피하는 것이 좋다. 그러기 위해서는 하나의 애그리거트에서 다른 애그리거트를 변경하지 말아야 한다는 이야기가 된다. (물론, 그러지 말아야 하는 또다른 이유는 그러한 행위가 애그리거트간의 결합도를 높이기 때문이기도 하다) 만약, 하나의 트랜잭션에서 둘 이상의 애그리거트를 변경해야 한다면 보통은 이벤트나 비동기를 사용하여 이를 피할 수 있으며, 이벤트나 비동기를 사용할 수 없는 경우에는 다른 애그리거트의 변경을 응용 서비스에서 수행하여 적어도 한 애그리거트에서 다른 애그리거트를 변경하는 것만은 피해야한다. Aggregate 의 참조 또하나 주의 해야 할 점은, 하나의 애그리거트 내에서 필드를 통해 다른 애그리거트를 직접 참조하면 다음과 같은 문제가 발생한다는 것이다. 객체 그래프 탐색이 쉬움 즉, 다른 애그리거트를 수정하기가 쉬움 성능에 대한 고민이 필요 지연 로딩/즉시 로딩 확장의 어려움 애그리거트1은 MySQL, 애그리거트2는 MongoDB, 애그리거트3은 Redis 에 저장 필드 대신 id 를 통해 애그리거트를 간접적으로 참조한다면 위와같은 문제가 모두 해결된다. 특정 애그리거트가 필요할 때마다 id 를 사용하여 Repository 를 통해 애그리거트를 가져오는것이다. 대신 이렇게 하면 N+1 과같은 문제를 해결하기 위해 JPQL 등으로 한 번에 데이터를 불러오는 조인 쿼리를 별도로 만들어줄 필요는 있다. Aggregate 간 연관 관계 JPA 를 처음 접하는 경우 흔히 하는 실수가 객체의 모든 연관 관계를 지연 로딩과 즉시 로딩으로 어떻게든 처리하고 싶은 욕구에 사로잡히는 것이다. 하지만 위에서 설명한 바와 같이, 애그리거트는 Id 를 통해 간접적으로 참조하는 것이 좋으며, 1:N 이나 M:N 연관 관계를 가지는 객체라고 하더라도 이를 그대로 실제로 구현에 반영하는 경우는 드물다. 예를 들어, User 와 User 가 작성한 Post 간의 관계는 1:N 관계이지만, 보통은 이 User 가 작성한 모든 Post 를 한 번에 불러오는 것이 아니라 페이징을 이용해 조금씩 불러오게 된다. 또한, Category 와 Product 간의 관계는 M:N 관계이지만, 하나의 Product 가 속한 Category 는 한 번에 보여줘도, 한 Category 에 속하는 모든 Product 를 한 번에 보여주지는 않기 때문에 단방향 M:N 연관만 적용하게 된다. Aggregate 와 팩토리 하나의 애그리거트가 다른 애그리거트를 생성하는 팩토리의 역할을 할 수도 있다. 예를 들어, 다른 유저의 신고로 인해 특정 유저가 일정 기간 댓글을 작성할 수 없다고 가정하자. 그럼 특정 유저 id 를 가진 경우 댓글 엔티티를 생성하는 것을 막아야할 것이다. 이 때, User 의 상태에 따라 Comment 생성 가능 여부를 판단해야하며, 이와 동시에 Comment 애그리거트를 생성할 때는 어차피 User 의 id 가 필요하기 때문에 User 클래스에 팩토리 메서드를 추가하여 User 애그리거트를 Comment 애그리거트의 팩토리로 사용하면 응용 서비스로 도메인 로직이 분산되는 것을 막고, 도메인의 응집도를 높일 수 있다.","link":"/2020/08/09/ddd-aggregate/"},{"title":"JPA 이해하기 (feat. ORM)","text":"JPA(Java Persistence API) 은 자바의 표준 ORM API 이다. 그렇다면 ORM 이란 무엇일까? JPA 를 제대로 이해하기 위해 우선 ORM 에 대한 이해가 필요하다. ORM 이란? ORM 이란 Object-Relational Mapping 의 약자로, 이름 그대로 객체(Object)와 관계형 데이터(Relational data) 를 매핑하기 위한 기술이다. 이러한 매핑이 필요한 이유는 객체 지향 언어(Object Oriented Language)과 관계형 데이터베이스(Relational Database)에서 데이터를 표현 하는 방식이 다르기 때문이다. 이 둘 간의 차이 때문에 개발자는 더 많은 코드를 작성해야 하며, 이는 반복적이고 실수하기 쉬운 작업이 된다. 그렇기 때문에 개발자는 Object Oriented 한 Design 에 집중할 수 없게 된다. ORM 은 이러한 문제를 해결해 준다. 패러다임의 불일치 객체 지향 프로그래밍(이하 OOP)과 관계형 데이터베이스(이하 RDB)의 데이터 표현 방식이 다른 문제를 패러다임의 불일치라고 부르기도 한다. OOP 와 RDB 에서 데이터를 표현하는 방식이 다른 이유는 애초에 이들의 목표와 동작 방식이 다르기 때문이다. 예를 들어, RDB 에서는 데이터의 중복을 줄이고 및 일관성을 높이기 위해 하나의 데이터를 여러개의 테이블로 쪼개어 저장하고 필요할 때 조인하여 사용하게 된다. 반면, OOP 에서는 하나의 객체가 다른 객체에 대한 참조를 포함하며, 연관된 두 객체는 모두 메모리 상에 존재하기 때문에 하나의 객체로 이와 연관된 객체들의 데이터를 손쉽게 얻을 수 있다. 그럼 4가지 관점에서 OOP 와 RDB 가 데이터를 다루는 방식이 어떻게 다른지, 또 JPA 는 이를 어떻게 해결하는지 좀 더 살펴보자. 상속 객체는 상속이라는 개념이 있는 반면 테이블은 상속의 개념이 없다. 예를 들어 여러 운송 수단의 리스트를 표현하기 위해 운송 수단에 대한 클래스를 다음과 같이 설계했다고 가정하자. 그렇다면 RDB 에서는 다음과 같이 테이블을 설계해야 할 것이다. 즉, Car 객체를 DB 에 저장하기 위해서는 Vehicle 클래스에 해당하는 데이터와 Car 에서 정의한 데이터로 나누어 2번의 INSERT 쿼리가 필요하며, DB 에서 데이터를 가져올 때는 조인을 통해 가져온 데이터로 Car 객체를 생성해야 한다. 반면, OOP 에서는 단순히 list.add() 와 list.get() 명령어 한 번으로 원하는 데이터를 저장하고 읽을 수 있다. JPA 는 개발자가 OOP 스타일로 Car 객체를 저장하더라도 내부적으로 2개의 쿼리를 만들어 상속 관계를 RDB 에 맞는 데이터로 변환하여 저장해준다. 연관 관계 학교에서 한 명의 학생이 하나의 반에 속해있는 상황을 가정해보자. class Student { int studentId; String name; Class clazz;}class Class { int classId; String className; Teacher teacher;} OOP 에서는 학생에 대한 객체가 반에 대한 객체를 포함한다. 정확히는 참조를 가지고 있다. 반면, RDB 에서는 참조를 가지는 것이 아니라 반에 대한 참조를 FK 로 대체하고, 반에 대한 데이터를 분리하여 따로 저장해야한다. 즉, 데이터 저장 시에 데이터를 분리하여 2개의 쿼리를 사용해야 하며, 다시 읽을 때는 조인을 통한 재조립이 필요하다. JPA 는 이를 내부적으로 처리해주기 때문에 개발자는 OOP 스타일로 데이터를 저장하고 읽을 수 있게 된다. 객체 그래프 탐색 객체 연관관계가 다음과 같은 그래프 형태를 이루고 있다고 하자. 이때 어떤 사람이 거주하는 국가를 얻기 위해 다음과 같은 코드를 작성했다고 하자. person.getHouse().getCity().getCountry(); 이렇게 연관된 객체를 얻기 위한 행위를 객체 그래프 탐색이라고 한다. DB 에서 person 데이터를 가져오려면 앞서 말한것 처럼 연관된 테이블과의 조인을 통해 가져와야 한다. 그리고 이때 정해진 쿼리문에 따라 탐색이 가능한 경계가 정해지게 된다. 만약 이 경계를 넘는 객체를 얻어 사용하려 하면 NPE(Null Pointer Exception)가 발생할 것이다. 문제는, 처음 프로그램을 개발할 때는 person 에서 부터 어디까지 그래프를 탐색해야 하는지 알 수 없다는 것이다. 물론 넉넉하게 모든 연관된 테이블을 조인하여 데이터를 가져올 수도 있을 것이다. 하지만 불필요하게 많은 테이블을 조인하여 모든 데이터를 가져오는 것은 좋은 방법이 아닐 것이다. JPA 는 지연 로딩을 사용하여 이 문제를 해결한다. 그때 그때 연관된 데이터가 필요할 때 데이터를 로딩하는 것이다. 비교 RDB 에서는 데이터를 PK 값으로 식별하기 때문에 같은 id 를 가진 데이터는 당연히 같은 데이터로 취급된다. 반면, OOP 에서 두 객체 간의 동일성 비교는 '==' 연산자를 사용한다. 즉, 두 객체가 같은 참조를 가지고 있는지를 보는것이다. 그런데 다음과 같이 DB 에서 가져온 객체의 동등성을 비교하는 경우를 보자. Student student1 = studentDAO.getStudent(studentId);Student student2 = studentDAO.getStudent(studentId);assert student1 == student2; 일반적으로 DAO 는 매번 새로운 인스턴스를 만들어 반환하도록 구현되기 때문에 두 객체의 동일성을 비교하면 물론 다르다는 결과가 나올 것이다. JPA 에서는 동일한 키 값으로 데이터를 요청하면 같은 인스턴스를 반환하기 때문에 이런 문제가 발생하지 않는다. 그럼 이제 JPA 에 대해 자세히 알아보자. JPA 란? JPA 는 Java Persistence API 의 약자로 자바 진영에서 만든 표준 ORM API 이다. (참고로 2019년에 Jakarta Persistence 로 이름이 바뀌었다) JPA 는 캐싱, 지연 로딩, 쓰기 지연 등을 통한 성능을 향상시켜 주고, 변경 감지, 동일성 보장 등을 통해 개발 편의성을 향상시켜 준다. 다음은 JPA 의 동작 방식에 대해 알아보자. JPA 의 동작 방식 JPA 에서는 저장하고자 하는 데이터를 엔티티라고 부르는데, 우선 JPA 에서 엔티티를 DB 에 저장하는 샘플 코드를 보자. EntityManagerFactory emf = Persistence.createEntityManagerFactory(\"persistence-unit\");EntityManager em = emf.createEntityManager();EntityTransaction tx = em.getTransaction();try { tx.begin(); Student student = new Student(); student.setName(\"홍길동\"); em.persist(student); tx.commit();} catch (Exception e) { e.printStackTrace(); tx.rollback();} finally { em.close();}emf.close(); Persistence 클래스의 createEntityManagerFactory() 메소드로 엔티티 매니저 팩토리(Entity Manager Factory) 를 생성하는데, 이때 DB 의 커넥션 풀도 함께 생성된다. 그렇기 때문에 팩토리를 생성하는 일은 비용이 많이 들며, 보통 하나만 만들어 애플리케이션 전체에서 공유한다. EntityManagerFactory 의 createEntityManager() 메소드로 엔티티 매니저(Entity Manager) 를 생성할 수 있는데, 이 엔티티 매니저는 가상의 DB 와 같은 역할을 한다고 보면된다. 우리는 엔티티와 관련된 일을 할 때 이 엔티티 매니저와 상호작용하게 된다. 엔티티를 생성하는 비용은 거의 없으며 각 엔티티 매니저는 필요할 때 커넥션 풀에서 커넥션을 얻어 사용한다. Entity Manager 를 생성할 때는 영속성 컨텍스트(Persistence Context) 가 같이 생성되는데, 이 영속성 컨텍스트를 이해하는 것이 중요하다. 기본적으로 하나의 엔티티 매니저에는 하나의 영속성 컨텍스트가 할당되지만 서로 다른 엔티티 매니저가 하나의 영속성 컨텍스를 사용할 수도 있다. 엔티티와 관련된 동작을 수행할 땐 트랜잭션 사이에 수행되어야 한다. 그럼 엔티티를 저장, 조회, 수정, 삭제할 때 내부 동작 원리를 알아보자. 엔티티의 저장 엔티티는 생명주기를 갖는다. 엔티티에는 영속(Managed), 준영속(Detached), 비영속(Transient), 삭제(Removed) 이렇게 4가지 상태가 있으며, 엔티티 매니저의 특정 메소드가 호출되거나 JPQL 이 실행되면 상태가 변한다. entityManager.persist(entity) 의 형태로 엔티티를 저장하면 엔티티는 비영속(Transient) 상태에서 영속(Managed) 상태가 되며 영속성 컨텍스트 내에서는 다음과 같은 일이 일어난다. 엔티티 매니저의 persist() 메소드를 통해 엔티티가 영속성 컨텍스트에 들어온다. 엔티티가 영속성 컨텍스트의 1치 캐시에 저장되면서 초기 상태의 스냅샷이 따로 보관된다. 엔티티를 DB 에 저장하기 위한 쿼리가 자동 생성되어 쓰기 지연 SQL 저장소에 저장된다. 트랜잭션을 커밋하면 내부적으로 먼저 flush() 를 수행하는데, 이는 쓰기 지연 SQL 저장소에 저장된 쿼리들을 DB 에 보내 수행하게 함으로써 영속성 컨텍스트와 DB 의 상태의 동기화를 위한 것이다. 트랜잭션을 커밋한다. 엔티티의 조회 엔티티 매니저의 find() 메소드로 엔티티를 DB 에서 읽을 수 있다. 하지만 곧바로 DB 에서 데이터를 찾는 것이 아니라 먼저 1차 캐시를 살펴본다. 1차 캐시에 해당 엔티티가 있으면 곧바로 이를 반환하고, 만약 없다면 DB 에서 데이터를 다져와 1차 캐시에 저장한 뒤 반환한다. 엔티티를 조회하는 다른 방법으로는 JPQL(Java Persistence Query Language) 이 있다. JPQL 은 SQL 과는 달리 자바 객체에 대한 쿼리를 정의한다. 그래서 JPQL 은 SQL 에 대해서는 전혀 모른다. JPQL 을 실행하기 전에 자동으로 flush() 를 수행하고 JPQL 을 실행하는데, 그 이유는 쓰기 지연 SQL 저장소에 있는 쿼리를 DB 와 동기화해야 정상적인 결과가 나올 것이기 때문이다. 엔티티의 수정 저장과 삭제와는 달리 수정을 위한 메소드가 따로 존재하지 않는다. 수정을 위해서는 엔티티 매니저의 메소드를 호출할 필요 없이 영속 상태의 엔티티 객체를 수정하기만 하면된다. 어떻게 엔티티 객체를 수정했을 뿐인데 DB 에 이 사실이 반영되는 것일까? 그것은 바로 영속성 컨텍스트 내부에서 변경 감지(dirty checking) 라는 것을 하기 때문이다. 엔티티 매니저의 flush() 메소드가 실행되면 엔티티가 처음 persist 될 때 저장된 스냅샷과 현재 상태를 비교하여 상태가 달라졌으면 쓰기 지연 SQL 저장소에 업데이트 쿼리를 저장한다. 그렇기 때문에 변경 사항이 DB 에 반영되는 것이다. 만약 엔티티를 영속성 컨텍스트에 의해 관리되지 않는 준영속 상태로 만들고 싶다면 엔티티 매니저의 detach() 메소드 인자로 엔티티를 넘겨주거나, clear() 메소드를 통해 영속성 컨텍스트의 내용을 모두 지우거나, 또는 close() 메소드를 통해 엔티티 매니저를 종료시키면 된다. 엔티티 매니저를 종료시키면 영속성 컨텍스트는 소멸한다. 준영속 상태의 엔티티는 1차 캐시에 존재하지 않으므로 수정해도 DB 에는 이것이 반영되지 않는다. 다시 영속 상태로 만들고 싶다면 merge() 메소드 인자로 엔티티를 넘겨주면 된다. merge() 메소드가 실행되면 엔티티를 1차 캐시에서 찾고, 만약 있다면 메소드 인자로 넘어온 값을 복사 한뒤 1차 캐시에 있는 엔티티를 반환한다. 만약 1차 캐시에 없다면 DB 에서 값을 읽어 1차 캐시에 저장하고 동일한 동작을 수행한다. 근데 여기서 만약 DB 에 해당 데이터가 없는 경우가 있다. 이런 경우 merge() 메소드는 사실상 persist() 메소드와 동일하게 동작한다. 즉, merge() 메소드는 엔티티의 상태가 준영속이건 비영속이건 상관없이 사용할 수 있어, 엔티티의 생성과 수정 모두에 사용될 수 있다. 엔티티의 삭제 엔티티 매니저의 remove() 메소드 인자로 삭제하고자 하는 엔티티를 넘겨주어 삭제할 수 있다. 엔티티의 저장하거나 수정할 때와 마찬가지로, 삭제 쿼리를 쓰기 지연 SQL 저장소에 저장했다가 flush 시에 실제로 DB 에서 삭제된다. 참고1: https://en.wikipedia.org/wiki/Object-relational_mapping 참고2: 자바 ORM 표준 JPA 프로그래밍","link":"/2020/07/26/jpa-basic/"},{"title":"Maven 시작하기","text":"Maven 은 Ant 의 대안으로 만들어진 자바용 프로젝트 관리 도구이다. 본 글에서는 메이븐의 핵심 용어인 Lifecycle, Phase, 그리고 Goal 에 대해 알아본다. Maven 의 Lifecycles Maven 은 3개의 lifecycle 로 이루어져 있다. default 프로젝트를 배포하기 위한 라이프 사이클 clean 프로젝트를 클린하는 라이프 사이클 site 프로젝트의 사이트 문서를 만들기 위한 라이프 사이클 Maven 의 Phases 각각의 라이프 사이클은 여러개의 Phase 로 이루어져있다. 예를 들어, default 라이프 사이클은 다음과 같은 phase 들로 이루어져 있다. validate - 프로젝트가 올바른 상태인지, 그리고 필요한 정보들은 모두 있는지 검증한다 compile - 소스코드를 컴파일한다 test - 알맞은 유닛 테스트 프레임워크로 테스트한다. 이러한 테스트는 코드가 패키징되거나 배포되었을 것을 요구헤선 안된다 package - 컴파일된 파일을 JAR와 같이 배포 가능한 형태로 패키징한다. verify - 통합 테스트를 수행한다 install - 로컬에서 다른 프로젝트의 의존성으로 사용될 수 있도록 이 패키지를 로컬 저장소에 인스톨한다 deploy - 패키지를 원격 저장소에 복사한다 물론 위의 phase 말고도 다른 phase 들이 있다. 만약 다음과 같은 명령어로 특정 phase 를 실행하고자 하면, mvn verify default 라이프 사이클에 있는 phase 들을 validate 부터 verify 까지 순차적으로 실행한다. 프로젝트 내에 서브 프로젝트들이 있다면 메이븐은 모든 서브 프로젝트들에 대해 같은 동작을 수행한다. 보통 이름에 하이픈(-)이 붙은 Phase(pre-*, post-*, or process-*)들은 커맨드 라인에서 직접 호출되지 않는다. 이 Phase 들은 외부에는 불필요한 중간 결과물을 만들기 때문이다. 예를 들어, integration-test phase 에는 Jacoco 같은 코드 커버리지 툴이나 Tomcat, Docker 같은 컨테이너 플러그인들이 pre-integration-test Phase 에 Goal 을 바인딩 하여 컨테이너 설정 등 환경 세팅을 하도록 하며, post-integration-test Phase 에도 Goal 을 바인딩하여 커버리지 통계를 수집하거나 컨테이너를 해제하도록 한다. 고로, 커맨드 라인에서 integration-test Phase 만 직접 호출하면 커버리지 레포트도 나오지 않으며, 톰캣 웹서버나 도커 인스턴스는 실행 중인 채로 남게 된다. Maven 의 Goals Goal 은 Phase 보다도 한 단계 하위 개념이며, 보통 jacoco:report 나 spring-boot:run 과 같이 plugin:goal 의 형태로 적는다. 각각의 Goal 은 0개 이상의 Phase 에 포함되며, 각각의 Build Phase 는 0개 이상의 Goals 로 이루어진다. Phase 가 실행된다는 것은 Phase 에 속한 Goal 들이 실행된다는 의미다. 메이븐 2.0.5 이상 버전에서 Phase 내 Goal 들은 POM 에 선언된 순서대로 실행된다. 어떠한 Phase 에도 속하지 않은 Goal 은 Phase 가 실행될 때 자동으로 실행되는 것이 아니라 다음의 dependency:copy-dependencies goal 과 같이 외부적에서 직접 명시하여 실행하는데 사용된다. mvn clean dependency:copy-dependencies package 위와 같은 명령어를 실행하면 적어준 순서대로 clean phase 를 먼저 실행하고(물론 clean lifecycle 에서 clean phase 까지의 모든 phase 들을 순차적으로 실행한다는 뜻) dependency:copy-dependencies goal 을 실행하고, 마지막으로 package phase 를 실행한다(마찬가지로 default lifecycle 에서 package phase 까지 순차적으로 실행) 만약 특정 Phase 에 속한 Goal 이 하나도 없다면, 해당 Phase 는 실행되지 않는다. Plugins 을 통해 새로운 Goals 을 Phases 에 추가할 수도 있다. 예를 들어, Compiler 플러그인은 메인 코드를 컴파일하는compile Goal 과 테스트 코드를 컴파일하는 testCompile Goal 을 가지고 있다. 다음은 POM 의 의 안에있는 특정 플러그인에 goals 을 설정해준 예다. ... &lt;plugin&gt; &lt;groupId&gt;com.mycompany.example&lt;/groupId&gt; &lt;artifactId&gt;display-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;process-test-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;time&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;... 참고: http://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html","link":"/2020/07/10/maven-basic/"},{"title":"파이썬과 동시성 프로그래밍","text":"파이썬의 동시성 프로그래밍과 관련하여 삽질한 내용을 바탕으로 블로그 포스팅을 작성해보았다. 지금까지는 파이썬으로 비교적 단순한 코드만 작성해 보았고, 팀에서 주로 사용하는 언어가 스칼라이기 때문에 파이썬에서의 동시성 프로그래밍에 대해서는 완전히 문외한이었다. 하지만 어쩌다 파이썬으로 동시성 프로그래밍을 해야할 일이 생겼고, 간단하게나마 내용을 정리해보았다. 동시성과 병렬성 우선 동시성(Concurrency)과 병렬성(Parallelism)의 차이에 대해 이야기 해보자. 철수가 소파에 앉아있고, 앞에 3개의 TV 가 서로 적당한 간격을 두고 있다고 가정하자. 철수는 자신이 가장 좋아하는 예능 프로그램인 무한도전, 1박2일, 아는형님을 각각의 TV 에 틀어놓았다. 3개의 예능 프로그램을 너무나도 좋아한 나머지 동시에 보고싶었기 때문이다. 철수는 우선 무한도전을 틀어놓은 TV를 10초 보다가, 1박2일을 틀어놓은 TV를 3초 보다가, 아는형님을 틀어놓은 TV를 5초간 보았다. 여기서 알 수 있는 사실은, 철수는 3개의 예능 프로를 동시에(Concurrent) 시청하고 있지만 한 번에 하나의 TV 만 볼 수 있다. 즉, 눈이 3쌍이 아니기 때문에 3개의 TV 를 한 번에(Parallel)볼 수는 없는 것이다. 물론 한국어와 영어 단어 사이에 1대1 대응이 어려워 두 행위 모두 '동시에'라고 얘기할 수 있기 때문에 헷갈릴 수도 있다. 하지만 철수가 한 번에 하나의 TV 만 볼 수 있기 때문에 병렬성은 없는 것이며, 동시에 여러개의 예능 프로를 본다는 것은 동시성은 있는 것이다. 컴퓨터의 세계에서는 철수를 컴퓨터로, 철수의 눈 한 쌍이 CPU 의 코어 하나라고 볼 수 있고, 각 예능 프로그램이 스레드라고 볼 수 있다. 여러개의 스레드가 실행중이지만 싱글 코어 CPU 이기 때문에 한 번에 하나의 작업만 수행할 수 있는 것이다. 물론 여러 스레드를 아주 빠른 속도로 번갈아 수행하면 마치 병렬적으로 수행되는것처럼 보일 수도 있다. 반면 멀티 코어 CPU 에서 여러개의 스레드가 동시에 여러개의 코어에서 실행될 수 있다. 어떤 특정 시점에 두 개 이상의 스레드가 코어에서 실행 중이라면 그것은 병렬성이 있다고 하는것이다. 파이썬에서의 동시성 프로그래밍 멀티프로세싱과 멀티스레딩 파이썬에서 동시성 프로그래밍을 하는 방법은 정말 많다. 우선 가장 쉽게 떠올릴 수 있는 방법은 멀티프로세싱과 멀티스레딩이다. 파이썬에는 GIL(Global Interpreter Lock)이라는 것이 있어서, 하나의 process 내의 여러개의 thread 가 병렬적으로 실행될 수 없다. 즉, 멀티 코어 CPU 에서 동작한다고 하더라도 하나의 프로세스는 동시에 여러개의 코어를 사용할 수 없다는 뜻이다. 그렇기 때문에 만약 수행하고자 하는 작업이 CPU bound job 이고 multi-core CPU 환경인 경우에는 멀티프로세싱을 사용하는 것이 유리하다. 왜냐하면 하나의 프로세스 내에서 아무리 여러개의 스레드를 만들어봐야, 하나의 스레드에서 순차적으로 수행하는것과 비교하여 딱히 성능이 좋아지지 않기 때문이다. context switching 을 생각하면 멀티스레딩 쪽이 오히려 더 느릴 수도 있다. 게다가 여러개의 스레드를 사용하면 메모리 사용량도 많아진다. 하지만 만약 수행하고자 하는 작업이 I/O bound job 이라면 이야기가 달라진다. 어떤 스레드가 I/O 를 수행하기 위해 block 이 되면 GIL 을 반환하게 되고, 그 동안 다른 스레드가 실행될 수 있기 때문이다. 물론 복수의 스레드가 복수의 코어에서 병렬적으로 실행될 수 없다는 사실은 변함이 없지만, 하나의 스레드만 사용하여 여러 작업을 동시에 수행하고자 하는 경우에는 이 스레드가 block 이 되면 아무런 일도 하지 않게 되기 때문에 이런 경우에는 멀티스레딩을 사용할 가치가 충분히 있는것이다. 하지만 스레드는 직접 사용하기가 까다롭다. race condition 도 발생할 수 있고, 메모리 사용량과 context switching 측면에서도 비용이 비싸다. 다음은 David Beazley 의 발표 영상에서 발췌한 파이썬에서 동시성 프로그래밍의 발전 과정을 단순화한 그림이다. 영상을 보면 파란 화살표가 둥그렇게 돌아서 다시 스레드와 가까운 쪽으로 오도록 그린 의도가, 스레드를 직접 사용하는 것의 단점에서 출발하여 결국은 스레드와 비슷한 모양새로 발전해왔기 때문이라고 한다. asyncio 와 같은 라이브러리들은 마치 스레드를 사용하는 것과 비슷한 느낌이 들지만, 실제로 기본적으로는 여러 스레드를 사용하지 않기 때문에 스레딩의 단점이었던 몇몇 특징을 가지고있지 않다. 코루틴 파이썬에서 동시성 프로그래밍을 할 수 있는 또다른 방법은 코루틴(Coroutine)을 사용하는 것이다. 코루틴은 특정 언어에 종속되는 개념이 아닌 general 한 개념이다. 보통의 프로그램은 함수1에서 함수2를 호출한 경우 함수2를 서브루틴이라고 부른다. 이 때 함수1과 함수2는 종속적인 관계를 갖는다고 볼 수 있다. 함수2에 정의된 일련에 코드가 모두 수행되면 항상 함수1로 실행의 흐름이 돌아가게 된다. 하지만 코루틴은 서로 종속적인 관계가 아닌 상호 협력적인(cooperative) 관계다. 코루틴1은 원하는 시점에 코루틴2, 혹은 코루틴3 에게 실행의 흐름을 넘겨줄 수 있다. 그러면 코루틴1은 실행 흐름을 넘겨준 지점에서 멈춰있는 상태가 되고 나중에 언제든지 해당 지점부터 다시 이어서 실행될 수 있는 상태가 된다. 실행 흐름을 양보(yield)받은 코루틴은 처음 실행되는 거라면 처음부터 실행되고 만약 이전에 코드를 실행하다 멈춰져있던 상태라면 멈춘 지점부터 다시 순차적으로 코드가 실행되기 시작한다. 그리고 마찬가지로 원하는 시점에 다른 코루틴으로 실행 흐름을 넘겨줄 수 있다. 이것은 한편으로는 스레드와 비슷하다. 다른 코루틴에게 실행의 흐름을 넘겨주는 행위는 스레드간의 컨텍스트 스위칭과 비슷하다고 할 수 있다. 다만 스레드간의 컨텍스트 스위칭이 발생하는 시점과 다음에 어떤 스레드가 실행될 것인지 결정하는 것은 kernel 영역에서, 즉 OS에 의해 결정되는 반면, 코루틴간에 흐름이 변경되는 지점은 user 영역에서, 즉 개발자가 코드로 명시하게 된다. 즉, 코루틴간의 제어의 흐름을 개발자가 완벽하게 컨트롤할 수 있다는 뜻이다(물론 여러 코루틴이 실행되는 스레드가 컨텍스트 스위칭에 의해 실행이 멈춰 그 안에서 수행되던 모든 코루틴의 실행도 함께 멈출 수는 있다). 이는 preemptive 한 OS 의 native thread 와는 대조된다. 게다가 코루틴은 스레드보다 메모리 사용량도 적으며, 컨텍스트 스위칭으로 인한 비용도 없다. 파이썬에서는 주로 이 코루틴과 이벤트 루프를 함께 사용하여 편리하게 동시성 프로그래밍을 할 수 있게 하는 패키지들이 많다. 많은 패키지들은 내부적으로 다음과같이 구현되어있다. 코루틴은 큐 안에 들어가고 이벤트 루프는 큐 안에 있는 코루틴을 하나씩 빼 코드를 실행하다가 비동기 작업이 시작되면 해당 라인에서 멈추어 다시 큐의 뒷부분에 넣는다. 만약 큐에서 코루틴을 꺼냈는데 비동기 작업이 종료된 상태라면 결과값을 사용하여 멈췄던 부분부터 다시 코드를 실행한다. 이렇기 때문에 하나의 스레드로도 여러 비동기 작업들을 동시에 수행할 수 있는 것이다. 또한 우리가 원하는 시점에 context switching 없이 다음 코루틴에게 실행권을 양도할 수 있게된다. 파이썬에서 코루틴 사용하기 generator 파이썬에서 코루틴을 사용하는 방법은 여러가지가 있다. 우선 generator 를 코루틴으로 사용할 수 있다. generator 의 동작 방식을 보면 코루틴의 동작방식과 유사하다. 제너레이터 내부에서 yield 키워드를 만날 때마다 실행의 흐름이 generator 를 인자로 next() 함수를 호출한 쪽으로 넘어가면서 값을 뱉는다. 그리고 언제든지 next() 메소드를 사용하여 generator 의 이전에 멈춘 지점부터 실행하여 다음 yield 문을 만날 때까지 실행된다. 파이썬 3.4 부터는 subgenerator 에게 위임하기 위한 문법으로 yield from이 추가되었는데 이를 통해 generator 를 활용한 동시성 프로그래밍이 더 편해졌다. asyncio 또 다른 방법은 파이썬 빌트인 라이브러리인 파이썬 3.4 버전부터 도입된 asyncio 와 파이썬 3.5 버전부터 도입된 async/await 키워드를 통한 방법이다. asyncio 는 자체적으로 coroutine 과 event loop 를 가지고 있어서 이를 통해 동시성 프로그래밍을 지원한다. 동작 원리와 구현 스타일은 전체적으로 generator 방식과 매우 유사하다. 개인적으로는 현시점에서 굳이 asyncio 를 안 쓸 이유가 있나 싶다. 왜냐하면 asyncio 는 파이썬에서 공식적으로 지원하는 built-in 패키지이기 때문에 수많은 파이썬 서버와 라이브러리가 이 asyncio 를 지원하며, 문서화도 잘 되어있기 때문이다. 3rd-party 라이브러리 또 하나는 써드파티 라이브러리를 통한 방법이다. 파이썬에는 greenlet 과 같은 써드파티 코루틴 라이브러리들이 있다. 대표적으로 gevent 는 내부적으로 greenlet 를 사용하며, 거기에 libev 라는 이벤트루프와 monkey patch 라는 기술 등등을 사용하여 동시성을 구현한다. greenlet 을 green thread 라고도 부른다. green thread 또한 general 한 개념인데, 위에서 설명한 코루틴의 특징과 비슷하게, OS가 아닌 user 영역에서 생성되고 관리되며, cooperative 한 lightweight 스레드라고 보면 된다. asyncio 가 등장하고 어느정도 정착되기 전까지는 gevent 와 같은 써드파티 라이브러리들이 많이 사용되었고, 아직도 많이 사용되는것 같다. 파이썬 서버 및 프레임워크와 동시성 보통 파이썬으로 프로덕션 수준의 서버를 개발한다면 Flask 의 Werkzeug 과 같이 프레임워크에 내장된 WSGI 서버를 사용하기 보다는 uWSGI 나 Gunicorn 와 같은 별도의 서버를 사용하는 경우가 많다. gunicorn 의 경우는 worker-class 옵션을 통해 각 worker process 가 어떤 방식으로 요청을 처리하게 할 지 설정할 수가 있는데, 기본적으로는 하나의 worker process 가 최대 하나의 요청만 동기적으로 처리하게 된다. worker-class 옵션을 gthread 로 설정하여 멀티스레드 기반으로 동작하도록 할 수도 있고, gevent 와 같은 비동기 worker 를 사용하여 처리하도록 할 수도 있다. 주의할 점이 있다면 gunicorn 에서 gevent worker 를 사용하고 스크립트 코드상에서는 asyncio 를 사용한다면 각자 코루틴과 이벤트 루프의 구현체가 다르기 때문에 둘의 호환을 위한 중간 라이브러리를 추가로 사용하거나 두 곳에서 같은 라이브러리를 사용하도록 통일해야 한다. 애초부터 비동기 프레임워크를 사용할 수도 있다. 특히 Sanic 이나 Quart 같은 프레임워크는 플라스크와 사용 방법이 매우 비슷하여 처음 사용하는데도 큰 무리가 없다. 이들은 비동기 서버를 내장하여 뷰 함수를 작성할 때 async def 와 같이 정의한다. 비동기 서버로 Uvicorn 이나 Hypercon 과 같은 ASGI 서버를 사용할 수도 있다. Hypercon 은 본래 Quart 내장 서버에서 분리된 프로젝트여서 Quart 는 기본적으로 Hypercon 을 사용한다. 아무튼 위와 같은 비동기 서버 및 비동기 worker 들은 멀티스레드를 사용하지 않고도 여러개의 요청을 비동기적으로 처리할 수 있게 한다. 참고 자료 GIL 유튜브 영상(https://www.youtube.com/watch?v=Obt-vMVdM8s) python coroutine 원리 유튜브 영상(https://www.youtube.com/watch?v=MCs5OvhV9S4) python concurrency 유튜브 영상(https://www.youtube.com/watch?v=lYe8W04ERnY) gevent 유튜브 영상(https://www.youtube.com/watch?v=GunMToxbE0E)","link":"/2020/11/25/python-and-concurrency/"},{"title":"[책 리뷰] Kubernetes in Action","text":"본 포스트에서는 쿠버네티스 인 액션(Kubernetes in Action) 재출간판에 대한 리뷰를 한다. 쿠버네티스 인 액션 재출간판은 2판이 아니라 1판의 초기 번역본의 번역 품질이 좋지 않아 재출간을 버전인 것으로 알고 있다. 이 책은 쿠버네티스를 제대로 공부하고 싶은 입문자라면 강력하게 추천하고 싶은 책이다. 책의 장점 다음은 이 책의 장점을 크게 5가지로 정리해보았다. 번역 품질 번역의 질이 굉장히 좋다. 여러 번역서를 읽어봤지만, 어떤 번역서의 경우는 분명히 읽었는데 읽은것 같지 않거나 몇 초만에 머릿속에서 사라지게 하는 신기한 경험을 선사한다. 하지만 이 책은 정말 술술 잘 읽힌다. 가끔 다른 번역서를 읽다보면 굳이 이것을 한글로 번역해야하나 라는 생각이 들 때도 있고, 단어만 그대로 직역하여 옮긴듯하여 분명 한국어인데 이해가 잘 가지 않는 경우가 많다. 하지만 이 책은 굉장히 매끄럽게 잘 읽힌다. 이 책의 경우도 초기판은 번역 품질이 좋지 않다고 들었다. 그렇기 때문에 만약 책을 구매하려고 한다면 재출간판인지 잘 확인하자. 내용의 순서 내용의 순서가 정말 좋다. 쿠버네티스 문서도 많은 정보를 제공하지만 레퍼런스의 성격이 강하기 때문에 이해에 최적화된 순서로 구성되어있지는 않다. 그래서 처음 공부하는 사람은 읽어도 이해할 수 없는 내용이 앞 부분부터 가득하다. 예를 들어 문서의 경우 쿠버네티스란 무엇인지 설명하고 나서, 바로 쿠버네티스의 구조와 여러 컴포넌트에 대해 설명한다. 반면, 책에서는 의도적으로 절반도 훌쩍 지난 11장에서야 쿠버네티스의 구조와 내부 동작 원리에 대해 설명한다. 물론 내부 구조를 먼저 알고 싶은 사람이 있을 수도 있지만, 아는 만큼 보인다는 말처럼 쿠버네티스의 기능과 사용 방법에 대해 모르는 상태에서는 읽어도 별로 와닿지 않을 가능성이 높다고 생각한다. 책을 읽으면서 내용의 순서에 신경을 많이 썼다는 느낌을 많이 받았다. 책을 처음부터 순서대로 따라 읽다보면 자연스레 빠르게 이해가 되도록 챕터가 배치되어있다. 물론 꼭 순서대로 읽어야 하는 것은 아니고, 그때 그때 궁금한 개념이 있을 때마다 건너뛰면서 읽어도 된다고 생각한다. 나도 그랬다. 하지만 일부 챕터가 연관성을 가지고있기 때문에 특별한 이유가 없다면 순서대로 읽으면 이해에 더 도움이 될것 같다. 내용의 범위 쿠버네티스의 전반을 자세하게 다뤘다. 설명도 굉장히 친절하다. 물론 이 책을 모두 읽는다고 쿠버네티스를 마스터할 수 있는 것은 아닐 것이다. 하지만 꼭 알아야하는 핵심적인 내용을 전반적으로 다뤘다. 그렇기 때문에 만약 이 책을 모두 읽고 나서 모르는 내용이 나온다고 하더라도 쉽게 찾을 수 있을 것이라고 생각한다. 도표와 예제 쿠버네티스가 여러 컴포넌트로 구성된 꽤 복잡한 시스템인 만큼, 이해를 도와주는 그림이 굉장히 중요하다고 생각한다. 이 책도 그림이 적절하여 이해에 도움을 준다. 예제도 따라하고 이해하기 쉽다. 최근 버전과의 차이 최근 버전에서 달라진 점들이 잘 표시되어 있다. 가끔 일부 서적들은 예제를 그대로 따라 쳤는데도 다른 결과가 나오는 경우가 있다. 물론 대부분의 경우는 별 문제가 되지 않거나 구글링을 조금 해보면 금방 해결할 수 있지만, 이런 일이 반복되면 책을 읽는 것이 지칠 수가 있다. 이 책은 최근 버전에서 달라진 점들에 대해 잘 명시하였고 위와같은 경우가 특별히 없었던것 같다. 결론 읽기 잘했다는 생각이 들었다. 좋은 책은 새로운 기술을 학습할 때 기초를 빠르게 습득하는데 큰 도움을 준다고 생각한다. 물론 책에 나오지 않는 내용은 문서나 소스 코드 분석을 통해 보충해야 할 것이고, 가장 중요한 것은 실무에서 사용하면서 익히는 것이라고 생각한다. 하지만 지식의 수준을 어느정도 궤도에 빠르게 올리는 것이 목적이라면 이 책은 후회없는 선택이 될 것이라고 생각한다.","link":"/2021/01/26/kubernetes-in-action-review/"},{"title":"[책 리뷰] Programming in Scala","text":"Programming in Scala 는 Scala 의 창시자인 마틴 오더스키(Martin Odersky) 가 집필한 Scala 서적이다. 저자 소개 일단 책의 저자인 마틴 오더스키에 대한 설명이 필요할 것같다. 마틴 오더스키는 세계적인 명문 대학인 스위스의 로잔 연방 공과대학교(약칭 EPFL)의 교수이며, Akka[1] 를 만든 Jonas Boner 등과 함께, 현재는 라이트밴드(Lightbend)로 이름이 바뀐 타입세이프(Typesafe)사의 공동 창업자다. 프로그래밍 언어와 시스템 분야에서 일해왔으며, 주로 객체지향과 함수형 프로그래밍을 조합하는 분야를 연구해왔다. 자바 제네릭 설계자 중 한 명으로 자바 언어에 크게 기여하였고, javac 컴파일러를 맨 처음 작성한 프로그래머다. 2001년 이후에는 스칼라를 설계하고 구현하고 다듬는 일을 해왔으며, ACM의 펠로우이기도 하다. 책 구매 동기 내가 처음 스칼라를 공부하려고 할 때 처음으로 본 것은 Coursera 에 있는 마틴 오더스키의 강좌[2]였다. 언어의 창시자가 강좌를 올리는 경우는 흔하지 않는데다, 꽤 유명한 강좌였고, 평도 좋은 편이기 때문에 한 번 쯤은 보고 싶었다. 강의의 내용은 굉장히 좋았다. 사실 이 강좌의 이름은 Functional Programming Principles in Scala 인데, 이름 처럼 단순히 스칼라에 대한 강의라기 보다는 함수형 프로그래밍에 대한 내용이 중심이다. 하지만 '스칼라를 통한' 함수형 프로그래밍 강좌인 만큼, 스칼라의 몇몇 API 와 문법들에 대한 설명도 있기 때문에 스칼라에 대해서도 어느정도 경험할 수 있었다. 하지만 이것만으로는 뭔가 조금 아쉬운 느낌이 들었다. 스칼라에 대해 좀 더 알고 싶었다. 그래서 924 페이지에 달하는 분량의 Programming in Scala[3] 를 구매하게 되었다. 엄청난 분량임에도 이 책을 구입한 이유는, 우선 저자인 마틴 오더스키가 스칼라 언어의 창시자라는 점이 꽤 컸고, 코세라 강좌에서 처럼 책에서도 설명을 잘 해주지 않을까 하는 기대에서였다. 읽어보니 정말 설명이 괜찮았다. 그리고 스칼라의 모든 문법을 한 번쯤은 다 훑어보고 싶었다. 책의 특징 스칼라의 창시자 답게 스칼라를 만든 동기와 철학, 역사에 대해서도 기술 하였으며, 단순히 스칼라의 기본적인 문법과 API, 그리고 활용에 대한 내용 뿐 아니라 함수형 프로그래밍에 대한 내용도 있어 굉장히 유익하다. 물론 함수형 프로그래밍에 대해서 더욱 상세하게 알고 싶다면 '스칼라로 배우는 함수형 프로그래밍(functional programming in scala)'[4] 와 같은 책을 별도로 봐야겠지만, 이 책에도 순수 함수, 부수 효과, 일급 함수 등 함수형 프로그래밍의 기본적인 용어에 대한 내용을 가볍게 다뤄서 FP 에 대한 기본적인 개념에 대해 배울 수 있다. 명령형(imperative)이 아닌 functional 하게 코드를 작성하는 방법에 대한 팁이나 FP 의 장점 등에 대한 내용도 있어 유익하다. 자바에 큰 기여를 한 사람 답게 자바와의 차이를 중심으로 설명하는 부분들도 꽤 있는데, 자바에 익숙한 개발자라면 이런 부분은 더 잘 읽힐 것이다. 유일한 단점을 꼽자면 책의 분량이 너무 많다는 것인데, 그럼에도 불구하고 충분히 읽을만 한 가치가 있다고 생각한다. 왜냐하면 그 만큼 유익한 내용이 많고, 스칼라의 거의 모든 문법을 상세히 다루기 때문이다. 결론 스칼라를 제대로 알고 사용하고 싶은 사람이라면 추천하고 싶다. 하지만, 만약 새로운 언어를 배울 때 빠르게 기본적인 문법을 훑고 바로 써먹는 것을 좋아하는 싶은 사람이라면 맞지 않을 수도 있을것 같다. 물론 이 책을 모두 읽는다고 해서 스칼라 프로그래밍을 잘 할 수 있게 되는 것은 아닐 것이다. 함수적으로 코드를 작성하는 것은 제쳐 두더라도, 스칼라 자체가 굉장히 공부할 게 많은 언어라고 생각하고, 실무에서 스칼라를 사용하여 개발을 하기 위해서는 추가적으로 다양한 라이브러리를 알아야 하기 때문이다. Scala 라는 언어가 굉장히 매력적인 언어긴 하지만 러닝 커브가 높은 언어임에는 분명하다는 생각이 든다. 하지만 이 책을 읽고나서 든 생각은 읽길 잘했다는 것이었다. 개인적으로 어떤 언어를 처음 배울 때 구글링을 통해 기본 문법과 함수들을 빠르게 학습하여 사용하면 어딘가 허전한 기분이 들 때가 있다. 이 책은 이런 허전함을 없애 주기에 충분했고 스칼라의 여러 매력을 느끼게 해준 매우 유익한 책이었다. https://akka.io/ ↩︎ http://coursera.org/learn/progfun1 ↩︎ http://www.yes24.com/Product/Goods/96640057 ↩︎ http://www.yes24.com/Product/Goods/16969986 ↩︎","link":"/2021/01/09/programming-in-scala/"},{"title":"WSGI 란?","text":"Python 으로 서버 개발을 하다보면 WSGI, uWSGI, Werzeug 등의 단어들이 자주 보인다. 본 포스트에서는 이것들에 대해 알아본다. Web Server 와 Web Application 우선 Web Server 의 개념과 간단한 히스토리를 알아야 한다. Web Server 는 다소 광범위한 용어라서 Hardware 로서의 의미와 Software 로서의 의미를 함께 가지고 있다. 하지만 여기서는 Software 로서의 의미에 대해 이야기하겠다. Web Server 는 Client 의 정적인 리소스(File, Image, HTML page, ...) 요청을 처리하는 프로그램이며, 대표적으로 Apache HTTP Server 와 Nginx 가 있다. 그리고 동적인 요청이 들어오면 비즈니스 로직을 수행하기 위해 Web Application(혹은, WAS, AS) 에게 요청을 위임하고, Web Application 은 Web Server 에게 로직을 수행한 결과를 다시 돌려준다. 그럼 Web Server 가 Web Application 과 대화할 수 있는 인터페이스가 필요할 것이다. 특히, 다양한 종류의 Web Server 와 Web Application 을 Interchangable 하게 사용하기 위해서는 잘 정의된 인터페이스가 필수다. CGI 2003년까지 Python Web Framework 는 주로 CGI 와 같은 방식으로 Web Server 와 대화했다. CGI(Common Gateway Interface) 는 다음과 같이 동작한다. 먼저 Web Server 가 Client 로 부터 HTTP Request 를 받는다. Web Server 는 Request 에 대한 정보(Method, Url, Parameters, ...)를 Environment Variable 과 Standard Input 을 통해 전달하면서 Script 를 실행한다. Script 는 비즈니스 로직을 수행하고 Standard Output 으로 결과를 Web Server 에게 전달한다. Web Server 는 이를 다시 Client 에게 전달한다. 문제는 매번 다시 스크립트를 실행하여 메모리에 적재하는 과정에서 발생하는 추가적인 시간 소요 등이었다. 이 때 2003년에 Python 표준(PEP333)인 WSGI 가 등장하게 된다. WSGI WSGI(Web Server Gateway Interface)(위스키라고 읽는다) 는 Callable Object 라는 녀석을 통해 Web Server 가 요청에 대한 정보를 Application 에 전달한다. Callable Object 는 Function 이나 Object 의 형태가 될 수 있으며, Web Server 는 Callable Object 를 통해 2가지 정보를 전해주어야 한다. HTTP Request 에 대한 정보(Method, URL, Data, ...) Callback 함수 다음은 각각 함수와 클래스 형태로 정의된 Callable Object 의 예이다. environ 이 HTTP Request 에 대한 정보를 담고 있고, start_response 가 Web Server 에게 결과를 돌려주기 위한 콜백 함수다. def application(environ, start_response): body = b'Hello world!\\n' status = '200 OK' headers = [('Content-type', 'text/plain')] start_response(status, headers) return [body] class Application: def __init__(self, environ, start_response): self.environ = environ self.start_response = start_response def __iter__(self): body = b'Hello world!\\n' status = '200 OK' headers = [('Content-type', 'text/plain')] self.start_response(status, headers) yield body Web Application 는 HTTP Request 에 대한 정보를 가지고 Business Logic 을 수행한 뒤에 Callback Function 을 통해 결과를 웹서버에 반환한다. 이러한 WSGI Interface 를 구현하는 Web Server 나 Application 을 WSGI compatible 하다고 하며, WSGI compatible 한 Application 을 WSGI Application 이라고 부르기도 한다. 또한, 이 WSGI 인터페이스를 구현하기만 한다면 누구나 Python Web Server 나 Python Framework 를 만들어서 기존에 WSGI 를 지원하던 웹서버나 프레임워크와 함께 동작하게 할 수 있다. WSGI Middleware Web Service 를 개발하다 보면 공통적으로 필요한 기능들이 있는데, Authentication, Routing, Session, Cookie, Error Page 보여주기, ... 와 같은 기능들이다. WSGI Middleware 는 Middleware 라는 이름처럼 Web Application 의 실행 전과 후에 이러한 기능들을 추가해주는 녀석이며, 그 자체로도 WSGI Application 이다. Design Pattern 의 Decorator Pattern 을 생각하면 이해가 쉽다. 양파 껍질 처럼 Web Application 을 감싸고 있는 구조이다. uWSGI, Gunicorn, Werkzeug 글 맨 처음에 언급한 uWSGI, Gunicorn, Werkzeug 과 같은 라이브러리들이 바로 위에서 이야기한 WSGI Middleware 의 기능을 가진 라이브러리들이다. WSGI Middleware 역할 외에도 uWSGI, Gunicorn, Werkzeug 은 자체적으로 Web Server 의 역할을 할 수도 있다. 그래서 WSGI Server 나 Stand alone WSGI Container 라고 불리기도 한다. Flask 에서 기본적으로 사용하는 WSGI Middleware 가 Werkzeug 인데, 별다른 설정없이 Flask 앱을 실행 해보면 Production 에서는 사용하지 말라는 경고 문구가 나올 정도로 단순한 개발용 서버를 내장하고 있다. Production Level 에서는 Nginx, Gunicorn, Django 와 같은 식의 구성을 많이 사용하기도 한다. Nginx 는 주로 Buffering, Reverse Proxying, Load Balancing 등의 기능을 위해 Gunicorn 앞단에 배치하고, Gunicorn 은 Django 로 작성한 Web Application 에 HTTP 요청을 전달해주는 역할의 WSGI HTTP Server 로서 사용하는 것이다. Gunicorn 을 사용할 땐 worker process 의 개수와 worker class(async 방식인 Gevent, Tornado, ...)를 설정하여 요청 처리 성능을 높일 수 있다. 더 읽을거리 https://www.ibm.com/cloud/learn/web-server-vs-application-server https://www.nginx.com/resources/glossary/application-server-vs-web-server/ https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface https://en.wikipedia.org/wiki/Common_Gateway_Interface https://www.python.org/dev/peps/pep-0333/ https://www.youtube.com/watch?v=WqrCnVAkLIo http://xplordat.com/2020/02/16/a-flask-full-of-whiskey-wsgi/","link":"/2020/08/15/python-wsgi/"},{"title":"Redis 의 RESP 프로토콜","text":"Redis 클라이언트는 Redis 서버에(보통 6379번 포트) TCP 커넥션을 맺어 통신을 하는데 RESP 라는 프로토콜을 사용하여 통신을 한다. RESP 란? RESP(REdis Serialization Protocol) 는 Redis 클라이언트가 Redis 서버와 통신할 때 사용하는 프로토콜이다. RESP 는 Redis 1.2 버전에서 처음 소개되어 2.0 버전부터는 Redis 클라이언트가 구현해야만하는 표준 프로토콜이 되었다. (참고로 RESP 는 Redis 의 client-server 통신에만 사용되며, Redis Cluster 에서 노드간의 통신은 RESP 가 아닌 binary protocol 을[1] 사용한다.) RESP 는 꼭 Redis 가 아니더라도 Client-Server 방식으로 통신한다면 어디에서든 사용될 수 있는 프로토콜이다. 꼭 TCP 에서 사용되어야 하는 것도 아니지만 Redis 에서는 TCP 로만 사용한다. RESP 는 다음 3가지 요소를 중점적으로 여기며 고안되었다. 쉬운 구현 빠른 파싱 사람이 읽을 수 있어야 함 위와 같은 장점에 더해, Redis 문서에 따르면 성능 또한 binary protocol 과 비슷한 수준이라고 한다. Request-Response 모델 Redis 는 기본적으로 다음의 2가지 경우를 제외하곤 Request-Response 모델을 사용한다. Pipelining 여러개의 명령어(Command) 를 한 번에 보내고 모든 답장이 올 때까지 기다린다. Pub/Sub 어떤 Client 가 특정 channel 을 subscribe 하면 push protocol 로 전환되어 더이상 명령어를 보내지 않게 된다. Redis 의 Request-Response 프로토콜은 다음과 같이 동작한다. 클라이언트가 Bulk String 의 Array 타입으로 명령어를 서버로 전송한다. 서버는 클라이언트가 보낸 명령어에 맞는 타입으로 응답들을 보낸다. RESP 의 Data Types RESP 에는 5가지의 데이터 타입이 존재한다. Simple Strings Erros Integers Bulk Strings Arrays 서버와 클라이언트가 주고받는 메시지 내 데이터의 타입은 위 5가지 중 하나이다. 데이터의 타입은 데이터의 첫번째 바이트를 통해 구분한다. 각 타입들의 첫번째 문자는 다음과 같다. Simple Strings 는 &quot;+&quot; Erros 는 &quot;-&quot; Integers 는 &quot;:&quot; Bulk Strings 는 &quot;$&quot; Arrays 는 &quot;*&quot; RESP 를 통해 오고가는 Request 와 Response는 항상 &quot;\\r\\n&quot; 로 끝난다. 그럼 각각의 타입에 대해 자세히 알아보자. Simple Strings Simple Strings 타입은 binary-safe[2] 하지 않은 일반 문자열을 전송할 때 사용하는 데이터 타입이다. (binary-safe 한 문자열을 전송하기 위해서는 이후 설명할 Bulk Strings 타입으로 전송해야 한다.) 앞서 설명한 것처럼 첫번째 바이트는 &quot;+&quot; 다. 뒤이어 문자열이 나오고 &quot;\\r\\n&quot; 으로 끝난다. 참고로 문자열은 Newline 을 포함할 수 없다. 다음은 Simple Strings 타입을 전송한 예다. &quot;+OK\\r\\n&quot; Errors Errors 타입은 이름대로 에러 정보에 대한 타입이며, &quot;-&quot; 문자로 시작한다. 강제는 아니지만 관습적으로 &quot;-ERRORTYPE Description for the error&quot; 와 같은 형태로 먼저 에러의 이름을 쓰고 한 칸 띄운 뒤에 에러 발생 원인과 같은 더 자세한 설명을 적는다. 클라이언트에서 이러한 타입의 데이터를 받으면, 에러 타입에 맞는 예외를 발생시키거나 false 를 반환하는 식으로 구현할 수 있겠다. 다음은 Errors 타입 데이터를 전송한 예이다. &quot;-ERR unknown command 'foobar'\\r\\n&quot; &quot;-WRONGTYPE Operation against a key holding the wrong kind of value\\r\\n&quot; Integers 정수 형태의 데이터이며, &quot;:&quot; 문자로 시작한다. 또한, 숫자의 크기는 signed 64 bit 범위 내여야 한다. 대표적으로, INCR, LLEN, LASTSAVE 와 같은 명령어에 대한 응답으로 오는 데이터의 타입이 이 Integers 타입이다. 일부 명령어(EXISTS 등) 에서는 true/false 의 의미로 이 Integers 타입의 1/0 을 쓰기도 한다. Bulk Strings binary-safe 한 문자열을 나타낼 때 사용하는 타입이며, &quot;$&quot; 로 시작한다. &quot;$&quot; 에 이어 문자열의 길이가 주어지고, 그 다음 &quot;\\r\\n&quot;, 그리고 실제 문자열이 등장한다. 즉, &quot;$문자열길이\\r\\n문자열\\r\\n&quot; 의 형태를 가진다. (e.g. &quot;$6\\r\\nfoobar\\r\\n&quot;) 빈 문자열을 나타낼 때는 &quot;$0\\r\\n\\r\\n&quot; 를 사용한다. 주의해야 할 점은, null 값을 나타낼 때는 &quot;$-1\\r\\n&quot; 를 사용한다는 것이다. 이것을 Null Bulk String 이라고 부른다. 만약 Redis 서버에서 Null Bulk String 을 받았다면 클라이언트에서는, 예를 들어 C 라이브러리라면 NULL 을 반환해야하하고, Ruby 라이브러리라면 nil 을 반환해야 할 것이다. Arrays Arrays 는 첫번째 byte 로 &quot;*&quot; 를 가지며, 이어서 10진수로 배열의 크기와 &quot;\\r\\n&quot; 이 나온다. 또한, 배열 내 원소는 각각 특정한 타입을 가질 수가 있으며, 그 타입은 모두 달라도 된다. 예를 들어 1 이라는 Integer, &quot;2&quot; 라는 Simple String, 그리고 &quot;bulk&quot; 라는 Bulk String 을 가지는 Arrays 타입의 Response 는 다음과 같은 형태를 띈다. &quot;*3\\r\\n:1\\r\\n+2\\r\\n$4\\r\\nbulk\\r\\n&quot; Null Array 의 경우에는 사이즈가 0인 Array 와 구분하기 위해 다음과 같이 표시한다. &quot;*-1\\r\\n&quot; 예를 들어 BLPOP command 가 timeout 이 났을 때 위와 같은 응답을 줄 수 있다. 클라이언트는 서버로부터 Null Array 를 받으면 빈 배열이 아닌, null object 를 반환해야 한다. 중첩된 형태, 즉 Arrays 의 Arrays 도 가능하다. 다음은 2개의 배열로 이루어진 배열이다. 편의상 알아보기 좋게 따옴표를 빼고 줄바꿈을 하였다. *2\\r\\n *3\\r\\n :1\\r\\n :2\\r\\n :3\\r\\n *2\\r\\n +Foo\\r\\n -Bar\\r\\n Redis 서버에 Command 보낼 때 맨 처음에 Redis 클라이언트가 서버에 명령어를 보낼 때 Bulk String 의 Array 타입으로 전송한다고 했다. 만약 Redis 서버에 저장된 mylist 라는 List 데이터의 길이를 알고자 한다면 LLEN mylist 명령어를 사용할 것이다. 그리고 서버는 이에 대한 응답으로 리스트의 길이를 반환할 것이다. 이때 실제로 Client 와 Server 간에 오고간 데이터를 뜯어보면 다음과 같을 것이다. (편의상 줄을 바꿈) Client *2\\r\\n $4\\r\\n LLEN\\r\\n $6\\r\\n mylist\\r\\n Server :48293\\r\\n 참고: https://redis.io/topics/protocol#resp-simple-strings binary protocol 는 전송하고자 하는 데이터를 human readable 한 방식으로 인코딩하는 text protocol 과는 반대되는 개념이다. Text protocol 의 대표적인 예인 HTTP 는 &quot;200 OK&quot; 라는 상태 코드의 200 이라는 값을 나타낼 때 '2', '0', '0' 이라는 문자를 사용한다. 하지만 만약 200 을 binary protocol 로 전송한다면 단지 200을 binary 로 나타낸 값을 전송할 것이다. 그렇기 때문에 binary protocol 이 좀 더 compact 한 형태라는 것을 알 수 있다. ↩︎ 문자열이 binary-safe 하다는 말은 문자열 내에 그 어떤 문자가 등장하더라도 문자열의 일부로 인식된다는 뜻이다. 예를 들어, C 에서 문자열 중간에 널 문자가 있다면 이를 문자열의 일부로 여기는 것이 아니라 널 문자 이후의 문자열을 무시하게 되는데, 이는 binary-safe 하지 않은 문자열의 예시이다. Redis 의 Bulk Strings 타입은 binary-safe 하다. 맨 앞에 문자열의 길이가 주어지는데, 그 길이 만큼은 어떠한 문자가 등장하더라도 문자열의 일부로 인식하기 때문이다. ↩︎","link":"/2020/07/22/redis-resp-protocol/"},{"title":"Spring Boot 의 @ConfigurationProperties","text":"@ConfigurationProperties 는 Spring Boot 에서 properties 파일에 정의된 프로퍼티 중 주어진 prefix 를 가지는 프로퍼티들을 POJO 에 매핑하여 Bean 으로 만들수 있게 해주는 어노테이션이다. 그럼 @ConfigurationProperties 의 다양한 쓰임새에 대해 알아보자. 단순한 형태의 Property @Configuration@ConfigurationProperties(prefix = \"mail\")public class ConfigProperties { private String hostName; private int port; private String from; // standard getters and setters} 스프링이 Bean 을 application context 에 만들도록 @Configuration 을 꼭 같이 붙여줘야하며, 만약 붙이지 않았을 경우에는 main Spring application 클래스에 @EnableConfigurationProperties(ConfigProperties.class) 를 붙여줘야한다. @SpringBootApplication@EnableConfigurationProperties(ConfigProperties.class)public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }} 스프링은 프로퍼티를 바인딩할 때 완화된 규칙을 적용하여 다음과같은 변형도 모두 같게 취급한다 mail.hostNamemail.hostnamemail.host_namemail.host-namemail.HOST_NAME Spring Boot 2.2 스프링 부트 2.2 에서는 @ConfigurationProperties 클래스들을 모두 찾아서 등록해주므로 @Component, 혹은 @Configuration 과 같은 어노테이션이나 @EnableConfigurationProperties 를 붙일 필요가 없다. 중첩 Property 기본 자료형 뿐만 아니라 List, Map, Class 도 만들 수가 있다. 다음과 같은 클래스가 있을 때, public class Credentials { private String authMethod; private String username; private String password; // standard getters and setters} 다음과 같은 ConfigProperties 클래스에 프로퍼티를 바인딩 하려면 public class ConfigProperties { private String host; private int port; private String from; private List&lt;String&gt; defaultRecipients; private Map&lt;String, String&gt; additionalHeaders; private Credentials credentials; // standard getters and setters} 다음과 같은 형태로 properties 를 작성하면 된다. #Simple propertiesmail.hostname=mailer@mail.commail.port=9000mail.from=mailer@mail.com #List propertiesmail.defaultRecipients[0]=admin@mail.commail.defaultRecipients[1]=owner@mail.com #Map Propertiesmail.additionalHeaders.redelivery=truemail.additionalHeaders.secure=true #Object propertiesmail.credentials.username=johnmail.credentials.password=passwordmail.credentials.authMethod=SHA1 @Bean 메소드에 사용하기 POJO 뿐만 아니라 @Bean 어노테이션이 붙은 메소드에도 사용할 수 있다. 우리가 코드를 수정할 수 없는(어노테이션을 붙일 수 없는) 써드파티 컴포넌트에 바인딩할 때 유용하다. 예를 들어 다음과 같은 이미 컴파일된 클래스가 있을 때 public class Item { private String name; private int size; // standard getters and setters} 다음과 같이 @Bean 이 붙은 메소드에 @ConfigurationProperties 를 붙이면 @Configurationpublic class ConfigProperties { @Bean @ConfigurationProperties(prefix = \"item\") public Item item() { return new Item(); }} 메소드의 리턴 타입인 Item 객체에 바인딩된다. Property 유효성 검사 아래와 같이 빈 문자열 검사, 문자열 길이 범위, 정수 범위, 정규표현식을 통한 문자열의 형태를 체크할 수 있다. @NotBlankprivate String hostName;@Length(max = 4, min = 1)private String authMethod;@Min(1025)@Max(65536)private int port;@Pattern(regexp = \"^[a-z0-9._%+-]+@[a-z0-9.-]+\\\\.[a-z]{2,6}$\")private String from; 만약 유효성 검사가 실패하면 IllegalStateException 이 발생하여 main application 의 실행이 실패한다. Property 변환 Duration, DataSize 와 같은 타입은 스프링이 자동으로 변환해준다. Duration Duration 은 시간을 나타내는 타입이다. 다음과 같은 Configuration 클래스가 있다고 가정하자. @ConfigurationProperties(prefix = \"conversion\")public class PropertyConversion { private Duration timeInDefaultUnit; private Duration timeInNano; ...} 그럼, 다음과 같이 바인딩 할 수 있다. conversion.sizeInDefaultUnit=300conversion.sizeInGB=2GBconversion.sizeInTB=4 default 단위는 ms 이며, 만약 이를 바꾸려면 @DurationUnit 을 사용한다 DataSize DataSize 는 파일의 크기를 나타내는 타입이다. 다음과 같은 configuration 클래스가 있다고 가정하자. private DataSize sizeInDefaultUnit; private DataSize sizeInGB; @DataSizeUnit(DataUnit.TERABYTES)private DataSize sizeInTB; 그럼 다윽뫄 같이 바인딩 할 수 있다. conversion.sizeInDefaultUnit=300conversion.sizeInGB=2GBconversion.sizeInTB=4 default 단위는 byte 이며, 이를 바꾸려면 @DataSizeUnit 을 사용한다 Custom Converter 커스텀 변환을 정의할 수도 있다. 다음과 같은 클래스에 public class Employee { private String name; private double salary;} 다음과 같이 바인딩 하려면 conversion.employee=john,2000 다음과 같은 커스텀 Converter 클래스를 정의하면 될것이다. @Component@ConfigurationPropertiesBindingpublic class EmployeeConverter implements Converter&lt;String, Employee&gt; { @Override public Employee convert(String from) { String[] data = from.split(\",\"); return new Employee(data[0], Double.parseDouble(data[1])); }} @ConstructorBinding 스프링 부트 2.2 부터 @ConstructorBinding 을 붙여서 configuration properties 를 바인딩할 수 있다. 이것은 곧 @ConfigurationProperties 이 붙은 클래스가 이제 Immutable 일 수 있다는 것을 의미한다. @ConfigurationProperties(prefix = \"mail.credentials\")@ConstructorBindingpublic class ImmutableCredentials { private final String authMethod; private final String username; private final String password; public ImmutableCredentials(String authMethod, String username, String password) { this.authMethod = authMethod; this.username = username; this.password = password; } public String getAuthMethod() { return authMethod; } public String getUsername() { return username; } public String getPassword() { return password; }} 생성자에는 우리가 비인딩 하고자 하는 프로퍼티를 모두 인자로 가져야 한다. 위의 ImmutableCredentials 클래스는 모든 필드가 final 이기때문에 setter 가 없다. 또한, 생성자로 프로퍼티를 바인딩 하기 위해서는 @EnableConfigurationProperties, 혹은 @ConfigurationPropertiesScan 를 사용하여 명시적으로 configuration 클래스 를 활성화 시켜줘야 한다. 참고: https://www.baeldung.com/configuration-properties-in-spring-boot","link":"/2020/07/07/spring-boot-configuration-properties/"},{"title":"[책 리뷰] 소프트웨어 장인","text":"소프트웨어 장인이라는 책을 읽어보았다. 저자가 경험한 일화를 소개하며 '소프트웨어 장인'은 평소에 어떤 태도를 가지며, 특정 상황에서 어떻게 행동하는 사람인지 정의한다. 우선 책 표지를 보면, 제목 오른쪽에 '프로페셔널리즘', '실용주의', '자부심' 이 세 단어가 적혀있다. 이 세 단어가 책의 내용을 가장 잘 함축할 수 있기 때문이라 짐작한다. 프로페셔널리즘 소프트웨어 장인은 성실하며 정직하다. 고객, 혹은 매니저 등이 불가능해 보이는 일을 부탁하는 경우에는 솔직하게 '아니오'라고 말 할 수 있다. 소프트웨어 장인은 고객을 만족시킬 수 있는 사람이다. 실용주의 실행 관례, 디자인 패턴 등 그 무엇이 되었건, 버그 없는 소프트웨어를 일정 안에 개발하여 고객을 만족시키는 것보다 중요한 것은 없다 자부심 소프트웨어 장인은 자신의 일에 자부심을 가지고 있다. 소프트웨어 분야와 커뮤니티가 성장하는 것에 기여한다. 자신의 지식을 나누고, 배우는 것에 거리낌이 없다. 그 외에도 소프트웨어 개발자로서 도움이 될 만한 소프트 스킬들에 대한 조언을 준다. 저자가 주는 팁들 좋은 팀원을 채용하기 위한 방법 채용 공고 작성법, 면접 방법 커리어를 만들기 위한 활동 블로그, 오픈 소스 활동, 커뮤니티 참여 등 팀원/매니저를 설득하는 방법 애자일, XP, TDD, 페어 프로그래밍 등의 실행 관례 적용을 위한 설득 고객이나 매니저의 요청에 '아니오'라고 말할 땐 항상 대안을 제시해야한다 저자가 강조하는 점들 저자는 책 전반에 걸쳐 XP, TDD, 페어 프로그래밍, 지속적인 통합 등을 계속 강조한다. 페어 프로그래밍 서로를 통해 새로운 지식을 얻을 수 있으며, 실시간으로 피드백을 받기 때문에 좋은 네이밍이 가능하고, 더 좋은 코드를 작성할 수 있게 된다. 주기적으로 짝을 바꾸어 실시하여 효율성을 극대화한다. TDD 테스트를 통과할 만큼의 코드를 작성하게 되어 오버 엔지니어링을 막아준다. 확장성을 고려한답시고 미래를 과도하게 예측하면 코드의 복잡도가 올라간다. 필요할 때 수정하자. 디자인 패턴을 과도하게 적용하면 안된다. 레드-그린-리팩터 사이클을 통해 작은 리팩토링을 자주 적용할 수 있다. 지속적인 통합 항상 동작하는 소프트웨어를 자주 릴리즈하면 빠르게 시장이나 고객의 피드백을 받을 수 있다. TDD 를 한다면 현재의 소프트웨어가 잘 동작하는 소프트웨어라는 것이 검증된다. 기획팀에서도 정확한 요구사항을 모를 때 빠른 배포 주기를 통해 오히려 개발팀이 가이드를 제시할 수도 있다. 기획자의 머리에서 기획이 수정되는 속도와 소스 코드가 수정되는 속도가 같아야한다. 유닛테스트 개발과 테스트는 별개의 업무가 아니다. 테스트까지 끝내야 해당 개발이 완료되는 것이다. 과거에는 디버깅을 잘하면 실력있는 개발자로 보았지만, 시대가 변했다. 테스트 코드를 잘 작성하면 디버깅이 필요없다. 열정과 훈련 소프트웨어 장인은 소프트웨어 개발에 열정을 가지고 있으며, 끊임없는 훈련을 통해 계속 성장한다. 그 외 소프트웨어 개발에서 가장 중요한 것은 결국 고객을 만족시키는 것이다. TDD와 같은 방식을 사용했을 때 시간이 더 오래걸리는 이유는, 이런 방식 자체의 문제가 아니라 새로운 방식에 익숙해지는데 걸리는 시간 때문이다. 이 외에도 많은 조언과 경험담이 있다. 이 책은 소프트웨어 개발자로 살아가면서 피와 살이되는 이야기들이 많은것같다. 그러므로 언젠가는 다시 펼쳐볼 날이 있을것이라 생각한다.","link":"/2020/07/05/software-craftman/"},{"title":"Spring 의 RestTemplate","text":"Spring 의 REST Client 인 RestTemplate 에 대해 알아보자 RestTemplate 을 사용하여 HTTP request 가 가능하며, GET, POST, PUT, DELETE, HEAD 등의 method 를 사용할 수 있다. 본 글에서는 이 중에서도 GET, POST, HEAD 메소드의 사용 예를 알아본다. 또, RestTemplate 에서 Exception Handling 을 하는 방식에 대해서도 알아본다. 기본적인 사용법 GET 메소드 JSON 받아오기 getForEntity 메소드를 사용하여 JSON을 받아올 수 있다. RestTemplate restTemplate = new RestTemplate();String fooResourceUrl = \"http://localhost:8080/spring-rest/foos\";ResponseEntity&lt;String&gt; response = restTemplate.getForEntity(fooResourceUrl + \"/1\", String.class);assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));ObjectMapper mapper = new ObjectMapper();JsonNode root = mapper.readTree(response.getBody());JsonNode name = root.path(\"name\");assertThat(name.asText(), notNullValue()); JsonNode 클래스는 Jackson 이 제공하는 JSON 노드 구조 클래스다. JSON 대신 POJO 받아오기 Response 를 곧바로 DTO 에 매핑할 수 있다. public class Foo implements Serializable { private long id; private String name; // standard getters and setters} 이런 POJO가 있을 때 getForObject 메소드를 쓸 수 있다. Foo foo = restTemplate .getForObject(fooResourceUrl + \"/1\", Foo.class);assertThat(foo.getName(), notNullValue());assertThat(foo.getId(), is(1L)); HEAD 메소드 headForHeaders 메소드로 간단하게 헤더만 가져올 수 있다. HttpHeaders httpHeaders = restTemplate.headForHeaders(fooResourceUrl);assertTrue(httpHeaders.getContentType().includes(MediaType.APPLICATION_JSON)); POST 메소드 postForLocation(), postForObject() 또는 postForEntity() 를 사용할 수 있다. postForLocation() 는 생성된 리소스의 URI 를 반환하는 반면, postForObject() 와 postForEntity() 는 리소스 자체를 반환한다. postForObject() 다음과 같이 사용한다. RestTemplate restTemplate = new RestTemplate(); HttpEntity&lt;Foo&gt; request = new HttpEntity&lt;&gt;(new Foo(\"bar\"));Foo foo = restTemplate.postForObject(fooResourceUrl, request, Foo.class);assertThat(foo, notNullValue());assertThat(foo.getName(), is(\"bar\")); postForLocation() 다음과 같이 사용한다. HttpEntity&lt;Foo&gt; request = new HttpEntity&lt;&gt;(new Foo(\"bar\"));URI location = restTemplate .postForLocation(fooResourceUrl, request);assertThat(location, notNullValue()); exchange() 좀 더 범용적인 exchange() 메소드를 사용할 수도 있다. RestTemplate restTemplate = new RestTemplate();HttpEntity&lt;Foo&gt; request = new HttpEntity&lt;&gt;(new Foo(\"bar\"));ResponseEntity&lt;Foo&gt; response = restTemplate .exchange(fooResourceUrl, HttpMethod.POST, request, Foo.class); assertThat(response.getStatusCode(), is(HttpStatus.CREATED)); Foo foo = response.getBody(); assertThat(foo, notNullValue());assertThat(foo.getName(), is(\"bar\")); 폼 데이터 전송하기 서버가 ‘&amp;’로 연결된 여러개의 ‘키=밸류’ 쌍을 받을 수 있도록 ‘Content-Type’ 을 application/x-www-form-urlencoded 로 변경해줘야한다. HttpHeaders headers = new HttpHeaders();headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED); 그리고 LinkedMultiValueMap 으로 폼 데이터를 감싼다. MultiValueMap&lt;String, String&gt; map= new LinkedMultiValueMap&lt;&gt;();map.add(\"id\", \"1\"); 다음은, HttpEntity 로 request를 빌드한다. HttpEntity&lt;MultiValueMap&lt;String, String&gt;&gt; request = new HttpEntity&lt;&gt;(map, headers); 마지막으로 postForEntity() 를 호출한다. ResponseEntity&lt;String&gt; response = restTemplate.postForEntity( fooResourceUrl+\"/form\", request , String.class); assertThat(response.getStatusCode(), is(HttpStatus.CREATED)); Exception Handling RestTemplate 은 내부적으로 errorHandler 라는 필드를 가지고 있다. 이 errorHandler 라는 녀석이 바로 HTTP 에러가 발생했을 때, 이를 처리하는 로직이 들어있는 객체이며, DefaultResponseErrorHandler 라는 클래스의 인스턴스로 초기화 되어있다. DefaultResponseErrorHandler 는 기본적으로 HTTP 에러가 발생했을 때 다음 3가지 중 하나의 Exception 을 던진다. HttpClientErrorException - 4xx HTTP 상태 코드가 응답 됐을 때 HttpServerErrorException - 5xx HTTP 상태 코드강 응답 됐을 때 UnknownHttpStatusCodeException - 알 수 없는 HTTP 상태 코드가 응답 됐을 때 위의 예외들은 모두 RestClientResponseException의 서브 클래스다. 물론 가장 단순한 방법은 try/catch 문으로 모든 HTTP 메소드 호출부를 감싸는 것이다. 하지만 이 방법은 API 의 종류와 호출부가 많아지면 코드의 품질이 좋지 않아질 것이다. 다른 좋은 방법은 없을까? 다행히도 스프링이 좋은 방법을 제공한다. ResponseErrorHandler 인터페이스 ResponseErrorHandler 를 구현하는 클래스를 작성하여 RestTemplate 의 setErrorHandler() 로 앞서 작성한 에러 핸들러의 인스턴스를 세팅해줄 수 있다. 일반적으로 에러 핸들러에서는 다음 둘 중 한가지 동작을 수행하게 된다. HTTP 응답에 따라서 우리의 앱에 맞는, 의미있는 Exception 을 던져준다. Exception 을 던지지 않고 HTTP 응답을 무시한 채 프로그램이 계속 실행 되도록 한다. 참고로, ResponseErrorHandler 인터페이스를 구현하는 대신 이미 ResponseErrorHandler 를 구현하고 있는 DefaultResponseErrorHandler 클래스를 상속받아도 된다. 이러면 이미 DefaultResponseErrorHandler 에서 hasError() 메소드가 4xx/5xx 상태 코드를 받는 경우 true 를 반환하도록 구현되어 있기 때문에 직접 구현해줄 필요가 없어서 편하다. 다음의 예제 코드를 보자. public class MyErrorHandler implements ResponseErrorHandler { @Override public boolean hasError(ClientHttpResponse httpResponse) throws IOException { return ( httpResponse.getStatusCode().series() == CLIENT_ERROR || httpResponse.getStatusCode().series() == SERVER_ERROR); } @Override public void handleError(ClientHttpResponse httpResponse) throws IOException { if (httpResponse.getStatusCode() .series() == HttpStatus.Series.SERVER_ERROR) { // handle SEVER_ERROR throw new My500ErrorException(); } else if (httpResponse.getStatusCode() .series() == HttpStatus.Series.CLIENT_ERROR) { // handle CLIENT_ERROR if (httpResponse.getStatusCode() == HttpStatus.NOT_FOUND) { throw new NotFoundException(); } else { throw new My400ErrorException(); } } }} 이렇게 작성한 에러 핸들러를 다음과 같이 RestTemplate 에 셋팅해 줄 수 있다. @Servicepublic class BarConsumerService { private RestTemplate restTemplate; @Autowired public BarConsumerService(RestTemplateBuilder restTemplateBuilder) { RestTemplate restTemplate = restTemplateBuilder .errorHandler(new MyErrorHandler()) .build(); } public Bar fetchBarById(String barId) { return restTemplate.getForObject(\"/bars/4242\", Bar.class); }} TestRestTemplate TestRestTemplate 은 RestTemplate 의 유용한 대체제로서 Integration Test 시 유용하다. 내장 서버와 함께 @SpringBootTest 어노테이션을 사용중이라면, Test 클래스 내에서 @Autowired 되어 곧바로 사용할 수 있다. 커스터마이징이 필요하다면 RestTemplateBuilder @Bean 을 사용하면 된다. RestTemplate과 다른점은, TestRestTemplate 은 RestTemplate 을 내장(composition)하며 다음과 같은 추가 기능을 제공한다는 것이다. Authentication 템플릿 생성 시 입력 생성자 인자로 credentials 를 줄 수 있다. TestRestTemplate testRestTemplate = new TestRestTemplate(\"user\", \"passwd\");ResponseEntity&lt;String&gt; response = testRestTemplate. getForEntity(URL_SECURED_BY_AUTHENTICATION, String.class); assertThat(response.getStatusCode(), equalTo(HttpStatus.OK)); 템플릿 생성 후 입력 이미 템플릿을 생성한 후에도 가능하다. TestRestTemplate testRestTemplate = new TestRestTemplate();ResponseEntity&lt;String&gt; response = testRestTemplate.withBasicAuth( \"user\", \"passwd\").getForEntity(URL_SECURED_BY_AUTHENTICATION, String.class); assertThat(response.getStatusCode(), equalTo(HttpStatus.OK)); HttpClientOption TestRestTemplate 내 ENUM 형태로 존재하는 HttpClientOption 을 통해 내부의 HTTP Client 를 커스터마이징할 수 있다. TestRestTemplate testRestTemplate = new TestRestTemplate(\"user\", \"passwd\", TestRestTemplate.HttpClientOption.ENABLE_COOKIES);ResponseEntity&lt;String&gt; response = testRestTemplate. getForEntity(URL_SECURED_BY_AUTHENTICATION, String.class); assertThat(response.getStatusCode(), equalTo(HttpStatus.OK)) 만약 인증이 필요하지 않다면 단순히 이렇게 할 수도 있다. TestRestTemplate(TestRestTemplate.HttpClientOption.ENABLE_COOKIES) RestTemplate 의 Wrapper 몇가지 생성자와 메소드를 통한 추가적인 기능 뿐만 아니라, TestRestTemplate 은 RestTemplate 의 Wrapper 의 역할도 할 수 있다. 이는 레거시 코드로 인해 꼭 RestTemplate 을 써야만 하는 경우에 유용하다. WebClient vs RestTemplate WebClient 는 Spring5 에서 새롭게 등장한 클래스다. Spring3 부터 지원되는 RestTemplate 가 블로킹 방식으로만 동작하는 반면, WebClient 는 블로킹 뿐만 아니라 논블로킹으로도 동작하여 RestTemplate 의 대체제로 사용되고있다. RestTemplate RestTemplate 은 블로킹 방식이며, 내부적으로 Java Servlet API 를 사용하여 하나의 request 당 하나의 스레드를 점유하기 때문에 요청에 대한 응답이 늦어지는 경우 불필요하게 CPU 와 메모리 자원이 낭비될 수가 있다. 또한, 잦은 컨텍스트 스위칭으로 성능에도 좋지않은 영향을 미칠 수도 있고 스레드 풀의 스레드가 고갈될 수도 있다. WebClient 반면, WebClient 는 논블로킹 방식이기 때문에, 응답이 늦어지더라도 큰 문제가 없다. WebClient 는 Spring WebFlux 라이브러리에 포함 되어있다. 내부적으로, WebClient 가 만든 task 를 Reactive framework 가 큐에 넣고, 응답이 왔을 때만 꺼내서 실행한다. Reactive framework 는 Java 9의 Reactive Streams API를 통해 비동기 로직을 조립할 수 있게 해준다. 참고1: https://www.baeldung.com/rest-template 참고2: https://www.baeldung.com/spring-webclient-resttemplate 참고3: https://www.baeldung.com/spring-rest-template-error-handling 참고4: https://www.baeldung.com/spring-boot-testresttemplate","link":"/2020/07/06/rest-template/"},{"title":"Boyer-Moore 과반수 투표 알고리즘","text":"Boyer-Moore 과반수 투표 알고리즘(majority vote algorithm)[1]은 배열에 포함된 원소들 중 절반 이상 포함된 원소를 linear time 과 constant space 로 찾을 수 있는 알고리즘이다. 참고로 Boyer–Moore 문자열 검색 알고리즘(string-search algorithm)과는 다르므로 헷갈리지 말자. 보이어 무어 과반수 투표 알고리즘은 스트리밍 알고리즘(streaming algorithm)의 대표적인 예다. 만약 배열 내 과반수(절반이 넘는 수)에 해당하는 원소가 존재한다는 보장이 된다면, 결과값은 항상 과반수 원소가 된다. 주의할 점은, 만약 배열 내에 과반수 만큼 등장하는 원소가 없다면 결과값으로 임의의 의미없는 값이 나오게 된다(딱 절반 만큼 등장하는 원소가 있더라도 마찬가지다) 즉, 과반수 만큼 등장하는 원소가 있다는 보장이 없다면, 결과값이 항상 과반수에 해당하는 원소라는 보장도 없다는 것이다. 동작 방식 major 와 count 를 0 으로 초기화 한다. 배열의 각각의 원소 x에 대해 if count = 0, major = x, count = 1 대입 else if major = x, count = count + 1 else, count = count - 1 major 를 반환 만약 딱 절반 만큼 존재하는 원소 위의 로직을 그림으로 나타내면 다음과 같다. X축에 있는 도형이 배열에 있는 원소이고, Y축에 있는 숫자가 count 값이며, 꺾은선 그래프 위에 있는 도형이 현재 major 변수에 들어있는 원소다. 증명 우선, 과반수 원소가 있을 때 위 로직대로 수행하면 결과값이 항상 과반수 원소가 된다는 것을 증명해보겠다. 뭔가 직관적으로 당연한것 같으면서도 애매한 분들을 위해 쉬운 비유를 하나 들어보겠다. 하나의 성을 차지하기 위해 여러 나라가 전투를 벌인다. 여러 나라에서 온 병사들은 일렬로 줄을 서 있고, 한 명씩 차례대로 성을 향해서 돌진한다. 만약 성에 아무도 없다면 바로 성을 함락하여 성의 주인이 된다. 성에 누군가가 있다면 2가지 케이스로 나뉜다. 성에 있는 사람이 나와 같은 나라 사람이면 성에 합류하고, 다른 나라 사람이면 싸워서 성에 있는 병사 한 명과 동반 자살(?)한다. 즉, 성에 있는 사람 수를 1만큼 감소시키는 동시에 자기 자신 또한 사라진다. 어떤 나라가 성을 함락하는 순간 다른 모든 나라는 같은 팀이나 다름없다. 왜냐하면 모두 성을 향해서만 공격하기 때문에 성을 함락한 나라의 병사 숫자가 집중적으로 감소하기 때문이다. 모든 병사가 돌진한 뒤에도 성에 병사가 남아있다면 그 병사의 나라가 최종적으로 성의 주인이 된다. 만약 성에 아무도 남지 않는다면 마지막으로 성을 소유했던 나라가 성의 주인이 된다. 결국 위의 동작 방식에서 얘기한 major 변수가 성이고, count 변수가 성에 있는 병사의 수이며, 배열의 각 원소는 병사, 원소의 값 x는 각 병사가 속한 나라이다. 그리고 결과값이 나타내는 것은 최종적으로 성의 주인이 되는 나라이다. 만약 병사의 숫자가 과반수에 해당하는 나라가 있다면 이 나라에게 최악의 경우는 처음부터 성을 함락하여 다른 모든 나라가 동맹을 맺어 성을 향해 돌진하게 되는 상황이다. 하지만 어차피 과반수 나라는 적어도 최후의 1인은 살아남아 성의 주인으로 남을 것이다. 만약 다른 나라가 성을 함락한 적이 있다면, 오히려 이득이다. 과반수가 아닌 다른 나라끼리 싸워서 자신들의 병사들을 비축시킬 수도 있기 때문이다. 그렇기 때문에 과반에 해당하는 원소가 있을 경우, 앞서 언급한 로직대로 수행하면 무조건 결과값으로 세팅되는 것이다. 하지만 과반에 해당하는 나라가 없다면 어떤 나라가 먼저 성을 함락하느냐에 따라 성의 최종 주인이 달라지게 되므로, 앞선 로직의 결과값은 의미가 없다. 다음으로, 과반수 원소가 없을 때 정확히 절반 만큼 존재하는 원소가 있으면 결과값이 해당 원소가 될 듯한 느낌도 드는데, 아니라는 것을 반례를 들어 증명하겠다. 만약 주어진 배열이 [1, 1, 1, 2, 3, 3] 이라면 결과값이 1이 되지만, 같은 구성에 순서만 바꾼 [2, 1, 1, 3, 3, 1] 이라면 결과값이 3이 된다. 소스 코드 알고리즘의 로직 자체가 단순하기 때문에 소스코드 또한 굉장히 간단하다. 다음은 C++ 로 보이어 무어 과반수 투표 알고리즘을 구현한 코드이다. int findMajority(vector&lt;int&gt;&amp; arr) { int count = 0; int major = 0; for (int num : arr) { if (count == 0) major = num; if (major == num) count++; else count--; } return major;} 예제 LeetCode 1157. Online Majority Element In Subarray 풀이 https://en.wikipedia.org/wiki/Boyer–Moore_majority_vote_algorithm ↩︎","link":"/2020/11/30/boyer-moore-majority-vote-algorithm/"},{"title":"Spring 애플리케이션 시작 시 실행되는 로직 작성하기","text":"본 글에서는 Spring 애플리케이션 시작 도중, 혹은 시작 직후에 특정 로직이 실행되도록 하기 위한 다양한 방법을 알아본다. 우선 용어부터 정확히 하겠다. 애플리케이션 시작 도중이라 함은 특정 빈이 초기화된 직후이며, 시작 직후라 함은 모든 빈이 초기화된 직후라고 정의하겠다. 또한, 빈이 초기화 되었다 함은 빈에 등록된 의존성들이 모두 주입되었다는 뜻으로 정의하겠다. IoC 의 특성상 우리는 애플리케이션 실행 흐름의 제어권을 어느정도 포기해야하기 때문에, Spring 에서 초기 셋업 로직을 작성하는 것은 아무래도 조금 귀찮기 마련이다. 단순히 Bean 의 생성자에 초기화 로직을 넣기만 해서는 안될 수도 있다. 다음의 예를 보자. @Componentpublic class InvalidInitExampleBean { @Autowired private Environment env; public InvalidInitExampleBean() { env.getActiveProfiles(); }} InvalidInitExampleBean 객체가 생성되는 시점에는 아직 env 가 초기화되지 않은 상태이므로 NullPointerException이 발생하게 된다. 그럼 어떤 방법으로 초기 셋업 로직을 작성해야 할까? 초기 셋업 로직을 정의하는 방법 본 글에서는 초기 셋업 로직을 작성하는 8가지 방법에 대해 알아본다. @PostConstruct 어노테이션 @PostConstruct 어노테이션은 특정 클래스의 메소드에 붙여서 해당 클래스의 객체 내 모든 의존성(Bean) 들이 초기화 된 직후에 딱 한 번만 실행되도록 해준다. 만약 객체에 의존성이 하나도 없더라도 실행된다. 다음의 코드를 보자. @Componentpublic class PostConstructExampleBean { private static final Logger LOG = Logger.getLogger(PostConstructExampleBean.class); @Autowired private Environment environment; @PostConstruct public void init() { LOG.info(Arrays.asList(environment.getDefaultProfiles())); }} 앞서 NullPointerException 이 발생했던 코드와는 달리 init() 메소드는 클래스 내 의존성인 environment 가 초기화된 직후에 호출되기 때문에 정상적으로 동작하게된다. InitializingBean 인터페이스 이 방식은 어노테이션을 붙이는 대신 InitializingBean 인터페이스와 afterPropertiesSet() 메소드를 구현한다는 것말곤 앞서 설명한 @PostConstruct 방식과 유사하게 동작한다. @Componentpublic class InitializingBeanExampleBean implements InitializingBean { private static final Logger LOG = Logger.getLogger(InitializingBeanExampleBean.class); @Autowired private Environment environment; @Override public void afterPropertiesSet() throws Exception { LOG.info(Arrays.asList(environment.getDefaultProfiles())); }} ApplicationListener 이 방식은 앞서 설명한 방식처럼 특정 Bean 과 관련된 것이 아니라 Spring 컨텍스트의 초기화가 완료된 후, 즉 모든 Bean 의 초기화가 완료된 후에 실행되도록 하는 방식이다. ApplicationListener 인터페이스를 구현하는 Bean 을 정의하고 onApplicationEvent() 메소드를 Override 하여, 그 안에 원하는 로직을 작성하면 된다. @Componentpublic class StartupApplicationListenerExample implements ApplicationListener&lt;ContextRefreshedEvent&gt; { private static final Logger LOG = Logger.getLogger(StartupApplicationListenerExample.class); public static int counter; @Override public void onApplicationEvent(ContextRefreshedEvent event) { LOG.info(\"Increment counter\"); counter++; }} 필요에 따라 ContextRefreshedEvent 대신 상황에 맞게 다른 이벤트를 넣어줄 수도 있다. @EventListener 어노테이션 앞서 설명한 ApplicationListener 처럼 Bean 을 하나 정의하고, 인터페이스를 구현하는 것이 아니라 특정 메소드에 @EventListener 어노테이션을 붙여 그 안에 원하는 로직을 작성한다. @Componentpublic class EventListenerExampleBean { private static final Logger LOG = Logger.getLogger(EventListenerExampleBean.class); public static int counter; @EventListener public void onApplicationEvent(ContextRefreshedEvent event) { LOG.info(\"Increment counter\"); counter++; }} @Bean 의 initMethod 속성 @Bean 어노테이션의 initMethod 속성으로 이 Bean 의 초기화가 완료(의존성이 모두 주입)된 뒤에 실행 되어야할 Bean 내 메소드의 이름을 지정할 수가 있다. 예를 들어 다음과 같은 Bean 이 있다고 가정하자. public class InitMethodExampleBean { private static final Logger LOG = Logger.getLogger(InitMethodExampleBean.class); @Autowired private Environment environment; public void init() { LOG.info(Arrays.asList(environment.getDefaultProfiles())); }} 이 때, @Bean 메소드에 initMethod 속성으로 InitMethodExampleBean Bean 내의 메소드인 init() 의 이름을 설정해준다. @Bean(initMethod=\"init\")public InitMethodExampleBean exBean() { return new InitMethodExampleBean();} XML 로도 가능하다. &lt;bean id=\"initMethodExampleBean\" class=\"com.baeldung.startup.InitMethodExampleBean\" init-method=\"init\"&gt;&lt;/bean&gt; 생성자 Injection 사실 생성자를 통해 의존성을 주입하는 경우에는 그냥 생성자에 원하는 로직을 넣으면 된다. Field injection 의 경우 Bean 객체를 생성한 뒤에 나중에 의존성을 주입하기 때문에, 주입하기 이전에는 null 인 상태지만 Constructor Injection 의 경우에는 생성과 동시에 주입을 하기 때문에 문제가 없는것이다. 이것이 Field Injection 보다 Constructor Injection 이 더 권장되는 여러가지 이유 중 하나다. 생성자 주입이 더 권장되는 여러가지 이유에 대해서는 추후 다른 포스팅에서 다룰 것이다. @Component public class LogicInConstructorExampleBean { private static final Logger LOG = Logger.getLogger(LogicInConstructorExampleBean.class); private final Environment environment; @Autowired public LogicInConstructorExampleBean(Environment environment) { this.environment = environment; LOG.info(Arrays.asList(environment.getDefaultProfiles())); }} Spring Boot 의 CommandLineRunner 스프링 부트는 run() 이라는 콜백 메소드를 가진 CommandLineRunner 라는 인터페이스를 제공한다. run() 메소드는 Spring application context 의 초기화가 완료된(모든 Bean 이 초기화된) 후에 실행되므로 이 안에 원하는 로직을 작성하면 된다. @Componentpublic class CommandLineAppStartupRunner implements CommandLineRunner { private static final Logger LOG = LoggerFactory.getLogger(CommandLineAppStartupRunner.class); public static int counter; @Override public void run(String...args) throws Exception { LOG.info(\"Increment counter\"); counter++; }} 참고로 CommandLineRunner Bean 은 같은 애플리케이션 컨텍스트 내에 여러개를 정의할 수 있으며, Ordered 인터페이스, 혹은 @Order 어노테이션으로 실행 순서를 정해줄 수도 있다. @Component 가 아니라 다음과 같이 @Configuration 과 @Bean 을 사용한 방식으로도 정의할 수 있다. @SpringBootApplicationpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } @Bean public CommandLineRunner run(UserRepository userRepository) throws Exception { return (String[] args) -&gt; { User user1 = new User(\"John\", \"john@domain.com\"); User user2 = new User(\"Julie\", \"julie@domain.com\"); userRepository.save(user1); userRepository.save(user2); userRepository.findAll().forEach(user -&gt; System.out.println(user)); }; }} 자바에서 하나의 메소드를 가지는 인터페이스의 경우에는 기존의 익명 클래스 방식대신 위와같이 람다식으로 작성할 수 있다. Spring Boot 의 ApplicationRunner 스프링 부트는 앞서 언급한 CommandLineRunner 인터페이스 외에 ApplicationRunner 인터페이스도 제공한다. 동일하게 run() 이라는 콜백 메소드를 가지고 있어 이 안에 원하는 로직을 작성하면 된다. 참고로 run() 메소드로 들어오는 문자열들은 커맨드 라인으로 앱을 실행할 때 들어온 명령행 인자들이다. @Componentpublic class AppStartupRunner implements ApplicationRunner { private static final Logger LOG = LoggerFactory.getLogger(AppStartupRunner.class); public static int counter; @Override public void run(ApplicationArguments args) throws Exception { LOG.info(\"Application started with option names : {}\", args.getOptionNames()); LOG.info(\"Increment counter\"); counter++; }} CommandLineRunner 와 차이가 있다면, run() 메소드의 인자가 String 가 아니라 ApplicationArguments 인데 ApplicationArguments 인터페이스는 보통의 커맨드라인 인자 뿐만 아니라, 옵션 읽어 들일 수 있는 getOptionNames(), getOptionValues() 등의 메소드도 가지고 있다. 결론 앞서 설명한 다양한 방식을 한꺼번에 적용해보면 다음과같은 모양이다. @Component@Scope(value = \"prototype\")public class AllStrategiesExampleBean implements InitializingBean { private static final Logger LOG = Logger.getLogger(AllStrategiesExampleBean.class); public AllStrategiesExampleBean() { LOG.info(\"Constructor\"); } @Override public void afterPropertiesSet() throws Exception { LOG.info(\"InitializingBean\"); } @PostConstruct public void postConstruct() { LOG.info(\"PostConstruct\"); } public void init() { LOG.info(\"init-method\"); }} 해당 빈이 초기화된 후에 다음과 같은 로그가 찍히게 된다. [main] INFO o.b.startup.AllStrategiesExampleBean - Constructor[main] INFO o.b.startup.AllStrategiesExampleBean - PostConstruct[main] INFO o.b.startup.AllStrategiesExampleBean - InitializingBean[main] INFO o.b.startup.AllStrategiesExampleBean - init-method 참고: https://www.baeldung.com/running-setup-logic-on-startup-in-spring#4-the-bean-initmethod-attribute","link":"/2020/07/09/spring-running-startup-logic/"},{"title":"DDD 시작하기","text":"본 글에서는 도메인 주도 설계(Domain Driven Design)의 기본 개념에 대해서 알아본다. 도메인 주도 설계(이하 DDD)에 대해 알기 위해서는 우선 기본적인 용어에 대한 정리가 필요하다. 도메인과 도메인 모델 도메인(Domain) 이란 우리가 소프트웨어로 해결하고자 하는 대상이다. 예를 들어 화상 채팅 서비스를 구현하고자 하는 경우에는 화상 채팅이 도메인이 된다. 도메인은 하위 도메인으로 나뉠 수도 있다. 예를 들어, 화상 채팅 도메인은 화면 공유, 텍스트 채팅, 친구 목록 등의 하위 도메인으로 나뉜다. 도메인 모델(Domain Model) 은 여러 정의가 있지만 우선 기본적으로 도메인을 개념적으로 표현한 것을 도메인 모델이라고 한다. 도메인을 개념적으로 표현하는 방법은 다양하기 때문에, 예를 들어 클래스 다이어그램이나 상태 다이어그램, 또는 그래프나 수학 공식이 모두 도메인 모델이 될 수 있다. 또한, 도메인 모델은 도메인 모델 패턴(Domain Model Pattern) 을 의미하기도 하는데, 도메인 모델 패턴은 아래와 같은 아키텍처 구조에서 도메인 계층을 객체 지향 기법으로 구현한 것을 의미한다. 여기서 도메인 모델은 인프라스트럭쳐 계층(DB, OS 등)과 프레젠테이션 계층(HTTP, HTML 등)에 종속성 없이 순수하게 비즈니스 요구사항을 담는다. 본 글에서는 객체 지향 기법을 사용하여 도메인 모델링을 설명하기 때문에, 도메인 모델의 의미는 두번째 의미에 좀 더 가까우며, 도메인 모델은 클래스나 인터페이스 형태라고 가정한다. 하지만 두 가지 모두가 본질적으로 갖는 공통점은, 도메인 모델이 도메인 규칙(비즈니스 룰, 요구사항)을 담고 있다는 것이다. 예를 들어, 자리 비움 상태인 유저에게는 화상 채팅 요청을 할 수 없다는 조건을 표현할 수 있다. 도메인을 모델링하기 위해서는 우선 핵심 구성요소, 규칙, 기능을 정의해야 하며, 도메인 모델은 이를 잘 담고 있어야한다. 코드가 도메인을 잘 표현해야 가독성이 높아지고, 코드 자체로 문서의 역할을 할 수가 있다. 이를 방해하는 하나의 예시는, 도메인 모델에 습관적으로 public 인 set/get 메서드를 추가하는 것이다. 일부 도메인 규칙을 위해서는 public set/get 메서드가 있으면 안되는 경우가 있기 때문에 잘 생각하여 필요한 경우에만 추가해야한다. 물론 불변 타입을 사용하면 set 메서드는 자연스레 사라질 것이다. 엔티티와 밸류 엔티티(Entity) 와 밸류(Value) 는 대표적인 도메인 모델이다. 둘의 가장 큰 차이는 식별자의 유무이다. 각 엔티티는 고유한 식별자를 가져서 이를 통해 엔티티 간의 구분이 가능하다. 예를 들어, 화상 채팅 앱에서 id 라는 고유한 값을 두어서 각 User 를 구분할 수 있다. 반면, 밸류는 엔티티에 속하는 일종의 데이터의 묶음이며 식별자가 없다. 예를 들어, firstName, middleName, lastName 을 묶어서 Name 이라는 하나의 밸류 타입을 만들 수 있다. 동명 이인이 있을 수 있다는 사실만 보아도 Name 이라는 도메인 모델은 엔티티가 아닌 밸류라는 것을 알 수 있다. 주의할 점은, 밸류도 DB에 저장되면 테이블의 PK가 있을 텐데 식별자가 있는 것 아닌가 하는 생각이 들 수도 있다. 하지만 엔티티에서 얘기했던 식별자는 이것을 의미하는 것은 아니다. PK 는 단순히 관계형 데이터베이스에 저장하기 위해 필요한 개념이지, 도메인 모델간의 구분에 사용되는 것은 아니기 때문이다. 그런데 밸류 타입이 꼭 두 개 이상의 데이터를 가져야하는 것은 아니다. 예를 들어, 돈의 액수를 나타내기 위해 단순히 int 타입을 사용할 수도 있지만 의미를 확실히 하고 돈과 관련된 연산을 내재하기 위해 int 타입의 변수를 하나 가진, Money 라는 밸류 타입을 만들 수도 있다. 이럴 경우, 코드에서 도메인 규칙이 더 잘 드러나기 때문에 코드의 가독성이 높아진다. 물론, Money 라는 밸류 타입을 만들지 않고 변수명을 money 로 할 수도 있겠지만, 밸류 타입을 만들면 변수 네이밍을 더 자유롭게 할 수 있고, 도메인 로직을 메소드 형태로 넣을 수 있다는 것과 같은 추가적인 장점이 있다. 밸류 타입은 Immutable 로 구현하는 것을 선호한다. 즉, 내부 상태를 변경하는 것보단 변경된 상태를 갖는 새로운 객체를 생성하는 것이다. 그 이유는 참조 투명성과 Thread Safety 를 갖게 되어 안전한 코드를 작성할 수 있기 때문이다. 도메인 용어 도메인 용어는 말 그대로 도메인 내에서 사용하는 용어이다. 예를 들어, 도메인 규칙에 사용자가 '자리 비움' 이나 '바쁨' 상태일 때는 통화를 걸 수 없으며, '한가함' 항태일 때는 통화를 걸 수 있다는 내용이 있다고 가정하자. 그렇다면 '자리 비움', '바쁨', '한가함' 은 모두 도메인 용어이다. 이 코드에는 이러한 도메인 용어가 잘 드러나도록 작성되어야 한다. 즉, 'STATE1', 'STATE2', 'STATE3' 과 같은 네이밍이 아닌, 'ABSENT', 'BUSY', 'FREE' 와 같은 네이밍을 사용해야 도메인이 잘 표현되었다고 할 수 있으며, 유지보수성과 가독성이 높아진다. 애그리거트 애그리거트(Aggregate)는 서로 관련이 있는 도메인 모델들의 집합이다. 도메인이 커지면 엔티티와 밸류의 수도 점점 많아지게 된다. 이 때 애그리거트 단위로 캡슐화를 하면 내부 구현을 숨기고 도메인 모델을 더 상위 수준에서 바라볼 수 있게 되어 관리나 변경도 용이해지며, 도메인 모델간의 관계를 좀 더 쉽게 파악할 수 있게 된다. 예를 들어, 인스타그램에서 타임라인과 관련된 기능을 하나의 애그리거트로 묶고, 채팅과 관련된 기능을 하나의 애그리거트로 묶을 수가 있을 것이다. 같은 애그리거트에 속하는 모델들은 유사하거나 동일한 라이프사이클을 가진다. 함께 생성되는 구성 요소들은 같은 애그리거트에 속할 가능성이 높다. 예를 들어, 주문, 주문할 상품 정보(상품의 종류와 개수), 수령인 정보, 배송지 정보 등은 주문이 들어올 때 함께 생성되므로 같은 애그리거트에 속한다. 또한, 함께 변경되는 빈도가 높은 구성요소들도 같은 애그리거트에 속할 가능성이 높다. 예를 들어, 주문하는 상품의 개수가 변경되면 주문의 총 액수를 다시 계산해야하며, 배송지 정보와 주문할 상품의 개수, 수령인 등을 동시에 변경하기도 한다. 또한, 같은 애그리거트에 속하는 모델들은 변경의 주체가 같다. 예를 들어, 상품의 상세 정보와 상품의 리뷰를 생각해보자. 상품의 상세 정보 페이지가 리뷰를 포함하기 때문에 둘이 같은 애그리거트에 속한다고 착각하기 쉬운데, 상품의 상세정보는 판매자가 수정하지만, 리뷰는 고객이 수정하므로 변경의 주체가 다르며, 생성이나 변경이 함께 이루어지지도 않기 때문에 둘은 다른 애그리거트에 속한다. 참고로, 하나의 애그리거트는 하나의 엔티티 객체만 포함하는 경우가 많으며, 둘 이상의 엔티티로 이루어진 경우는 드물다. (밸류는 여러개가 있을 수 있다.) 애그리거트는 DDD 에서 굉장히 중요한 개념이기 때문에 자세한 설명은 별도의 포스트인 DDD 의 Aggregate에서 다루도록 하겠다. 리포지터리 리포지터리(Repository)는 도메인 모델의 영속성(Persistency)를 위해 필요한 도메인 모델이다. 엔티티와 밸류가 요구사항에서 도출되는 도메인 모델이라면, 리포지터리는 구현을 위한 도메인 모델이다. 리포지터리도 도메인 모델이기 때문에 엔티티와 밸류와 함꼐 도메인 계층에 포함된다. 덧붙이자면, 리포지터리는 특정 기술에 종속되지 않고 순수하게 도메인 규칙을 정의하기 때문에 보통 인터페이스 형태로 존재하는데 실제로 도메인 모델을 저장하기 위해서는 리포지터리를 구현하는 구체적인 구현체가 필요하며, 이는 인프라스트럭처 계층에 속한다. 리포지터리는 애그리거트 단위로 도메인 모들을 저장하거나, 애그리거트 루트의 식별자로 애그리거트를 조회하는 역할을 한다. 도메인 서비스 여러 애그리거트가 사용되어 하나의 애그리거트에 넣기는 애매한 로직을 가지는 개념이다. 여러 애그리거트가 관여하는 로직을 한 애그리거트에 넣게되면, 그 애그리거트는 역할이 늘어나고 응집도가 떨어지게 된다. 이 때 도메인 서비스라는 별도의 클래스를 만들어 이러한 로직을 가지는 메서드를 정의할 수 있다. 도메인 서비스는 응용 서비스에서 사용될 수도 있고, 애그리거트에서 사용될 수도 있는데, 애그리거트의 메소드 인자로 도메인 서비스와 다른 애그리거트를 전달하는 경우는 애그리거트가 도메인 서비스를 사용하는 주체가 된다. 이 때, 응용 서비스는 도메인 서비스를 애그리거트에게 전달해줄 책임이 있다. 도메인 서비스의 메서드 인자로 여러 애그리거트를 넣어 로직을 수행하는 경우엔 응용 서비스가 도메인 서비스의 사용 주체가 된다. 도메인 서비스를 응용 서비스와 헷갈릴 수 있는데, 둘은 엄연히 다르다. 도메인 서비스는 도메인 계층에 있으며 도메인 로직을 다루는 반면, 응용 서비스는 응용 계층에 있으며 응용 로직을 다룬다. 또한, 응용 서비스는 다른 도메인 모델들을 이용하여 애플리케이션의 기능을 구현한다.","link":"/2020/08/09/ddd-basic/"},{"title":"[책 리뷰] 테스트 주도 개발","text":"본 포스트에서는 켄트 벡(Kent Beck)의 테스트 주도 개발(Test-Driven Development) 에 대해 리뷰한다. TL;DR; 3개의 파트로 이루어져 있음 파트1, 2는 각각 서로 다른 예제를 TDD 를 사용하여 바닥부터 개발하면서 TDD 가 어떤식으로 진행되는지 체험 시켜주고, TDD 에서 사용할 수 있는 기술들과 핵심 개념에 대해서 설명해 줌 파트3은 코드 보다는 TDD 와 관련하여 전반적인 개념 설명을 하고 팁을 알려 줌 각 파트를 개인적으로 도움이 된 정도로 비교해보자면 1 &gt;&gt; 3 &gt;&gt; 2 그러므로 시간이 없다면 파트1만 이라도 읽어보길 권하고 싶음(TDD가 뭔지 제대로 느낄 수 있게 해 줌) 파트1은 코드를 따라치는게 중요하다고 생각 TDD 에 대한 전반적인 개념을 알게된다고 해도 이 방법론을 실제 개발에 사용하는 것은 다른 이야기 실제로 코드를 작성하며 TDD 장점을 몸소 체득하지 않으면 왜 이것을 사용해야되는지 와 닿지 않을 것이고, 그럼 결과적으로 사용하지 않게 될 가능성이 크기 때문 파트1 파트1은 TDD 를 사용하여 다중 통화 시스템을 구현한다. 여기서 TDD 의 가장 핵심적인 내용을 거의 다 다룬다. 다음은 기억이 나는 내용들에 대해 정리해봤다. 내가 가장 중요하다고 생각이 되는 부분을 생각나는 대로 나열했기 때문에 조금 두서가 없을 수도 있다. 일반적인 소프트웨어 개발에서는 보통 먼저 전체적으로 윤곽을 설계하고나서 세부적인 구현을 하는 탑다운 방식을 사용하게 된다. 하지만 TDD 는 일반적으로 이와 반대로[1] 전체적인 설계를 전혀 하지 않은 채, 그때 그때 필요한 기능들을 할일 목록에 적어가면서 지금 당장 필요한 구체적인 구현을 한다. 그럼에도 불구하고 우리의 소프트웨어는 저절로 좋은 설계를 향해 가는데, 그 이유는 TDD 의 개발 사이클 내에서 중간 중간 중복을 없애는 리팩토링의 과정을 거치기 때문이다. 참고로, 저자는 파트1의 예제를 TDD 로 여러번 개발해 봤지만 최종 결과물이 매번 다른 설계를 갖게 되었다고 한다. 즉, 구현 이전에 설계가 선행되지 않기 때문에 최종적으로 어떤 설계가 나올지 알 수가 없는 것이다. 소프트웨어 개발은 전통적으로 건물을 짓는 행위에 비유되곤 한다. 하지만 실제로는 건물을 짓는 행위와는 엄연히 다르다. 건물은 처음 설계된대로 지어지고 나면 잘 바뀌지 않는다. 그렇기 때문에 처음 설계를 하고, 그대로 지으면 웬만해선 그냥 끝이다. 하지만 소프트웨어는 성격이 조금 다르다. 예를 들어, 개발 도중이나 이후에 요구사항이 바뀌는 경우가 많으며, 개발이 완료된 이후에 발견되는 버그들이 많다. 즉, 처음에 설계한대로 깔끔하게 개발이되고 끝나는 경우는 드물다. 그렇기 때문에 건물을 짓는 것보다는 정원을 가꾸는 것에 비유하는것이 좀 더 정확하다. TDD 로 개발을 할 때는 테스트를 작성하기 전에 절대로 기능 코드를 작성하지 않으며, 이미 널리 알려진 것처럼 Red-Green-Refactor 이렇게 3가지 단계로 이루어진 사이클을 반복하게 된다. 여기서 중요한 점 몇가지에 대해 말해보겠다. 우선, Red 에서 Green 으로 갈 때는 가능하면 가장 빨리 가야한다. 우선 컴파일조차 되지 않는 상황이 있을 것이다. 예를 들어, 특정 타입의 객체를 반환하는 특정 이름의 메서드가 존재하지 않는다는 컴파일 에러가 발생하면 해당 이름을 가진 null 을 반환하는 메서드를 생성하여 일단 컴파일이 되게 한다. 이제는 컴파일은 되지만 NullPointerException 이 발생할 것이다. 이번엔 테스트에서 원하는 바로 그 객체를 반환하도록 변경한다. 이것을 가짜 구현이라고 한다. 가짜 구현을 통해 가능한한 빨리 Green(초록 막대라고도 부른다)을 본다. 이것이 핵심이다. 일단 초록 막대를 보고 난 후에 리팩토링 단계에서 명확한 코드를 작성하거나 삼각측량법 등을 사용하여 가짜 구현을 일반화한다. 만약 확신이 있다면 즉시 명확한 코드를 원하는 만큼 작성해도 된다. 하지만 그렇게 했더니 테스트가 실패한다면? 다시 작성한 코드를 지우고 전 단계로 돌아가 더 적은 기능을 구현해 보거나 삼각측량법을 사용하던 한다. 만약 중간 중간에 새로 개발해야하거나 수정해야하는 부분이 생각나면 일단 할 일 목록(To-Do List)에 적어놓기만 한다. 이러한 TDD 의 개발 방식은 다음과같은 장점이 있다. 개발자가 자유자재로 개발의 보폭을 조절할 수 있다. 현재 프로젝트 내에서 자신이 위치한 곳을 정확히 인지할 수 있다. 매 단계 마다 지금 해야할 일을 정확히 알고 그것에만 온전히 집중할 수 있다. 지금까지 작성한 코드에 대한 확신을 가지게 된다. 그렇기 때문에 오히려 전체적인 개발 속도를 향상시키는 결과를 가져온다. TDD 의 라이프사이클 특성상 개발 도중 반복적으로 리팩토링을 수행하기 때문에 개발 도중 어떠한 순간에도 코드의 품질은 좋은 상태로 유지된다. 내가 다음에 구현할 기능에 대한 테스트만 작성하며, 테스트를 통과하기 위한 만큼의 코드만 작성하기 때문에 오버 엔지니어링의 가능성이 줄어들게 된다. (파트3 의 내용) 꼭 필요한 기능만 구현하기 때문에 지금 당장은 미래에 발생할지도 모르는 변화에 대해 유연한 코드는 아닐 수 있다. 이를테면 일시적으로 OCP 를 위반하게 될 수도 있으나, 이미 작성해 둔 많은 테스트가 서포트 해주기 때문에 미래에 변화가 발생하는 그 시점에 코드를 변경하더라도 비용이 그리 크지 않게 된다. 파트2 파트1에서와 비슷하게 TDD 를 사용하여 또다른 앱인 xUnit 을 직접 작성하는 예제를 담았다. 특별히 기억에 남는 내용은 없고, 파트1에서 배운 TDD 개발 과정을 다른 예제와 언어를 통해 다시 한 번 연습하기위한 파트라고 생각이 든다. 이 파트에서는 스스로 뇌 수술을 하는 것에 비유하기도 하는, 자기참조 프로그래밍이라는 것을 하는데(테스트 케이스를 작성하기 위해 사용할 프레임워크를 테스트하기 위한 테스트 케이스를 작성해야 하기 때문..) 굉장히 신기한 작업이라 중간 중간 이해를 위해 생각해야하는 시간이 조금 있었다. 여담으로, 저자는 새로운 언어를 학습할 때 TDD 로 xUnit 을 작성해본다고 한다. xUnit 을 작성하기 위해 사용되는 여러 문법과 함수 등을 익히다보면 그 언어의 핵심 기능들을 대부분 익힐 수 있게 되기 때문이라고. 파트3 실무에서 유용하게 사용할 수 있는 테스트의 종류들, 실제 개발을 할 때 마주칠 수 있는 다양한 경우에 대한 저자의 견해, 여러 디자인 패턴들을 TDD 에서 어떤 식으로 활용할 수 있는지 등에 대한 이야기를 한다. 파트3의 마지막에는 자문자답을 통해 TDD 에 대해 깊이 알아본다. 다음은 내가 가장 기억에 남는 내용이다. 훌륭한 엔지니어링이 프로젝트의 성공의 필수적인 부분이 아니기 때문에 적당한 수준의 엔지니어링 만으로도 프로젝트를 성공적으로 이끌 수도 있다는 점에서 TDD 는 오버액션이라고 볼 수도 있다. 그렇기 때문에 TDD 가 항상 가장 좋은 방법은 아니다. 업계에서 통용하는 수준보다 훨씬 더 적은 결함과 훨씬 더 깨끗한 설계의 코드를 작성하게 해주기 때문이다. 보통 프로젝트가 진행됨에 따라 코드의 품질은 떨어지게 되고, 다른 프로젝트를 하고싶어 지는데, TDD 는 프로젝트를 처음 시작할 때의 흥미와 설렘을 유지할 수 있게 해준다. 개인적으로 파트1만 읽고 실제 개발에 TDD 를 적용해보면서 들었던 고민이 일부 해소가 되기도 했고, 읽어도 잘 이해가 잘 가지 않은 부분도 있었던 파트였다. 마지막 쯤에 있는 마틴 파울러의 추천사가 TDD 의 장점을 굉장히 잘 정리한것 같다. TDD 는 여러가지 고민을 한 번에 하는 것이 아니라 한 번에 한 가지 기능에만 집중할 수 있게 해준다. 그리고 그 기능에 있어서도, 기능을 구현하거나 리팩토링을 하거나, 둘 중 하나에만 집중할수 있게 해준다. 느낀점 실무에서 TDD 로 코드를 작성해보니 잦은 리팩토링과 테스트 코드의 실행을 통해 나중에 꽤나 힘겹게 발견했을 구현 실수를 조기에 발견할 수 있게되는 경우가 꽤 많았다. 그래서 테스트를 작성하는데 시간이 든 만큼 앞선 경우로 인해 절감된 시간도 있기 때문에 단기적인 개발 시간만 보더라도 그렇게 손해보는 장사는 아니라고 생각했다. 게다가 사실 테스트는 TDD 와는 별개로 당연히 작성해야되는 것이기 때문에 대부분의 경우 TDD 로 개발하는 것이 이득이라고 볼 수도 있을것 같다. 또한, 어떻게보면 내가 추가하고자 하는 모든 기능에 대해 강제로 테스트를 작성하게 되기 때문에 확실히 나의 코드에 자신감을 가지게 된다. 그렇기 때문에 리팩토링을 하는데 부담이 별로 없게 되어 리팩토링을 자주하게 되고, 자연스레 코드 품질은 올라간다. 일부 빨간색(혹은 노란색)인 테스트 결과를 모두 초록색으로 바꾸는 단순한 행위 자체가 더욱 성취감을 줘서 프로그래밍을 더 재미있게 만들어주기도 한다. 아쉬운 점도 있었다. 개인적으로는 예제와 실무 코드 사이에 약간의 괴리가 있다고 느꼈다. 책에서 나오는 예제들은 주로 굉장히 단순한 도메인 룰을 작성하는 경우가 대부분이다. 하지만, 실무에서 스프링 프레임워크를 사용하여 개발을 하다보면 하나의 Service 클래스가 여러 서비스와 리포지토리, mapper 등의 의존성을 주입받는 경우가 많은데, mocking 을 해야하는 의존성이 많은 경우 AAA(Arrange, Action, Assert)의 Arrange 와 Assert 에 해당하는 부분에서 해야할 일이 많아진다. 도메인 엔티티가 가진 필드의 수나 메서드의 인자의 수가 많으면 더미 데이터를 선언하는 부분이 장황해 지기도 한다. 책에 나오는 모든 예제는, 물론 설명을 위해 단순화한 형태의 코드를 사용한 것이겠지만서도, 테스트 코드가 굉장히 짧기 때문에 하나의 기능에 대해 여러개의 테스트 코드를 작성하는것에 대해 아무런 부담감이 들지 않는다. 하지만 의존성이 많아지면 때때로 하나의 기능에 대해 여러개의 테스트 코드를 작성하는 것을 주저하게 되는 자신을 발견했다. 예를 들어 삼각측량법을 통해 중복을 제거하고 일반화를 하기 위해서는 사실상 거의 같은 코드를 한 번 더 작성해야 되는데 하나의 테스트가 너무 길다보니 주저하게 되는 것이다. 그래서 개인적으로 삼각측량법은 자연스레 안쓰게 되었다. 물론 어느정도의 확신이 있다면 굳이 굉장히 짧은 호흡인 삼각측량법같은 방법을 사용하지 않아도 된다. 하지만 빠르고 간단하게 사용할 수 있지만 불필요해서 사용하지 않는 것과, 필요성이 모호한 상황에서 약간의 귀찮음이 더해져 사용하지 않는건 엄연히 다르다. 그리고 테스트 코드에서 발생하는 중복도 리팩토링을 통해 적절히 줄일 수가 있다. 하지만 나는 하나의 테스트 클래스 내 존재하는 여러 테스트들이 가진 중복을 private 메서드로 분리하여 호출하는 것을 웬만해선 하고싶지 않아하는 편이다. 왜냐하면 테스트 코드는 위에서 아래로 쭉 읽기만 해도 스토리가 자연스럽게 읽히도록 작성하는것이 베스트라고 생각한다. 하지만 메서드를 분리하게 되면 이것에 방해가 되어 다소 가독성이 떨어지기 때문이다. 중복이 많아지면 어쩔 수 없이 메서드를 분리하는 편이긴 하다. TDD 로 코드를 구현하면 IDE 의 자동완성 기능을 조금은 포기해야 한다는 것을 느끼기도 했다. 아직 존재하지 않는 클래스, 메서드, 변수 등을 사용하는 코드를 작성하기 때문이다. 물론, 존재하지 않는 클래스, 메서드, 변수를 사용하는 코드를 먼저 작성하고나서 IDE 가 제공하는 자동생성 기능을 활용할 수도 있다. 하지만 존재하지 않는 변수 등을 테스트 내에서 여러번 사용해야 하는 경우에는 매번 직접 타이핑하거나 복붙을 하기 위해 마우스를 사용하든, 커서를 위로 올렸다 내려야 해야하는 귀찮음이 있기 때문에 결국 일반적인 개발 순서와 반대로 개발하는 것에 대한 아주 약간의 불편함이 존재한다고 느꼈다. 새로운 방식이라 적응을 위해 좀 더 시간이 필요할 것 같기도 하다. 물론 위와 같은 이유로 실무에 적용하기 어렵다고 느낀다기 보다는, 책에서 언급하지 않은 특정 상황에서는 어떻게 적용하는 것이 맞는지에 대한 일부 물음이 해소가 되지 않았다는 표현이 정확할 것같다. 단순히 내가 아직 TDD 에 익숙하지 않아서 그런 것일 수도 있다. 그래서 앞으로 TDD 의 강력함을 실무에서 좀 더 느껴보아야 할 것 같다. 다른 사람들의 TDD 에 대한 경험은 어땠는지 알아보며 실무의 다양한 상황에서의 구체적인 적용방법에 대한 사례도 들어 보아야 할 것 같다. TDD 에서 일반적으로는 전체 시스템의 작은 조각을 나타내는 테스트부터 조금씩 붙여나가는 바텀업 빙식을 사용하겠지만, 사실 탑다운과 바텀업을 모두 사용할 수 있기 때문에 켄트 백은 TDD 의 개발 방향이 단순히 수직적인 메타포 보다는 '아는 것에서 모르는 것으로'(known-to-unknown)라는 표현이 정확할 것이라고 말하고 있긴 하다. 왜냐하면 TDD 에서는 확실히 아는 것부터 구현을 하게 되고 그러다 보면 계속 새로운 것을 알게되면서 할 일 목록에 적어나가기 때문이다. ↩︎","link":"/2021/07/23/tdd-review/"},{"title":"Spring 의 Profiles","text":"Profiles 을 사용하면 애플리케이션이 실행되는 환경에 따라 다른 Bean 들을 매핑할 수 있다. 예를 들어, 개발 환경, 스테이징 환경, 혹은 실 서비스 환경에 따라 다른 의존성을 주입할 수 있다. Profiles 구분하기 Bean 에 @Profile 붙이기 @Component@Profile(\"dev\")public class DevDatasourceConfig 다음과 같이 특정 profile 이 active 하지 않을 때만 container 에 포함시킬 수도 있다. @Component@Profile(\"!dev\")public class DevDatasourceConfig XML 에 정의하기 태그의 ”profiles” 속성으로 설정이 가능하다. &lt;beans profile=\"dev\"&gt; &lt;bean id=\"devDatasourceConfig\" class=\"org.baeldung.profiles.DevDatasourceConfig\" /&gt;&lt;/beans&gt; Profiles 설정하기 WebApplicationInitializer 인터페이스를 통한 방법 ServletContext 를 코드 기반으로 설정해주기 위해 다음과같이 WebApplicationInitializer 를 사용할 수 있다. @Configurationpublic class MyWebApplicationInitializer implements WebApplicationInitializer { @Override public void onStartup(ServletContext servletContext) throws ServletException { servletContext.setInitParameter( \"spring.profiles.active\", \"dev\"); }} ConfigurableEnvironment 를 통한 방법 @Autowiredprivate ConfigurableEnvironment env;...env.setActiveProfiles(\"someProfile\"); web.xml 를 통한 방법 &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/app-config.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;context-param&gt; &lt;param-name&gt;spring.profiles.active&lt;/param-name&gt; &lt;param-value&gt;dev&lt;/param-value&gt;&lt;/context-param&gt; JVM 파라미터를 통한 방법 -Dspring.profiles.active=dev 환경변수를 통한 방법 export spring_profiles_active=dev Maven profiles 를 통한 방법 이 방식은 매우 복잡하므로 그냥 참고만 하자. &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;spring.profiles.active&gt;dev&lt;/spring.profiles.active&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;properties&gt; &lt;spring.profiles.active&gt;prod&lt;/spring.profiles.active&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 위와같이 설정한 뒤에 application.properties 파일에서 @spring.profiles.active@ placeholder 를 사용하고 spring.profiles.active=@spring.profiles.active@ Resource filtering 을 활성화해주고 &lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; ...&lt;/build&gt; 마지막으로 메이븐에 -P parameter 를 주면 된다. mvn clean package -Pprod 이 명령어는 prod 프로파일을 적용하여 앱을 패키지할것이고, spring.profiles.active 의 값도 prod 로 설정할것이다. 테스트 시 @ActiveProfiles 를 통한 방법 테스트할 땐 profile 를 명시하는것이 매우 쉽다. 테스트 클래스에 다음과 같이 어노테이션을 붙여주면 된다. @ActiveProfiles(\"dev\") 여러가지를 함께 사용할 경우 위의 모든 방식들을 동시에 여러개 사용하는 경우 다음의 우선순위에 따라 결정된다. (우선순위가 높은 순으로) Context parameter in web.xml WebApplicationInitializer JVM System parameter Environment variable Maven profile 디폴트 Profile 만약 Profiles 를 지정하지 않으면 &quot;default&quot; profile 로 설정되며, spring.profiles.default 프로퍼티를 통해 디폴트 profile 를 &quot;default&quot; 에서 다른 값으로 변경할 수 있다. 활성화된 Profiles 사용하기 Environment 를 통한 방법 public class ProfileManager { @Autowired private Environment environment; public void getActiveProfiles() { for (String profileName : environment.getActiveProfiles()) { System.out.println(\"Currently active profile - \" + profileName); } }} spring.active.profile 을 통한 방법 @Value(\"${spring.profiles.active}\")private String activeProfile; 그런데 만약 현재 active 한 profile 이 없는 경우에는 IllegalArgumentException 이 발생할 수 있으므로 default 값을 줘야한다. @Value(\"${spring.profiles.active:}\")private String activeProfile; Spring Boot 에서의 Profiles 스프링 부트는 위에 언급된 모든 profile 설정 방식은 물론, 몇가지 추가적인 기능을 제공한다. properties 파일을 통한 방법 우선, 다음과같이 properties 파일에서 설정이 가능하다. spring.profiles.active=dev SpringApplication 클래스를 통한 방법 또한, 다음과같이 SpringApplication class 를 사용하여 코드 기반의 설정도 가능하다. SpringApplication.setAdditionalProfiles(\"dev\"); Maven 을 통한 방법 마지막으로, 메이븐으로도 설정 가능하다. pom.xml 의 spring-boot-maven-plugin 내에 프로파일 이름을 명시할 수 있다. &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;profiles&gt; &lt;profile&gt;dev&lt;/profile&gt; &lt;/profiles&gt; &lt;/configuration&gt; &lt;/plugin&gt; ...&lt;/plugins&gt; 그리고 mvn 명령어로 앱을 실행하면된다. man spring-boot:run 하지만 스프링 부트의 profiles 에 관한 기능 중 가장 중요한 것이 다음 기능이다. Profile 별 properties files 스프링 부트는 우선. application.properties 파일 내 모든 property 를 로드한 후, active 된 profile 들에 대한 .properties 파일에 대해서만 property 를 로드한다. 참고: https://www.baeldung.com/spring-profiles","link":"/2020/07/06/spring-profiles/"},{"title":"Spring 의 Properties","text":"Spring 과 Spring Boot 에서 프로퍼티를 세팅하고 사용하는 방법을 알아보자 Properties 파일 불러오기 Annotation 방식 우선 Spring 3.1 에서 소개된 @PropertySource 어노테이션을 이용하여 properties 파일을 불러오고, 사용할 수 있다. @PropertySource 은 @Configuration 과 함께 사용되어야 한다. @Configuration@PropertySource(\"classpath:foo.properties\")public class PropertiesWithJavaConfig { //...} 런타임에 동적으로 파일을 선택할 수도 있다. @PropertySource({ \"classpath:persistence-${envTarget:mysql}.properties\"})... 와 같이 작성하면 구동 환경에 따라 여러개의 properties 파일을 만들어 두고, 실행시 간편하게 선택할 수 있다. 여러개의 파일을 지정할 수도 있다. @PropertySource(\"classpath:foo.properties\")@PropertySource(\"classpath:bar.properties\")public class PropertiesWithJavaConfig { //...} Java 7 이하라면 아래와 같이도 할 수 있다 @PropertySources({ @PropertySource(\"classpath:foo.properties\"), @PropertySource(\"classpath:bar.properties\")})public class PropertiesWithJavaConfig { //...} 만약 property 이름이 중복되는 경우에는 나중에 로드된 property 가 우선권을 갖는다. XML 방식 &lt;context:property-placeholder location=\"classpath:foo.properties, classpath:bar.properties\"/&gt; Properties 사용/주입 하기 파일로부터 불러온 Properties 를 사용하는 방법은 여러가지다. @Value 어노테이션 XML 에서 bean 설정 Environment API @Value 어노테이션 @Value( \"${jdbc.url}\" )private String jdbcUrl; 디폴트 값을 설정할 수도 있다. @Value( \"${jdbc.url:aDefaultUrl}\" )private String jdbcUrl; in XML &lt;bean id=\"dataSource\"&gt; &lt;property name=\"url\" value=\"${jdbc.url}\" /&gt;&lt;/bean&gt; Environment API @Autowiredprivate Environment env;...dataSource.setUrl(env.getProperty(\"jdbc.url\")); Spring Boot 에서 Properties 불러오기 스프링 부트에서는 간편한 설정(부트의 주 목표 중 하나)을 위해 추가 기능을 지원한다. 디폴트 프로퍼티 파일 부트에서는 디폴트 프로퍼티 파일로 src/main/resources 내의 application.properties 를 사용한다. 그러므로 @PropertySource 를 등록해 줄 필요가 없다. 또한, 만일 다른 프로퍼티 파일을 사용해야 한다면 런타임에 지정해줄 수도 있다. java -jar app.jar --spring.config.location=classpath:/another-location.properties Spring Boot 2.3 부터는 프로퍼티 파일 지정에 와일드카드도 사용할 수 있다. java -jar app.jar --spring.config.location=config/*/ 환경별 프로퍼티 파일 “src/main/resources” 디렉터리 안에 “application-environment.properties” 의 형태로 파일을 만들어두고 profile 을 해당 environment 와 같은 이름으로 설정해주면 디폴트 properties 가 로드된 이후에 해당 profile-specific 한 properties 파일인 application-environment.properties가 로드된다. 나중에 로드된 properties 가 높은 우선순위를 가지므로 중복된 프로퍼티는 profile-specific 프로퍼티를 따른다. @TestPropertySource 어노테이션 테스트 시에 다음과 같이 디폴트 property 보다 높은 우선순위로 특정 property 파일을 로드할 수 있다. @RunWith(SpringRunner.class)@TestPropertySource(\"/foo.properties\")public class FilePropertyInjectionUnitTest { @Value(\"${foo}\") private String foo; @Test public void whenFilePropertyProvided_thenProperlyInjected() { assertThat(foo).isEqualTo(\"bar\"); }} 파일을 사용하기 싫다면 다음과같이 바로 name 과 value 를 줄 수도 있다. @RunWith(SpringRunner.class)@TestPropertySource(properties = {\"foo=bar\"})public class PropertyInjectionUnitTest { @Value(\"${foo}\") private String foo; @Test public void whenPropertyProvided_thenProperlyInjected() { assertThat(foo).isEqualTo(\"bar\"); }} 또한, @SpringBootTest 어노테이션의 properties 인자로도 동일한 효과를 낼 수 있다. @RunWith(SpringRunner.class)@SpringBootTest( properties = {\"foo=bar\"}, classes = SpringBootPropertiesTestApplication.class)public class SpringBootPropertyInjectionIntegrationTest { @Value(\"${foo}\") private String foo; @Test public void whenSpringBootPropertyProvided_thenProperlyInjected() { assertThat(foo).isEqualTo(\"bar\"); }} 계층형 프로퍼티 @ConfigurationProperties 로 여러 프로퍼티를 하나의 자바 객체로 묶을 수 있다. database.url=jdbc:postgresql:/localhost:5432/instancedatabase.username=foodatabase.password=bar 의 경우에는 아래와 같은 형태의 객체로 매핑할 수 있다. @ConfigurationProperties(prefix = \"database\")public class Database { String url; String username; String password; // standard getters and setters} YAML 파일 properties 파일과의 차이점은 확장자가 다르다는 것과 SnakeYAML 라이브러리에 대한 의존성이 classpath 에 있어야 한다는 것이다. 명령행 인자 파일을 사용하지 않고 명령행 인자로 바로 프로퍼티를 줄 수도 있다. java -jar app.jar --property=&quot;value&quot; java -Dproperty.name=&quot;value&quot; -jar app.jar 환경변수 스프링 부트는 환경변수도 감지하여 프로퍼티로 취급한다. export name=valuejava -jar app.jar Raw Bean PropertyPlaceholderConfigurer(스프링 3.0), 혹은 PropertySourcesPlaceholderConfigurer(스프링 3.1) 을 사용하는 방법도 있다. 장황하며 대부분의 경우 불필요하지만, 직접 property configuration bean 을 정의하여 configuration 을 완벽하게 제어할 수가 있다. 여기서는 다루지 않겠다. 참조: https://www.baeldung.com/properties-with-spring","link":"/2020/07/06/spring-properties/"}],"tags":[{"name":"Cloud","slug":"Cloud","link":"/tags/Cloud/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"DevOps","slug":"DevOps","link":"/tags/DevOps/"},{"name":"Architecture","slug":"Architecture","link":"/tags/Architecture/"},{"name":"MSA","slug":"MSA","link":"/tags/MSA/"},{"name":"DB","slug":"DB","link":"/tags/DB/"},{"name":"Open Source","slug":"Open-Source","link":"/tags/Open-Source/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Books","slug":"Books","link":"/tags/Books/"},{"name":"Design Pattern","slug":"Design-Pattern","link":"/tags/Design-Pattern/"},{"name":"CS General","slug":"CS-General","link":"/tags/CS-General/"},{"name":"Algorithms","slug":"Algorithms","link":"/tags/Algorithms/"},{"name":"Domain Driven Design","slug":"Domain-Driven-Design","link":"/tags/Domain-Driven-Design/"},{"name":"Languages","slug":"Languages","link":"/tags/Languages/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Maven","slug":"Maven","link":"/tags/Maven/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Coroutine","slug":"Coroutine","link":"/tags/Coroutine/"},{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"Asynchronous","slug":"Asynchronous","link":"/tags/Asynchronous/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Functional Programming","slug":"Functional-Programming","link":"/tags/Functional-Programming/"},{"name":"Web Server","slug":"Web-Server","link":"/tags/Web-Server/"},{"name":"WAS","slug":"WAS","link":"/tags/WAS/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"}],"categories":[{"name":"Cloud","slug":"Cloud","link":"/categories/Cloud/"},{"name":"Server","slug":"Server","link":"/categories/Server/"},{"name":"Open Source","slug":"Open-Source","link":"/categories/Open-Source/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"},{"name":"Books","slug":"Books","link":"/categories/Books/"},{"name":"Kubernetes","slug":"Cloud/Kubernetes","link":"/categories/Cloud/Kubernetes/"},{"name":"General","slug":"General","link":"/categories/General/"},{"name":"Architecture","slug":"Server/Architecture","link":"/categories/Server/Architecture/"},{"name":"Languages","slug":"Languages","link":"/categories/Languages/"},{"name":"Git","slug":"Tools/Git","link":"/categories/Tools/Git/"},{"name":"Database","slug":"Database","link":"/categories/Database/"},{"name":"Spring","slug":"Server/Spring","link":"/categories/Server/Spring/"},{"name":"Design Pattern","slug":"General/Design-Pattern","link":"/categories/General/Design-Pattern/"},{"name":"Algorithms","slug":"General/Algorithms","link":"/categories/General/Algorithms/"},{"name":"Domain Driven Design","slug":"General/Domain-Driven-Design","link":"/categories/General/Domain-Driven-Design/"},{"name":"Java","slug":"Languages/Java","link":"/categories/Languages/Java/"},{"name":"Python","slug":"Languages/Python","link":"/categories/Languages/Python/"},{"name":"Redis","slug":"Database/Redis","link":"/categories/Database/Redis/"}]}